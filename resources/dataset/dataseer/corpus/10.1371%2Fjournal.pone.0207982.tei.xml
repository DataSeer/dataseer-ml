<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/lopez/grobid/dataseer-ml/../grobid-home/schemas/xsd/Grobid.xsd">
  <teiHeader xml:lang="en">
    <encodingDesc>
      <appInfo>
        <application ident="GROBID" version="0.5.6-SNAPSHOT" when="2019-11-05T20:13+0000">
          <ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
        </application>
      </appInfo>
    </encodingDesc>
    <fileDesc>
      <titleStmt>
        <title level="a" type="main">A deep learning model for the detection of both advanced and early glaucoma using fundus photography</title>
      </titleStmt>
      <publicationStmt>
        <publisher/>
        <availability status="unknown">
          <licence/>
        </availability>
        <date type="published" when="2018-11-27">Published: November 27, 2018</date>
      </publicationStmt>
      <sourceDesc>
        <biblStruct>
          <analytic>
            <author>
              <persName>
                <forename type="first">Jin</forename>
                <forename type="middle">Mo</forename>
                <surname>Ahn</surname>
              </persName>
              <affiliation key="aff0">
                <orgName type="department">Department of Bioinformatics and Life Science</orgName>
                <orgName type="institution">Soongsil University</orgName>
                <address>
                  <settlement>Seoul</settlement>
                  <country key="KR">Korea</country>
                </address>
              </affiliation>
            </author>
            <author role="corresp">
              <persName>
                <forename type="first">Sangsoo</forename>
                <surname>Kim</surname>
              </persName>
              <email>*ungsookim@kimeye.com</email>
              <affiliation key="aff0">
                <orgName type="department">Department of Bioinformatics and Life Science</orgName>
                <orgName type="institution">Soongsil University</orgName>
                <address>
                  <settlement>Seoul</settlement>
                  <country key="KR">Korea</country>
                </address>
              </affiliation>
            </author>
            <author>
              <persName>
                <forename type="first">Kwang-Sung</forename>
                <surname>Ahn</surname>
              </persName>
              <affiliation key="aff1">
                <orgName type="department">Functional Genome Institute</orgName>
                <orgName type="institution">PDXen Biosystems Inc</orgName>
                <address>
                  <settlement>Seoul</settlement>
                  <country key="KR">Korea</country>
                </address>
              </affiliation>
            </author>
            <author>
              <persName>
                <forename type="first">Sung-Hoon</forename>
                <surname>Cho</surname>
              </persName>
              <affiliation key="aff1">
                <orgName type="department">Functional Genome Institute</orgName>
                <orgName type="institution">PDXen Biosystems Inc</orgName>
                <address>
                  <settlement>Seoul</settlement>
                  <country key="KR">Korea</country>
                </address>
              </affiliation>
            </author>
            <author>
              <persName>
                <forename type="first">Kwan</forename>
                <forename type="middle">Bok</forename>
                <surname>Lee</surname>
              </persName>
              <affiliation key="aff2">
                <orgName type="institution">Kim's Eye Hospital</orgName>
                <address>
                  <settlement>Seoul</settlement>
                  <country key="KR">Korea</country>
                </address>
              </affiliation>
            </author>
            <author>
              <persName>
                <forename type="first">Ungsoo</forename>
                <forename type="middle">Samuel</forename>
                <surname>Kimid</surname>
              </persName>
              <affiliation key="aff2">
                <orgName type="institution">Kim's Eye Hospital</orgName>
                <address>
                  <settlement>Seoul</settlement>
                  <country key="KR">Korea</country>
                </address>
              </affiliation>
              <affiliation key="aff3">
                <orgName type="department">Department of Ophthalmology</orgName>
                <orgName type="institution">Konyang University College of Medicine</orgName>
                <address>
                  <settlement>Daejeon</settlement>
                  <country key="KR">Korea</country>
                </address>
              </affiliation>
            </author>
            <title level="a" type="main">A deep learning model for the detection of both advanced and early glaucoma using fundus photography</title>
          </analytic>
          <monogr>
            <imprint>
              <date type="published" when="2018-11-27">Published: November 27, 2018</date>
            </imprint>
          </monogr>
          <note type="submission">Received: March 24, 2018 Accepted: November 11, 2018</note>
          <note>RESEARCH ARTICLE OPEN ACCESS Citation: Ahn JM, Kim S, Ahn K-S, Cho S-H, Lee KB, Kim US (2018) A deep learning model for the detection of both advanced and early glaucoma using fundus photography. PLoS ONE 13(11): e0207982. https://doi.org/10.1371/journal. pone.0207982 Editor: Sanjoy Bhattacharya, Bascom Palmer Eye Institute, UNITED STATES Copyright: Data Availability Statement: All data files are available on the Harvard Dataverse repository at https://doi.org/10.7910/DVN/1YRRAC. Funding: PDXen partially funded and sponsored the study. There was no additional external funding received for this study. SHC and KSA: Employees -PDXen. PDXen provided support in the form of salary for authors SHC and KSA, and participated in data analysis, and interpretation of the results, and in the preparation of the manuscript. The specific</note>
        </biblStruct>
      </sourceDesc>
    </fileDesc>
    <profileDesc>
      <abstract>
        <div>
          <head>Purpose</head>
          <p>
            <s>To build a deep learning model to diagnose glaucoma using fundus photography.</s>
          </p>
        </div>
        <div subtype="dataseer">
          <head>Design</head>
          <p>
            <s cert="0.6254080533981323" id="dataset-1" type="Tabular data">Cross sectional case study Subjects, Participants and Controls: A total of 1,542 photos (786 normal controls, 467 advanced glaucoma and 289 early glaucoma patients) were obtained by fundus photography.</s>
          </p>
        </div>
        <div>
          <head>Method</head>
          <p>
            <s>The whole dataset of 1,542 images were split into 754 training, 324 validation and 464 test datasets.</s>
            <s>These datasets were used to construct simple logistic classification and convolutional neural network using Tensorflow.</s>
            <s>The same datasets were used to fine tune pretrained GoogleNet Inception v3 model.</s>
          </p>
        </div>
        <div>
          <head>Results</head>
          <p>
            <s>The simple logistic classification model showed a training accuracy of 82.9%, validation accuracy of 79.9% and test accuracy of 77.2%.</s>
            <s>Convolutional neural network achieved accuracy and area under the receiver operating characteristic curve (AUROC) of 92.2% and 0.98 on the training data, 88.6% and 0.95 on the validation data, and 87.9% and 0.94 on the test data.</s>
            <s>Transfer-learned GoogleNet Inception v3 model achieved accuracy and AUROC of 99.7% and 0.99 on training data, 87.7% and 0.95 on validation data, and 84.5% and 0.93 on test data.</s>
          </p>
        </div>
        <div>
          <head>Conclusion</head>
          <p>
            <s>Both advanced and early glaucoma could be correctly detected via machine learning, using only fundus photographs.</s>
            <s>Our new model that is trained using convolutional neural network is more efficient for the diagnosis of early glaucoma than previously published models.</s>
          </p>
        </div>
      </abstract>
    </profileDesc>
    <revisionDesc>
      <date type="submission" when="207982-11-27"/>
    </revisionDesc>
  </teiHeader>
  <text xml:lang="en">
    <body>
      <div>
        <formula xml:id="formula_0">a1111111111 a1111111111 a1111111111 a1111111111 a1111111111</formula>
      </div>
      <div>
        <head>Introduction</head>
        <p>
          <s>Machine learning is a system of artificial computer intelligence that provides computers with the ability to automatically learn without being programmed.</s>
          <s>In the healthcare sector, machine learning has been used to investigate skin cancer classification, to evaluate complex genetic interactions in autism, and to perform monitoring within the intensive care unit <ref target="#b0" type="bibr">[1]</ref>
            <ref target="#b1" type="bibr">[2]</ref>
            <ref target="#b2" type="bibr">[3]</ref> .</s>
          <s>A recent study of diabetic retinopathy using deep machine learning revealed that machine learning exhibited high sensitivity and specificity for the detection of diabetic retinopathy <ref target="#b3" type="bibr">[4]</ref> .</s>
        </p>
        <p>
          <s>Glaucoma is a progressive optic nerve disorder consisting of various optic disc changes, such as the notching of neuroretinal rims and enlarged optic disc cupping.</s>
          <s>Notably, glaucoma is one of the leading causes of blindness <ref target="#b4" type="bibr">[5]</ref> .</s>
          <s>Thus, an effective, early investigation of optic disc changes is important in the diagnosis of glaucoma.</s>
          <s>Several reports have proven the efficacy of machine learning in the early detection of glaucoma <ref target="#b5" type="bibr">[6]</ref>
            <ref target="#b6" type="bibr">[7]</ref>
            <ref target="#b7" type="bibr">[8]</ref>
            <ref target="#b8" type="bibr">[9]</ref> .</s>
          <s>However, the previous reports have utilized optical coherence tomography (OCT), red-free retinal-nerve-fiber-layer (RNFL) photography, or visual field tests.</s>
          <s>In the clinic, fundus photography is the most familiar and easiest test.</s>
          <s>Therefore, we investigated the efficacy of machine learning and deep learning for detection of both advanced and early glaucoma, using only fundus photography.</s>
          <s>Firstly, we have used logistic classification, a traditional machine learning technique, to check the performance on discriminating glaucoma patients from normal control.</s>
          <s>Secondly, we have used GoogleNet Inception v3 <ref target="#b9" type="bibr">[10]</ref> , a pre-trained model, for transfer learning of our data to check the efficacy of deep learning.</s>
          <s>Finally, we have constructed our own convolutional neural network and compared the performance.</s>
        </p>
      </div>
      <div>
        <head>Methods</head>
      </div>
      <div subtype="dataseer">
        <head>Data preparation</head>
        <p>
          <s cert="0.746660053730011" id="dataset-1" type="Image:Photography">Fundus photographs of normal and glaucoma patients were collected from Kim's Eye Hospital.</s>
          <s cert="0.8913145065307617" corresp="#dataset-1" id="dataset-3" type="Image:Photography">Fundus photography was performed using a non-mydriatic auto fundus camera (AFC-330, Nidek, Japan).</s>
          <s cert="0.7538757920265198" corresp="#dataset-1" id="dataset-4" type="Image:Photography">A total of 1,542 photos were obtained, including 786 photos from normal patients and 756 photos from 467 advanced and 289 early glaucoma patients.</s>
          <s>These photos had different sizes, and thus were scaled to have fixed width size of 800 pixels.</s>
          <s>In order to produce a fixed size input necessary for machine learning models, the photos were cropped at the region of optic nerve with size of 240X240pixels.</s>
          <s>The normal patients exhibited normal findings on red-free RNFL photography (Vx-10; Kowa Optimed, Inc., Tokyo, Japan), OCT (Cirrus HD-OCT, Carl Zeiss Meditec Inc., Dublin, CA), and visual field test (Humphrey 740 visual field analyzer, Carl Zeiss Meditec Inc., Dublin, CA).</s>
          <s>The inclusion criteria of the glaucoma patients were as follows: typical glaucomatous visual field defects, and/or bundle defects of RNFLs on HD-OCT, and/or bundle defects of RNFLs on red-free RNFL photography.</s>
          <s>Among 756 glaucoma patients, 467 cases were in advanced stage (near total cupping of the optic nerve, with or without severe visual field loss within 10˚of fixation), and 289 cases were early glaucoma (glaucomatous RNFL defects in red-free RNFL photography, without visual field defects.</s>
          <s>The classification of early glaucoma and advanced glaucoma was determined by agreement of two specialists.</s>
        </p>
        <p>
          <s>For the classification of glaucoma images from normal images even with the presence of early glaucoma images, the entire set of 1,542 images were split into 754 training, 324 validation, and 464 test datasets (images) ( <ref target="#tab_1" type="table">Table 1</ref> ).</s>
          <s>The test dataset comprises about 30% of the whole dataset.</s>
          <s>The remaining dataset was split to about 70% training and 30% validation datasets.</s>
          <s>The study was approved by the Institutional Review Board of Kim's Eye Hospital and was conducted in accordance with the tenets of the Declaration of Helsinki.</s>
        </p>
      </div>
      <div>
        <head>Logistic classification</head>
        <p>
          <s>Since fundus photographs are color images, they consist of three-dimensional arrays (240×240×3).</s>
          <s>To perform logistic regression, images were flattened into a one-dimensional array of 1×(240×240×3).</s>
          <s>A single layer of weights was used to produce logits; the softmax function was applied to obtain the probability of being classified as a normal or advanced glaucoma image.</s>
          <s>These probabilities were compared to one-hot encoded labels and loss was calculated using cross entropy.</s>
          <s>A gradient descent optimizer, with a learning rate of 0.5, was used for optimization.</s>
          <s>
            <ref type="figure">Fig 1 shows</ref> the detailed architecture.</s>
          <s>The model was constructed using Google's Tensorflow deep learning framework <ref target="#b10" type="bibr">[11]</ref> .</s>
        </p>
      </div>
      <div subtype="dataseer">
        <head>Convolutional Neural Network (CNN)</head>
        <p>
          <s>Data augmentation.</s>
          <s>Since the images comprised a small dataset, we applied augmentation to each image to overcome overfitting.</s>
          <s>Each image was cropped at all four corners, as well as in the middle, to generate five images with fixed size of 224X224.</s>
          <s>This cropping process was repeated after flipping the image, thereby generating 10 images per photograph.</s>
          <s>Data augmentation can help overcome overfitting by showing the computer an image from various views to aid in making a decision <ref target="#b11" type="bibr">[12]</ref> .</s>
        </p>
        <p>
          <s>Training model.</s>
          <s>We used a GoogleNet Inception v3 pre-trained model for transfer learning, which included training our data with a predefined (trained) existing model.</s>
          <s>We modified the last classification layer of the Inception v3 model to fit our classification needs, and then fine-tuned using our data.</s>
          <s>For backpropagation, the Adam optimizer, an adaptive learning rate method, was used as an optimization function, while cross entropy was used as a loss function.</s>
          <s>
            <ref target="#fig_0" type="figure">Fig 2 shows</ref> the original architecture of the Inception v3 model.</s>
        </p>
        <p>
          <s id="dataset-2" type="Tabular data">We also constructed our own Convolutional Neural Network, using Google's Tensorflow as backend.</s>
          <s>Two convolutional layers, with patch sizes of 2020 and 4040, were used with a stride of 1 and depths of 16 and 32.</s>
          <s>Max pooling was applied, with a patch size of 22 and a stride of 2. Two hidden layers, with 32 and 64 hidden units, were used as fully connected layers.</s>
          <s>A dropout rate of 0.5 was used in convolutional and fully connected layers to overcome overfitting; ReLu (Rectifier Linear unit) was used as an activation function.</s>
          <s>For backpropagation, cross entropy was used as a loss function and the Adagrad optimizer was used as an optimization function.</s>
          <s>All weights were initialized using the Xavier initializer <ref target="#b12" type="bibr">[13]</ref> .</s>
          <s>
            <ref target="#fig_1" type="figure">Fig 3 shows</ref> the architecture of our model.</s>
          <s>Evaluation.</s>
          <s>Our models accept an image as input and output the probability that the image represents a photograph of a glaucoma or normal patient.</s>
          <s>Since we have used augmented data (10 images per photography), we generate 10 probabilities from a single image.</s>
          <s>By averaging this probability, we can obtain the single probability that the image represents a glaucoma or normal patient, based on each image <ref target="#fig_2" type="figure">(Fig 4)</ref> .</s>
          <s>Using this strategy, we have evaluated our own model and GoogleNet Inception v3 model based on ROC (receiver operating characteristic) curve by calculating sensitivity and specificity of the models.</s>
          <s>Moreover, we measured the area under the ROC curve (AUC) as our performance indicator.</s>
        </p>
      </div>
      <div>
        <head>Results</head>
      </div>
      <div subtype="dataseer">
        <head>Traditional machine learning (Logistic classification) approach</head>
        <p>
          <s>Our simple logistic classification model exhibited a training accuracy of 82.9%, a validation accuracy of 79.9% and a test accuracy of 77.2%.</s>
          <s>To check whether logistic classification model can discriminate advanced glaucoma from normal control without early glaucoma images, advanced glaucoma images were selected from entire dataset to be used to train the logistic classification model.</s>
          <s>Among 756 glaucoma images, 467 images were advanced glaucoma.</s>
          <s>495 normal images were also selected from 786 normal images to avoid imbalanced data problem.</s>
          <s>About 30% of the 467 advanced glaucoma images and 495 normal control images were randomly split into the test dataset.</s>
          <s>This resulted in training accuracy of 99.7% and test accuracy of 98.6%.</s>
          <s>We also checked whether logistic classification model can discriminate early glaucoma from normal control without advanced glaucoma images.</s>
          <s>Among 756 glaucoma images, 289 images were early glaucoma.</s>
          <s cert="0.8511672019958496" id="dataset-5" type="Image">289 normal images were selected from 786 normal images to avoid imbalanced data problem.</s>
          <s>About 30% of the 289 early glaucoma images and 289 normal control images were randomly split into the test dataset.</s>
          <s>This resulted in training accuracy of 83.7% and test accuracy of 73.0% <ref target="#tab_2" type="table">(Table 2</ref> ).</s>
          <s>This suggests the needs of complex algorithm such as deep learning technique to discriminate both advanced glaucoma and early glaucoma from normal control.</s>
          <s>
            <ref target="#tab_3" type="table">Table 3</ref> shows summarized results from our own model and from the Inception v3 model.</s>
          <s>Accuracy refers to the raw accuracy of augmented data, whereas average accuracy refers to ensemble predicted accuracy <ref target="#fig_2" type="figure">(Fig 4)</ref> .</s>
          <s>Inception v3 transfer learning model achieved accuracy and AUC of 99.7% and 0.99, respectively, on the training data, 87.7% and 0.95 on the validation data, and 84.5% and 0.93 on the test data.</s>
          <s>To improve test accuracy, we have developed our own convolutional neural network model.</s>
          <s>In order to build a new model, we have tuned manually various combinations of the hyper-parameters such as convolution patch size, strides, filter size, number of convolution layers, number of fully connected layers, number of hidden nodes, which optimizer to use, learning rate and so on.</s>
          <s>Our final model achieved accuracy and AUC of 92.2% and 0.98 on the training data, 88.6% and 0.95 on the validation data, and 87.9% and 0.94 on the test data.</s>
          <s>Both our own model and Inception v3 transferred model showed slightly higher ensemble accuracy than raw accuracy.</s>
          <s>The ROC curve for each model is depicted in <ref type="figure">Fig 5.</ref> The training stage was considered finished when the average loss for each epoch started to increase for the validation data.</s>
          <s>Our Convolutional Neural Network needed 29 epochs for optimization whereas Inception v3 transferred model needed 14 epochs for optimization.</s>
        </p>
      </div>
      <div>
        <head>Deep learning (convolutional neural network) approach</head>
      </div>
      <div>
        <head>Discussion</head>
        <p>
          <s>This study demonstrates that deep learning techniques can be combined with fundus photography as an effective approach to distinguish between normal controls and glaucoma patients, even at early stages.</s>
          <s>A simple traditional machine learning approach, such as logistic classification, was sufficient for classifying advanced glaucoma patients.</s>
          <s>However, discrimination of both advanced glaucoma and early glaucoma from normal control required a complex deep learning approach, such as CNN.</s>
          <s>Using a complex deep learning model yields a vast array of parameters that may cause overfitting of the training data.</s>
          <s>Thus, we incorporated regularization techniques, such as dropout and data augmentation.</s>
          <s>Dropout randomly corrupts hidden nodes, which are passed to the succeeding layers.</s>
          <s>Since this process is random, the detailed architecture of the model changes at each iteration of training, leading to a generalized model with a sufficient number of training iterations.</s>
          <s>Data augmentation allows the machine to learn an image from a different view; using this approach, we generated 10 images per fundus photograph and averaged the results for the final evaluation.</s>
          <s>This ensemble prediction process yielded an improved model <ref target="#tab_3" type="table">(Table 3)</ref> .</s>
          <s>Transfer learning, using the Google Inception v3 model, required less epochs for training than our CNN model.</s>
          <s>Since transfer learning requires the use of an existing trained model, all the parameters that are provided within Inception v3 were used as initial parameters <ref target="#b13" type="bibr">[14]</ref> .</s>
          <s>Notably, these parameters have already been optimized for detecting natural images, such as edges and curves, and may require fewer epochs for optimization than a model that began from random parameters.</s>
          <s>Further, considering that the Inception v3 models are trained using extremely large numbers of images (approximately 1.28 million images), the initial convolutional patches are more generalized at detecting features; thus, these will provide a more generalized model when trained with small amounts of data.</s>
          <s>In the case of large volume data, it may take a long time to build and optimize a new model.</s>
          <s>Therefore, many studies on developing image classification model, have used transfer learning based on the state of the art Convolutional Neural Network models <ref target="#b0" type="bibr">[1,</ref>
            <ref target="#b14" type="bibr">15,</ref>
            <ref target="#b15" type="bibr">16]</ref> .</s>
          <s>These models include GoogleNet Inception v3, Very Deep Convolution Network from Visual Geometry group(VGG) <ref target="#b16" type="bibr">[17]</ref> and ResNet <ref target="#b17" type="bibr">[18]</ref> .</s>
          <s>Recent study using large scale fundus photography used ensemble of AlexNet <ref target="#b18" type="bibr">[19]</ref> , VGG and Inception v3 transferred learned model to classify age related eye disease <ref target="#b19" type="bibr">[20]</ref> .</s>
          <s>While transfer learning is an attractive option in building image classification model regardless of how big the data is, an alternative strategy for a small scale data would be to develop one's own convolutional neural network model with a simpler architecture.</s>
          <s>In fact, our convolutional neural network model with far less parameters worked slightly better than the Google Inception v3 transfer learning model in terms of test accuracy <ref target="#tab_3" type="table">(Table 3)</ref> .</s>
          <s>This may be due to complexity of the Inception v3 algorithm.</s>
          <s>Since our purpose is to discriminate glaucoma and normal control, a model architecture like Inception v3, which is designed to classify 1000 categories, can be too heavy.</s>
          <s>On the other hand, our model, which was specifically tuned in terms of architecture for binary classification of glaucoma and normal control, showed improved test accuracy on a small scale.</s>
          <s>However, many trials and errors, along with a substantial amount of hyper-parameter tuning time, were required to build a new, optimized model.</s>
        </p>
        <p>
          <s>Due to the complexity of neural network and many feature maps created during training time, further analysis is required to explain why a machine-trained model classified the image as a glaucoma or normal patient.</s>
          <s>Such analysis can be performed by viewing the image after each convolutional layer, along with plotting the image using a technique such as t-distributed Stochastic Neighboring Embedding (t-SNE) <ref target="#b20" type="bibr">[21]</ref> .</s>
          <s>However, this method does not provide a score of variable importance, as in the random forest technique; thus, it may require an expert assistance.</s>
        </p>
        <p>
          <s>Kim et al <ref target="#b21" type="bibr">[22]</ref> .</s>
          <s>reported that the classification accuracy, sensitivity, specificity, and AUC for glaucoma, using machine learning, were 0.98, 0.983, 0.975, and 0.979, respectively.</s>
          <s>However, their study used multimodal imaging, including fundus photography, red-free fundus photography, visual field testing, and spectral domain OCT.</s>
          <s>While their study showed higher accuracy, sensitivity, and AUC than the present study, the latter used only fundus photography, achieving similar accuracy in cases of advanced glaucoma and only a slight difference in cases of early glaucoma.</s>
        </p>
        <p>
          <s>In conclusion, deep learning using only fundus photography could be an ancillary test for the diagnosis of glaucoma.</s>
          <s>In addition, if the algorithm becomes more sophisticated, it may serve as a robust aid for detection of the early stages of glaucoma.</s>
        </p>
      </div>
      <div>
        <head>Author Contributions</head>
        <p>
          <s>Conceptualization: Jin Mo Ahn, Kwang-Sung Ahn, Sung-Hoon Cho, Ungsoo Samuel Kim.</s>
        </p>
        <p>
          <s>Data curation: Jin Mo Ahn, Kwang-Sung Ahn, Sung-Hoon Cho, Kwan Bok Lee, Ungsoo Samuel Kim.</s>
        </p>
      </div>
      <figure xml:id="fig_0">
        <head>Fig 2 .</head>
        <label>2</label>
        <figDesc>Google Inception v3 architecture: A schematic view of the Inception v3 model. Each layer consists of an inception module along with merging, and a fully connected layer at the end. https://doi.org/10.1371/journal.pone.0207982.g002</figDesc>
      </figure>
      <figure xml:id="fig_1">
        <head>Fig 3 .</head>
        <label>3</label>
        <figDesc>Convolutional neural network architecture: A schematic view of our convolutional neural network used in this study. It consists of three convolutional layers with max pooling applied at each layer, along with two fully connected layers. https://doi.org/10.1371/journal.pone.0207982.g003 Machine learning for glaucoma PLOS ONE | https://doi.org/10.1371/journal.pone.0207982 November 27, 2018</figDesc>
      </figure>
      <figure xml:id="fig_2">
        <head>Fig 4 .</head>
        <label>4</label>
        <figDesc>Overview of evaluation strategy: A schematic view of our convolutional neural network evaluation strategy. The probability for each augmented image is outputted by our model and averaged for the final evaluation. https://doi.org/10.1371/journal.pone.0207982.g004</figDesc>
      </figure>
      <figure type="table" validated="false" xml:id="tab_1">
        <head>Table 1 .</head>
        <label>1</label>
        <figDesc>Sample numbers for the machine learning. https://doi.org/10.1371/journal.pone.0207982.t001 Fig 1. Logistic classification model architecture: A schematic view of the logistic classification model used in this study. Flattened fundus photography refers to transformation of three-dimensional array photography to a onedimensional array in order to perform logistic regression and produce logits. https://doi.org/10.1371/journal.pone.0207982.g001</figDesc>
        <table>Advanced Glaucoma 
Early Glaucoma 
Normal 
Total 

Entire Data 
467 
289 
786 
1,542 

Training Data 
228 
141 
385 
754 

Validation Data 
98 
61 
165 
324 

Test Data 
141 
87 
236 
464 

</table>
      </figure>
      <figure type="table" validated="true" xml:id="tab_2">
        <head>Table 2 .</head>
        <label>2</label>
        <figDesc>Evaluation of the simple logistic classification model.</figDesc>
        <table>Advanced 
and Early Glaucoma 

Advanced 
Glaucoma only 

Early Glaucoma only 

Training Data 
82.9% 
99.7% 
83.7% 

Test Data 
77.2% 
98.6% 
73% 

https://doi.org/10.1371/journal.pone.0207982.t002 

</table>
      </figure>
      <figure type="table" validated="false" xml:id="tab_3">
        <head>Table 3 .</head>
        <label>3</label>
        <figDesc>Comparison of our own model (convolutional neural network) and the Inception v3 model. https://doi.org/10.1371/journal.pone.0207982.t003 Fig 5. Comparative of classification performance between our Convolutional Neural Network (CNN) model and the Inception v3 model. Receiver Operating Characteristic curve for our CNN model and the transfer-learned Inception v3 model. Blue dotted line represents the training data, red dotted line represents the validation data, and yellow line represents the test data.</figDesc>
        <table>Our new model 
Inception v3 

Raw accuracy 
Average accuracy 
AUROC 
Raw accuracy 
Average accuracy 
AUROC 

Training data 
91.7% 
92.2% 
0.98 
99.7% 
99.7% 
0.99 

Validation data 
88.6% 
88.6% 
0.95 
87.0% 
87.7% 
0.95 

Test Data 
86.9% 
87.9% 
0.94 
83.9% 
84.5% 
0.93 

Number of Epochs 
29 
14 

</table>
      </figure>
      <note place="foot">PLOS ONE | https://doi.org/10.1371/journal.pone.0207982 November 27, 2018</note>
    </body>
    <back>
      <div type="acknowledgement">
        <div/>
      </div>
      <div type="references">
        <listBibl>
          <biblStruct xml:id="b0">
            <analytic>
              <title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Esteva</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Kuprel</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <forename type="middle">A</forename>
                  <surname>Novoa</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Ko</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">M</forename>
                  <surname>Swetter</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <forename type="middle">M</forename>
                  <surname>Blau</surname>
                </persName>
              </author>
              <idno type="DOI">10.1038/nature21056</idno>
              <idno type="PMID">28117445</idno>
              <ptr target="https://doi.org/10.1038/nature21056"/>
            </analytic>
            <monogr>
              <title level="j">Nature</title>
              <imprint>
                <biblScope unit="volume">542</biblScope>
                <biblScope unit="issue">7639</biblScope>
                <biblScope from="115" to="118" unit="page"/>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b1">
            <analytic>
              <title level="a" type="main">Brain-expressed exons under purifying selection are enriched for de novo mutations in autism spectrum disorder</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Uddin</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Tammimies</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Pellecchia</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Alipanahi</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <surname>Hu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Z</forename>
                  <surname>Wang</surname>
                </persName>
              </author>
              <idno type="DOI">10.1038/ng.2980</idno>
              <idno type="PMID">24859339</idno>
              <ptr target="https://doi.org/10.1038/ng.2980"/>
            </analytic>
            <monogr>
              <title level="j">Nat Genet</title>
              <imprint>
                <biblScope unit="volume">46</biblScope>
                <biblScope unit="issue">7</biblScope>
                <biblScope from="742" to="747" unit="page"/>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b2">
            <analytic>
              <title level="a" type="main">Gaussian processes for personalized e-health monitoring with wearable sensors</title>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Clifton</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">A</forename>
                  <surname>Clifton</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">A</forename>
                  <surname>Pimentel</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <forename type="middle">J</forename>
                  <surname>Watkinson</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Tarassenko</surname>
                </persName>
              </author>
              <idno type="DOI">10.1109/TBME.2012.2208459</idno>
              <idno type="PMID">23268532</idno>
              <ptr target="https://doi.org/10.1109/TBME.2012.2208459"/>
            </analytic>
            <monogr>
              <title level="j">IEEE Trans Biomed Eng</title>
              <imprint>
                <biblScope unit="volume">60</biblScope>
                <biblScope unit="issue">1</biblScope>
                <biblScope from="193" to="197" unit="page"/>
                <date type="published" when="2013"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b3">
            <analytic>
              <title level="a" type="main">Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs</title>
              <author>
                <persName>
                  <forename type="first">V</forename>
                  <surname>Gulshan</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Peng</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Coram</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">C</forename>
                  <surname>Stumpe</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Wu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Narayanaswamy</surname>
                </persName>
              </author>
              <idno type="DOI">10.1001/jama.2016.17216</idno>
              <idno type="PMID">27898976</idno>
              <ptr target="https://doi.org/10.1001/jama.2016.17216"/>
            </analytic>
            <monogr>
              <title level="j">JAMA</title>
              <imprint>
                <biblScope unit="volume">316</biblScope>
                <biblScope unit="issue">22</biblScope>
                <biblScope from="2402" to="2410" unit="page"/>
                <date type="published" when="2016"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b4">
            <analytic>
              <title level="a" type="main">Risk Factors, and Visual Features of Undiagnosed Glaucoma: The Singapore Epidemiology of Eye Diseases Study</title>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Chua</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Baskaran</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <forename type="middle">G</forename>
                  <surname>Ong</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Zheng</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <forename type="middle">Y</forename>
                  <surname>Wong</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <surname>Aung</surname>
                </persName>
              </author>
              <idno type="DOI">10.1001/jamaophthalmol.2015.1478</idno>
              <idno type="PMID">26043441</idno>
              <ptr target="https://doi.org/10.1001/jamaophthalmol.2015.1478"/>
            </analytic>
            <monogr>
              <title level="j">JAMA Ophthalmol</title>
              <imprint>
                <biblScope unit="volume">133</biblScope>
                <biblScope unit="issue">8</biblScope>
                <biblScope from="938" to="946" unit="page"/>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b5">
            <analytic>
              <title level="a" type="main">Comparison of machine learning and traditional classifiers in glaucoma diagnosis</title>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Chan</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <forename type="middle">W</forename>
                  <surname>Lee</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <forename type="middle">A</forename>
                  <surname>Sample</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">H</forename>
                  <surname>Goldbaum</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <forename type="middle">N</forename>
                  <surname>Weinreb</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <forename type="middle">J</forename>
                  <surname>Sejnowski</surname>
                </persName>
              </author>
              <idno type="DOI">10.1109/TBME.2002.802012</idno>
              <idno type="PMID">12214886</idno>
              <ptr target="https://doi.org/10.1109/TBME.2002.802012"/>
            </analytic>
            <monogr>
              <title level="j">IEEE Trans Biomed Eng</title>
              <imprint>
                <biblScope unit="volume">49</biblScope>
                <biblScope unit="issue">9</biblScope>
                <biblScope from="963" to="974" unit="page"/>
                <date type="published" when="2002"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b6">
            <analytic>
              <title level="a" type="main">Comparing machine learning classifiers for diagnosing glaucoma from standard automated perimetry</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">H</forename>
                  <surname>Goldbaum</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <forename type="middle">A</forename>
                  <surname>Sample</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Chan</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Williams</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <forename type="middle">W</forename>
                  <surname>Lee</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <surname>Blumenthal</surname>
                </persName>
              </author>
              <idno type="PMID">11773027</idno>
            </analytic>
            <monogr>
              <title level="j">Invest Ophthalmol Vis Sci</title>
              <imprint>
                <biblScope unit="volume">43</biblScope>
                <biblScope unit="issue">1</biblScope>
                <biblScope from="162" to="169" unit="page"/>
                <date type="published" when="2002"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b7">
            <analytic>
              <title level="a" type="main">Machine learning classifiers for glaucoma diagnosis based on classification of retinal nerve fibre layer thickness parameters measured by Stratus OCT</title>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Bizios</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Heijl</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">L</forename>
                  <surname>Hougaard</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Bengtsson</surname>
                </persName>
              </author>
              <idno type="DOI">10.1111/j.1755-3768.2009.01784.x</idno>
              <idno type="PMID">20064122</idno>
              <ptr target="https://doi.org/10.1111/j.1755-3768.2009.01784.x"/>
            </analytic>
            <monogr>
              <title level="j">Acta Ophthalmol</title>
              <imprint>
                <biblScope unit="volume">88</biblScope>
                <biblScope unit="issue">1</biblScope>
                <biblScope from="44" to="52" unit="page"/>
                <date type="published" when="2010"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b8">
            <analytic>
              <title level="a" type="main">Glaucoma Diagnostic Accuracy of Machine Learning Classifiers Using Retinal Nerve Fiber Layer and Optic Nerve Data from SD-OCT</title>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <forename type="middle">A</forename>
                  <surname>Barella</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">V</forename>
                  <forename type="middle">P</forename>
                  <surname>Costa</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Gonçalves</forename>
                  <surname>Vidotti</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">V</forename>
                  <surname>Silva</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">F</forename>
                  <forename type="middle">R</forename>
                  <surname>Dias</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Gomi</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <forename type="middle">S</forename>
                </persName>
              </author>
              <idno type="DOI">10.1155/2013/789129</idno>
              <idno type="PMID">24369495</idno>
              <ptr target="https://doi.org/10.1155/2013/789129"/>
            </analytic>
            <monogr>
              <title level="j">J Ophthalmol</title>
              <imprint>
                <biblScope unit="page">789129</biblScope>
                <date type="published" when="2013"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b9">
            <monogr>
              <title level="m" type="main">Rethinking the Inception Architecture for Computer Vision</title>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Szegedy</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">V</forename>
                  <surname>Vanhoucke</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Ioffe</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Shelns</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Z</forename>
                  <surname>Wojna</surname>
                </persName>
              </author>
              <imprint>
                <date type="published" when="2016"/>
                <biblScope from="2818" to="2826" unit="page"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b10">
            <monogr>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Abadi</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Agarwal</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <surname>Barham</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <surname>Brevdo</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Z</forename>
                  <surname>Chen</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Citro</surname>
                </persName>
              </author>
              <idno>arXiv:160304467</idno>
              <title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
              <imprint>
                <date type="published" when="2016"/>
              </imprint>
            </monogr>
            <note type="report_type">arXiv preprint</note>
          </biblStruct>
          <biblStruct xml:id="b11">
            <analytic>
              <title level="a" type="main">Understanding Data Augmentation for Classification: When to Warp?</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">C</forename>
                  <surname>Wong</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Gatt</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">V</forename>
                  <surname>Stamatescu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">D</forename>
                  <surname>Mcdonnell</surname>
                </persName>
              </author>
              <idno type="DOI">10.1109/DICTA.2016.7797091</idno>
              <ptr target="https://doi.org/10.1109/DICTA.2016.7797091"/>
            </analytic>
            <monogr>
              <title level="m">Int Conf Digit Image Comput Tech Appl DICTA 2016</title>
              <imprint>
                <date type="published" when="2016"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b12">
            <analytic>
              <title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
              <author>
                <persName>
                  <forename type="first">X</forename>
                  <surname>Glorot</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Bengio</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">PMLR</title>
              <imprint>
                <biblScope unit="volume">9</biblScope>
                <biblScope from="249" to="56" unit="page"/>
                <date type="published" when="2010"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b13">
            <monogr>
              <title level="m" type="main">Deep Transfer Learning: A new deep learning glitch classification method for advanced LIGO</title>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>George</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <surname>Shen</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <surname>Huerta</surname>
                </persName>
              </author>
              <idno>arXiv:170607446</idno>
              <imprint>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
            <note type="report_type">arXiv preprint</note>
          </biblStruct>
          <biblStruct xml:id="b14">
            <monogr>
              <title level="m" type="main">Deep Convolutional Neural Network Based Screening And Assessment Of Age-Related Macular Degeneration From Fundus Images</title>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>York</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <forename type="middle">D</forename>
                  <surname>Overview</surname>
                </persName>
              </author>
              <imprint>
                <date type="published" when="2018"/>
                <biblScope from="1525" to="1528" unit="page"/>
              </imprint>
            </monogr>
            <note>Isbi</note>
          </biblStruct>
          <biblStruct xml:id="b15">
            <analytic>
              <title level="a" type="main">Deeply Supervised ResNet</title>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Zhang</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Ubiquitous Intell Comput Adv Trust Comput Scalable Comput Commun Cloud Big Data Comput Internet People Smart City Innov</title>
              <imprint>
                <biblScope from="1" to="6" unit="page"/>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
            <note>IEEE SmartWorld</note>
          </biblStruct>
          <biblStruct xml:id="b16">
            <monogr>
              <title level="m" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Simonyan</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Zisserman</surname>
                </persName>
              </author>
              <idno type="DOI">10.1016/j.infsof.2008.09.005</idno>
              <ptr target="https://doi.org/10.1016/j.infsof.2008.09.005"/>
              <imprint>
                <date type="published" when="2014"/>
                <biblScope from="1" to="14" unit="page"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b17">
            <analytic>
              <title level="a" type="main">Deep residual learning for image steganalysis</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Wu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Zhong</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Liu</surname>
                </persName>
              </author>
              <idno type="DOI">10.1007/s11042-017-4440-4</idno>
              <ptr target="https://doi.org/10.1007/s11042-017-4440-4"/>
            </analytic>
            <monogr>
              <title level="j">Multimed Tools Appl</title>
              <imprint>
                <biblScope unit="volume">2017</biblScope>
                <biblScope from="1" to="17" unit="page"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b18">
            <analytic>
              <title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Krizhevsky</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">I</forename>
                  <surname>Sutskever</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <forename type="middle">E</forename>
                  <surname>Hinton</surname>
                </persName>
              </author>
              <idno type="DOI">10.1016/j.protcy.2014.09.007</idno>
              <ptr target="http://dx.doi.org/10.1016/j.protcy.2014.09.007"/>
            </analytic>
            <monogr>
              <title level="j">Adv Neural Inf Process Syst</title>
              <imprint>
                <biblScope unit="volume">2012</biblScope>
                <biblScope from="1" to="9" unit="page"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b19">
            <analytic>
              <title level="a" type="main">A Deep Learning Algorithm for Prediction of Age-Related Eye Disease Study Severity Scale for Age-Related Macular Degeneration from Color Fundus Photography</title>
              <author>
                <persName>
                  <forename type="first">F</forename>
                  <surname>Grassmann</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Mengelkamp</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Brandl</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Harsch</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">E</forename>
                  <surname>Zimmermann</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Linkohr</surname>
                </persName>
              </author>
              <idno type="DOI">10.1016/j.ophtha.2018.02.037</idno>
              <idno type="PMID">29653860</idno>
              <ptr target="https://doi.org/10.1016/j.ophtha.2018.02.037"/>
            </analytic>
            <monogr>
              <title level="j">Ophthalmology</title>
              <imprint>
                <biblScope from="1" to="11" unit="page"/>
                <date type="published" when="2018"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b20">
            <analytic>
              <title level="a" type="main">Visualizing data using t-SNE</title>
              <author>
                <persName>
                  <forename type="first">Lvd</forename>
                  <surname>Maaten</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Hinton</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Journal of machine learning research</title>
              <imprint>
                <biblScope unit="volume">9</biblScope>
                <biblScope from="2579" to="2605" unit="page"/>
                <date type="published" when="2008-11"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b21">
            <analytic>
              <title level="a" type="main">Development of machine learning models for diagnosis of glaucoma</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">J</forename>
                  <surname>Kim</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <forename type="middle">J</forename>
                  <surname>Cho</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Oh</surname>
                </persName>
              </author>
              <idno type="DOI">10.1371/journal.pone.0177726</idno>
              <idno type="PMID">28542342</idno>
              <ptr target="https://doi.org/10.1371/journal.pone.0177726"/>
            </analytic>
            <monogr>
              <title level="j">PLoS One</title>
              <imprint>
                <biblScope unit="volume">12</biblScope>
                <biblScope unit="issue">5</biblScope>
                <biblScope unit="page">177726</biblScope>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
          </biblStruct>
        </listBibl>
      </div>
    </back>
  </text>
</TEI>
