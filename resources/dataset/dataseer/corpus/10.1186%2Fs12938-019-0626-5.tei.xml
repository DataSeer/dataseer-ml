<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/lopez/grobid/dataseer-ml/../grobid-home/schemas/xsd/Grobid.xsd">
  <teiHeader xml:lang="en">
    <encodingDesc>
      <appInfo>
        <application ident="GROBID" version="0.5.6-SNAPSHOT" when="2019-07-08T06:26+0000">
          <ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
        </application>
      </appInfo>
    </encodingDesc>
    <fileDesc>
      <titleStmt>
        <title level="a" type="main">Two-stage CNNs for computerized BI-RADS categorization in breast ultrasound images</title>
      </titleStmt>
      <publicationStmt>
        <publisher/>
        <availability status="unknown">
          <licence/>
        </availability>
      </publicationStmt>
      <sourceDesc>
        <biblStruct>
          <analytic>
            <author>
              <persName>
                <forename type="first">Yunzhi</forename>
                <surname>Huang</surname>
              </persName>
              <affiliation key="aff0">
                <orgName key="dep1" type="department">Department of Biomedical Engineering</orgName>
                <orgName key="dep2" type="department">College of Materials Science and Engineering</orgName>
                <orgName type="institution">Sichuan University</orgName>
                <address>
                  <postCode>610065</postCode>
                  <settlement>Chengdu</settlement>
                  <country key="CN">China</country>
                </address>
              </affiliation>
              <affiliation key="aff1">
                <orgName type="department">College of Electrical Engineering and Information Technology</orgName>
                <orgName type="institution">Sichuan University</orgName>
                <address>
                  <postCode>610065</postCode>
                  <settlement>Chengdu</settlement>
                  <country key="CN">China</country>
                </address>
              </affiliation>
            </author>
            <author>
              <persName>
                <forename type="first">Luyi</forename>
                <surname>Han</surname>
              </persName>
              <affiliation key="aff1">
                <orgName type="department">College of Electrical Engineering and Information Technology</orgName>
                <orgName type="institution">Sichuan University</orgName>
                <address>
                  <postCode>610065</postCode>
                  <settlement>Chengdu</settlement>
                  <country key="CN">China</country>
                </address>
              </affiliation>
            </author>
            <author>
              <persName>
                <forename type="first">Haoran</forename>
                <surname>Dou</surname>
              </persName>
            </author>
            <author>
              <persName>
                <forename type="first">Honghao</forename>
                <surname>Luo</surname>
              </persName>
            </author>
            <author>
              <persName>
                <forename type="first">Zhen</forename>
                <surname>Yuan</surname>
              </persName>
            </author>
            <author>
              <persName>
                <forename type="first">Qi</forename>
                <surname>Liu</surname>
              </persName>
              <affiliation key="aff1">
                <orgName type="department">College of Electrical Engineering and Information Technology</orgName>
                <orgName type="institution">Sichuan University</orgName>
                <address>
                  <postCode>610065</postCode>
                  <settlement>Chengdu</settlement>
                  <country key="CN">China</country>
                </address>
              </affiliation>
            </author>
            <author>
              <persName>
                <forename type="first">Jiang</forename>
                <surname>Zhang</surname>
              </persName>
              <affiliation key="aff1">
                <orgName type="department">College of Electrical Engineering and Information Technology</orgName>
                <orgName type="institution">Sichuan University</orgName>
                <address>
                  <postCode>610065</postCode>
                  <settlement>Chengdu</settlement>
                  <country key="CN">China</country>
                </address>
              </affiliation>
            </author>
            <author>
              <persName>
                <forename type="first">Guangfu</forename>
                <surname>Yin</surname>
              </persName>
              <affiliation key="aff0">
                <orgName key="dep1" type="department">Department of Biomedical Engineering</orgName>
                <orgName key="dep2" type="department">College of Materials Science and Engineering</orgName>
                <orgName type="institution">Sichuan University</orgName>
                <address>
                  <postCode>610065</postCode>
                  <settlement>Chengdu</settlement>
                  <country key="CN">China</country>
                </address>
              </affiliation>
            </author>
            <title level="a" type="main">Two-stage CNNs for computerized BI-RADS categorization in breast ultrasound images</title>
          </analytic>
          <monogr>
            <imprint>
              <date/>
            </imprint>
          </monogr>
          <idno type="DOI">10.1186/s12938-019-0626-5</idno>
        </biblStruct>
      </sourceDesc>
    </fileDesc>
    <profileDesc>
      <textClass>
        <keywords>
          <term>Breast tumor in ultrasound image</term>
          <term>Breast Imaging Reporting and Data System (BI-RADS)</term>
          <term>Automatic categorization</term>
          <term>Deep convolutional neural network</term>
        </keywords>
      </textClass>
      <abstract>
        <div>
          <head>Abstract</head>
          <p>
            <s>Background: Quantizing the Breast Imaging Reporting and Data System (BI-RADS) criteria into different categories with the single ultrasound modality has always been a challenge.</s>
            <s>To achieve this, we proposed a two-stage grading system to automatically evaluate breast tumors from ultrasound images into five categories based on convolutional neural networks (CNNs).</s>
          </p>
        </div>
        <div>
          <head>Methods:</head>
          <p>
            <s>This new developed automatic grading system was consisted of two stages, including the tumor identification and the tumor grading.</s>
            <s>The constructed network for tumor identification, denoted as ROI-CNN, can identify the region contained the tumor from the original breast ultrasound images.</s>
            <s>The following tumor categorization network, denoted as G-CNN, can generate effective features for differentiating the identified regions of interest (ROIs) into five categories: Category "3", Category "4A", Category "4B", Category "4C", and Category "5".</s>
            <s>Particularly, to promote the predictions identified by the ROI-CNN better tailor to the tumor, refinement procedure based on Level-set was leveraged as a joint between the stage and grading stage.</s>
          </p>
        </div>
        <div>
          <head>Results:</head>
          <p>
            <s>We tested the proposed two-stage grading system against 2238 cases with breast tumors in ultrasound images.</s>
            <s>With the accuracy as an indicator, our automatic computerized evaluation for grading breast tumors exhibited a performance comparable to that of subjective categories determined by physicians.</s>
            <s>Experimental results show that our two-stage framework can achieve the accuracy of 0.998 on Category "3", 0.940 on Category "4A", 0.734 on Category "4B", 0.922 on Category "4C", and 0.876 on Category "5".</s>
          </p>
        </div>
        <div>
          <head>Conclusion:</head>
          <p>
            <s>The proposed scheme can extract effective features from the breast ultrasound images for the final classification of breast tumors by decoupling the identification features and classification features with different CNNs.</s>
            <s>Besides, the proposed scheme can extend the diagnosing of breast tumors in ultrasound images to five subcategories according to BI-RADS rather than merely distinguishing the breast tumor malignant from benign.</s>
          </p>
        </div>
      </abstract>
    </profileDesc>
  </teiHeader>
  <text xml:lang="en">
    <body>
      <div>
        <ref type="bibr">Huang et al. BioMed Eng OnLine (2019)</ref>
        <ref type="bibr">18:8</ref>
        <p>
          <s>and appropriate handcrafted features.</s>
          <s>Recently, through exploiting hierarchical feature representations automatically learned from large-scale dataset, deep learning techniques have successfully addressed numerous medical image analysis problems <ref target="#b17" type="bibr">[18]</ref>
            <ref target="#b18" type="bibr">[19]</ref>
            <ref target="#b19" type="bibr">[20]</ref>
            <ref target="#b20" type="bibr">[21]</ref>
            <ref target="#b21" type="bibr">[22]</ref>
            <ref target="#b22" type="bibr">[23]</ref>
            <ref target="#b23" type="bibr">[24]</ref> .</s>
          <s>Due to the superiority of deep learning in automatic feature extraction, several related works on detecting breast tumors from US images utilized deep learning methods instead of traditional feature engineering <ref target="#b19" type="bibr">[20,</ref>
            <ref target="#b20" type="bibr">21,</ref>
            <ref target="#b22" type="bibr">[23]</ref>
            <ref target="#b23" type="bibr">[24]</ref>
            <ref target="#b24" type="bibr">[25]</ref>
            <ref target="#b25" type="bibr">[26]</ref> .</s>
          <s>For example, Yap et al. <ref target="#b24" type="bibr">[25]</ref> attempted to detect breast ultrasound lesion with different convolutional neural network (CNN) models, including a Patch-based LeNet model, a U-Net model and a transfer learning approach with a pre-trained FCN-AlexNet.</s>
          <s>Bian et al. <ref target="#b25" type="bibr">[26]</ref> developed the detection work on automated whole breast ultrasound (AWBU) with a deep convolutional encoder-decoder network.</s>
          <s>Generally, the detection of breast tumor can preliminarily provide region of interests (ROIs) for the successive tumor classification task.</s>
          <s>More effective tumor/lesion region can guide CNNs to learn better discriminative features for the classification task.</s>
          <s>A few studies have validated the feasibility of classifying breast tumor into different categories with CNNs <ref target="#b22" type="bibr">[23,</ref>
            <ref target="#b26" type="bibr">[27]</ref>
            <ref target="#b27" type="bibr">[28]</ref>
            <ref target="#b28" type="bibr">[29]</ref> .</s>
          <s>Huynh et al. <ref target="#b26" type="bibr">[27]</ref> highlighted that deep learning can be a promising new direction for obtaining "good" features for automatic breast tumor classification by comparing the results with those of typical methodologies (quantized features + typical classifier).</s>
          <s>However, the detailed information about the breast tumor classification network was not provided.</s>
          <s>Zhang et al. <ref target="#b27" type="bibr">[28]</ref> has demonstrated the feasibility of using a CNN in classifying breast tumors with shear wave elastography (SWE).</s>
          <s>Although features from SWE images are helpful in localizing breast tumors, equipping each ultrasound device with SWE is not practical.</s>
          <s>And the image features may involve an abundance of interference because the contour determined by SWE is rather coarse.</s>
          <s>Moreover, the attempt only focused on classifying the BUS images into benign or malignant.</s>
          <s>At present, few studies developed the research on the automatic multi-category classification based on the BI-RADS.</s>
          <s>He et al. <ref target="#b28" type="bibr">[29]</ref> implemented the multi-category classification based on electronic medical records from the aspect of natural language description.</s>
          <s>Clinically, a direct analysis on the tumor's category based on the collected BUS images can better assistant physicians in relieving the diagnose burden.</s>
          <s>Due to abundant noise and interference from other tissues in BUS images, it is a rather challenging task to implement accurate multi-category classification corresponding to the BI-RADS only with the BUS images.</s>
        </p>
        <p>
          <s>In this paper, the systematic research on breast tumor grading is performed based on the CNN architecture.</s>
          <s>To effectively learn the discriminative features of more detailed categories from BUS images, two considerations are taken into our fully automated categorization system: (1) identifying the tumor region from BUS images to reduce the influence of other tissues, and (2) making full use of the features in BUS images to increase the discriminative ability.</s>
          <s>The contributions of this paper are elaborated as follows;</s>
        </p>
        <p>
          <s>1.</s>
          <s>This is the first comprehensive quantized grading system depending on BUS images, which can achieve a 5-score categorization based on BI-RADS, covering Category 3, Category 4a, Category 4b, Category 4c, and Category 5, thus potentially relieving the burden of a tedious image review process and alleviating subjective influence due to physicians' experiences in clinical practice.</s>
        </p>
        <p>
          <s>2. With our two-stage CNNs, features can be decoupled in the detection phase and classification phase, since the weights of the identification task and classification task cannot be well compatible in a one-stage CNN architecture for BUS images.</s>
          <s>Our two-stage system can perform better accuracy than the state-of-art one-stage methods.</s>
        </p>
      </div>
      <div>
        <head>Materials and methods</head>
      </div>
      <div subtype="dataseer">
        <head>Subjects</head>
        <p>
          <s id="0" type="Image:Ultrasonography">In this study, all the collected 2-D breast ultrasound (BUS) images were from female patients and contained a breast tumor.</s>
          <s>For each volunteer participant, only one case corresponding to the maximum cut surface of breast tumor was used to generate the datasets.</s>
          <s>This study included 531 cases of Category 3, 443 cases of Category 4A, 376 cases of Category 4B, 565 cases of Category 4C, and 323 cases of Category 5. Human subject ethical approval was obtained from a relevant committee at West China Hospital of Sichuan University before collecting ultrasound images.</s>
          <s>Each subject provided written consent prior to the research.</s>
          <s id="1" type="Tabular data">Philips IU22 ultrasound scanner (Philips Medical System, Bothell, WA) with a 5-to 12-MHz linear probe was utilized while collecting the data.</s>
        </p>
      </div>
      <div>
        <head>Method overview</head>
        <p>
          <s>The CNN architecture is an extensively utilized deep learning technique for analyzing medical images <ref target="#b17" type="bibr">[18,</ref>
            <ref target="#b29" type="bibr">30]</ref> .</s>
          <s>Typically, a CNN is constructed with several convolution layers <ref target="#b30" type="bibr">[31,</ref>
            <ref target="#b31" type="bibr">32]</ref> , maxpooling layers <ref target="#b32" type="bibr">[33]</ref> , and fully connected layers <ref target="#b33" type="bibr">[34]</ref> .</s>
          <s>And the extensively utilized activation methods in the CNN include the rectified linear unit (ReLU), sigmoid, and tanh <ref target="#b34" type="bibr">[35]</ref> .</s>
          <s>Based on CNN architecture, the schematic illustration of our breast tumor categorization system is exhibited in <ref target="#fig_0" type="figure">Fig. 1</ref> .</s>
          <s>First, all input images were scaled into a uniform size of 288 × 288.</s>
          <s>Second, the ROI-CNN was designed to automatically identify the rough localization of the breast tumor Since the predicted ROIs of the ROI-CNN may mix other non-tumor regions and loss several important texture or boundary information, the following refinement procedure, including area filtering and the perfect Chan-Vese (C-V) level-sets methodology <ref target="#b35" type="bibr">[36]</ref> , was introduced to enable the identified ROI to be better tailored to the real boundary of the breast tumor.</s>
          <s>Finally, the G-CNN model was applied to analyze the refined ROIs by rating them with a score of five, as five categories of breast tumors were involved in the classification.</s>
        </p>
      </div>
      <div>
        <head>CNN-based localization and grading models</head>
        <p>
          <s>Inherent speckle noise and low image contrast of the US images may bring unnecessary distraction while extracting features, thus making the automatic classification of the breast US images difficult.</s>
          <s>To extract effective features for the classification, the tumor identification network (ROI-CNN) and refinement procedure were first exposed on the whole BUS image to determine the effective ROI.</s>
          <s>Then, the following tumor grading network (G-CNN) can focus on extracting the discriminative features for classifying tumors.</s>
        </p>
      </div>
      <div>
        <head>The identification model-ROI-CNN</head>
        <p>
          <s>To effectively reduce the influence of other tissues, like Cooper's ligaments, identifying the tumor from the corresponding whole BUS image is the first and most important procedure for implementing the automatic grading system.</s>
          <s>Our ROI identification network (ROI-CNN) was developed based on the fully convolutional networks (FCN) <ref target="#b36" type="bibr">[37]</ref> .</s>
        </p>
        <p>
          <s>Considering that the tumor size varies among different patients, the designed identification network requires to be robust and effective on the tumor with different size.</s>
          <s>To increase the feasibility of ROI-CNN on variable target size, our designed ROI-CNN introduced a multi-scale architecture based on the typical FCN-16s network.</s>
          <s>Firstly, a typical VGG network (refer to the blue dashed box in <ref target="#fig_1" type="figure">Fig. 2</ref> ) was utilized to extract features.</s>
          <s>After four times down sampling, the output size of feature map from the VGG is 18 × 18, to further extract high level features, we need to compress the size of the feature maps.</s>
        </p>
        <p>
          <s>However, too small size of the feature maps cannot well reflect the detailed boundary information of the breast tumor.</s>
          <s>In this study, a atrous convolution layer (refer to the yellow block in <ref target="#fig_1" type="figure">Fig. 2</ref> ) was then incorporated into our ROI-CNN, which can effectively enlarge the receptive field of filters and capture a larger context without increasing the amount of parameters or the cost of computation <ref target="#b37" type="bibr">[38]</ref>
            <ref target="#b38" type="bibr">[39]</ref>
            <ref target="#b39" type="bibr">[40]</ref> .</s>
          <s>In the atrous convolution layer, the kernel size was set to be 3 × 3 and the dilation rate was set to be 2. Besides, concatenating feature maps from different depths was performed (refer to <ref target="#fig_1" type="figure">Fig. 2</ref> ) to ensure that features with two different receptive fields can be merged together.</s>
          <s>Following the atrous convolution layer, a convolution layer was additionally used as a transitional layer between the atrous convolution layer and the top convolution layer to provide balanced number of features from the deep layer and shallow layer for the concatenation operation.</s>
          <s>In the transitional convolution layer, the kernel size was set to 1 × 1 and the number of filters was set to 512.</s>
          <s>For the output of the ROI-CNN, the predicted identification possibility in the breast tumor region should be higher than the non-tumor region.</s>
        </p>
      </div>
      <div>
        <head>The grading model-G-CNN</head>
        <p>
          <s>Effective classifier can enhance the distinguishing ability of tumor features from different categories, thus promoting the accurate classification.</s>
          <s>Clinically, apart from the inner texture of breast tumors, the texture and the boundary information are also significant for classifying the breast tumors into different grades <ref target="#b9" type="bibr">[10,</ref>
            <ref target="#b13" type="bibr">14]</ref> .</s>
          <s>Therefore, the grading model needs to take the texture and the boundary features into consideration to enhance the expression of the grading features.</s>
          <s>Usually, the texture and the boundary information are well represented in the low-level convolution layers, and the essential features can be well extracted with more convolution layers.</s>
          <s>In our proposed G-CNN, feature maps from different depths were concatenated together to make full use of the low-level and high-level information.</s>
          <s>Referring to <ref type="figure">Fig. 3</ref> , the G-CNN model was consisted of 9 blocks.</s>
          <s>The first four blocks (from Block 1 to Block 4) formed the encode path.</s>
          <s>Each block in the encode path shared the same structure, which contained two convolution layers and one max pooling layer.</s>
          <s>In the encode path, the number of feature channels of the convolution layer was doubled when followed by the max pooling layer.</s>
          <s>Following the encode path is the Block 5 contained of three convolution layers.</s>
          <s>The concatenate path was then followed with the four same blocks (from Block 6 to Block 9).</s>
          <s>Each block in the concatenate path was consisted of a concatenation layer and a convolution layer.</s>
          <s>In our G-CNN model, the feature maps from the lower layer was additionally concatenated with the feature maps from the deeper layer.</s>
          <s>Each block in the encode <ref type="figure">Fig. 3</ref> Illustration of the G-CNN network.</s>
          <s>The blue dashed rectangle box represents the encode path and the orange dashed rectangle box refers to the concatenate path.</s>
          <s>Different colors correspond to different operations.</s>
          <s>Each blue box corresponds to a convolution layer with a kernel size of 3 × 3, each purple box denotes a max pooling layer with a stride of 2, each orange box denotes as a concatenation layer, and each green box represents a fully connected layer path was exploited to provide low-level features for the corresponding block in the concatenate path.</s>
          <s>For example, Block 3 and Block 7 were concatenated together.</s>
          <s>Note that, to ensure the size of the two inputs imported into the concatenation layer was consistent, the first three blocks in the encode path were followed by a convolution layer and a max pooling layer.</s>
          <s>Totally, the G-CNN network contained 18 convolution layers.</s>
          <s>The batch normalization strategy <ref target="#b40" type="bibr">[41]</ref> was encapsulated at the top convolution layer in each block, and the first two FC layers, to regularize the model.</s>
          <s>A L2 regularization operation was performed to reduce overfitting, which can enable better test performance via better generalization.</s>
          <s>The kernel size of all convolution layers was 3 × 3, and each layer was followed by ReLU <ref target="#b33" type="bibr">[34]</ref> .</s>
          <s>All the max pooling layers was set to be 2 × 2 with a stride 2. At the end of the G-CNN were three fully connected (FC) layers that consisted of 4096 neurons, 1024 neurons and 5 neurons.</s>
          <s>A softmax layer followed the topmost FC layer with five neurons to conduct the grading output.</s>
        </p>
      </div>
      <div>
        <head>The refinement</head>
        <p>
          <s>Affected by the ambulant speckle noise and other tissues in the BUS image, the prediction of the ROI-CNN may involve non-tumor region besides the tumor region.</s>
          <s>Moreover, the contour of the predicted region may have a bias from that of the real tumor contour.</s>
          <s>Therefore, additional refinement is imperative to ensure the effectiveness of the predicted ROI.</s>
        </p>
        <p>
          <s>To ensure that only the lesion was export to subsequent grading system and improve the accuracy of the final categorization, the rough ROI from the ROI-CNN, which enclosed the breast tumor region, was then further refined by the following steps: (1) remove the connected domain with a smaller area (smaller than 40% of the max area) and choose the connected region closest to the image center; and (2) refine the boundary with a typical C-V level-sets methodology <ref target="#b35" type="bibr">[36]</ref> .</s>
        </p>
        <p>
          <s>where I is the image, C refers to the boundary of the segmented region, c 1 and c 2 are the respective averages of I inside and outside C , and κ is the curvature of C.</s>
        </p>
      </div>
      <div>
        <head>Implementation details a. Loss function</head>
        <p>
          <s>In the ROI-CNN, the Dice loss function can be expressed as follows, where PRED denotes the predicted ROI, and GT corresponds to the ground truth.</s>
          <s>A PRED and A GT refer to the predicted tumor area and the ground truth tumor area, respectively.</s>
        </p>
        <p>
          <s>In the G-CNN, multi-class cross entropy <ref target="#b41" type="bibr">[42]</ref> was employed as the loss function, where m denotes the number of classes and y is the class label of each input.</s>
          <s>Both variables range from one to five.</s>
          <s>ϑ represents the parameters of the G-CNN, and f ϑ corresponds to the mapping relationship from the input image I to the predicted output f ϑ (I).</s>
        </p>
        <formula xml:id="formula_0">(1) E(C) = µ 1 inside(C) I(x, y) − c 1 2 dxdy + µ 2 outside(C) I(x, y) − c 2 2 dxdy + ακ (2) L ROI = 1 − 2|A PRED | · |A GT | |A PRED | + |A GT |</formula>
        <p>
          <s>In the G-CNN, each input can generate an output vector with size 1 × m, where the category with the highest possibility was taken as the predicted result.</s>
        </p>
      </div>
      <div>
        <head>b. Train process</head>
        <p>
          <s>Our proposed framework was implemented on Tensorflow and all experiments were conducted on a workstation equipped with a 2.40 GHz Intel Xeon E5-2630 CPU and an NVIDIA GF100GL Quadro 4000 GPU.</s>
        </p>
        <p>
          <s>During the training phase of the ROI-CNN, the layers in the blue dotted box (refer to <ref target="#fig_1" type="figure">Fig. 2</ref> ) were initialized with a VGG model <ref target="#b42" type="bibr">[43]</ref> based on a pre-trained image classification dataset provided by ImageNet Large-Scale Visual Recognition Challenge in 2012 (ILS-VRC-2012 CLS).</s>
          <s>The other layers in the ROI-CNN were initialized with a Gaussian randomizer.</s>
          <s>The minibatch size involved 16 images, and the optimizer SGD <ref target="#b43" type="bibr">[44,</ref>
            <ref target="#b44" type="bibr">45]</ref> was set with a learning rate of 0.0001 and a momentum of 0.9 until convergence was attained.</s>
        </p>
        <p>
          <s>In the training phase of the G-CNN, Random initialization was employed to yield better performance and faster convergence.</s>
          <s>16 images were set as the minibatch size, and the SGD optimizer was set with learning rates of 0.001 which would be gradually decreased by a factor of 0.9 until convergence was attained.</s>
        </p>
      </div>
      <div>
        <head>Performance evaluation</head>
        <p>
          <s>To validate the effectiveness of the grading scheme for breast tumors from US images, the localization and grading results were evaluated by comparing the corresponding manual annotations and labeling from the three physicians.</s>
          <s>The experiments implemented two aspects to assess our grading system.</s>
          <s>One was the effect of different options in tumor identification stage on the final grading results, and the other was the discriminative capability for different breast tumor categories.</s>
        </p>
      </div>
      <div>
        <head>The accuracy of the identified tumor</head>
        <p>
          <s>Three metrics were utilized to quantitatively evaluate the similarity between the predicted contour and the ground truth contour, including the Dice similarity coefficient (DSC) <ref target="#b45" type="bibr">[46,</ref>
            <ref type="bibr">47]</ref> , Hausdorff distance between two boundaries (HDist) <ref type="bibr">[47,</ref>
            <ref type="bibr">48]</ref> , and average distance between two boundaries (AvgDist) <ref type="bibr">[47]</ref> .</s>
          <s>DSC was employed to examine the overlapping areas between the two comparisons.</s>
          <s>HDist and AvgDist were exploited to measure the Euclidean distance between a computer-identified tumor boundary and the boundary determined by physicians.</s>
          <s>Higher DSC, lower HDist, and lower AvgDist corresponded to more similarity between the two boundaries.</s>
          <s>Furthermore, AUC values and ROC curves were exploited to evaluate the performance of different experiments with a variable scope of ROIs.</s>
        </p>
        <formula xml:id="formula_1">(3) J (θ) = − 1 m m i=1 y (i) logf ϑ (I (i) ) + (1 − y (i) )log(1 − f ϑ (I (i) ))</formula>
        <p>
          <s>Page</s>
        </p>
        <note type="other">9 of 18 Huang et al. BioMed Eng OnLine (2019) 18:8</note>
      </div>
      <div>
        <head>Experiment configurations</head>
      </div>
      <div>
        <head>Image data involved in each stage of the categorization system</head>
        <p>
          <s>The input size of our two-stage grading system was set to 288 × 288.</s>
          <s>And fivefold crossvalidation was employed to construct the training and testing datasets.</s>
        </p>
        <p>
          <s>• Image annotation</s>
        </p>
        <p>
          <s>Each involved image was scored by three physicians with more than 3 years of experience performing BUS examinations based on the BI-RADS criteria.</s>
          <s>If the physicians differed in their annotations of the category, they discussed and then made consensus on the final category of the breast tumor.</s>
        </p>
      </div>
      <div>
        <head>• Data preprocessing and augmentation</head>
        <p>
          <s>Due to the sample size of volunteer patients is limited, effective data preprocessing and augmentation is imperative for medical image datasets.</s>
          <s>The premise of augmentation is that the ROI must be incorporated into all augmented data regardless of the type of transformation exposure on the dataset.</s>
        </p>
      </div>
      <div>
        <head n="1.">Data augmentation in the ROI-CNN model</head>
        <p>
          <s>In the ROI-CNN training stage, the augmentation times of each input were set to the same with the number of training epochs.</s>
          <s>This type of augmentation can enhance the randomization of input data and reduce the possibility of overfitting of the trained model, thus improving the robustness of the ROI-CNN model.</s>
          <s>Each input image was followed by the subsequent procedures in each calculated epoch, including random brightness, random contrast, random movement, random flip, and standardization.</s>
          <s>Each input can export N times outputs while experiencing N epochs.</s>
          <s>Conversely, in the testing phase, only standardization was exposed using input samples.</s>
        </p>
      </div>
      <div>
        <head n="2.">Data augmentation in the G-CNN model</head>
        <p>
          <s>In the G-CNN training stage, to maintain the shape textures of breast tumors for the final classification, only geometric translation and flipping were involved.</s>
          <s>The original datasets were augmented four times with random movement, in which two augmentations were followed by horizontal flipping.</s>
        </p>
      </div>
      <div>
        <head>Effect of identification accuracy on the final grading</head>
        <p>
          <s>The coverage of localization, which denotes the area of the ROI, theoretically affects the feature mapping and may influence the final grading.</s>
          <s>To investigate the effects of the accuracy of the identified breast tumor on the final categorization from the BUS images, three types of import into the G-CNN with the corresponding experiments were involved and denoted as "No ROI-CNN", "No Refined ROI-CNN", and "Refined ROI-CNN".</s>
          <s>"No ROI-CNN" corresponded to the experiment in which the input to the G-CNN directly applied the C-V level-sets method to input US images and lacked the prediction on the rough localization by the ROI-CNN.</s>
          <s>In the "No Refined ROI-CNN" experiment, the output of the ROI-CNN was not refined and was directly exported to the G-CNN.</s>
          <s>In the "Refined ROI-CNN" experiment, the original US images underwent complete processing procedures in our designed scheme.</s>
          <s>The parameters of our designed method, experiment "Refined ROI-CNN", were set as follows; (1) μ 1 , μ 2 , and α in equation <ref target="#b3" type="bibr">(4)</ref> were all set to 1; (2) the maximum number of contour evolution iterations was set to 50.</s>
          <s>The parameters μ 1 , μ 2 , and α in the C-V levelsets experiment was the same as those in our refined ROI experiment.</s>
          <s>But the maximum number of contour evolution iterations in the "No ROI-CNN" experiment was set to 1000.</s>
        </p>
      </div>
      <div>
        <head>One-stage vs. two-stage categorization of BUS images</head>
        <p>
          <s>Making full use of the effective features is likely to achieve better categorization of breast tumor.</s>
          <s>To investigate the superiority of our two-stage system on grading BUS images, the accuracy of the predicted categorization of tumor was employed as an indicator, we compared the categorization of two-stage grading system with that of the one-stage grading architecture, which directly classified input breast US images into six classes, including the background and five breast tumor categories.</s>
        </p>
        <p>
          <s>There are two types of the two-stage methods, one is with the refinement procedure, and the other is without the refinement procedure.</s>
          <s>In each type of the two-stage methods, we compared our G-CNN model with the other two typical classification network, one is the VGG network <ref target="#b42" type="bibr">[43]</ref> , and the other is the ResNet50 network</s>
        </p>
      </div>
      <div>
        <head>Results</head>
      </div>
      <div>
        <head>Effect of the identification on final grading accuracy</head>
        <p>
          <s>With DSC, AvgDist, and HDist, <ref type="table">Table 2</ref> exhibits the comparison on the similarity of the generated tumor areas from experimental cases "No ROI-CNN", "No Refined ROI-CNN", and "Refined ROI-CNN".</s>
          <s>From <ref type="table">Table 2</ref> , we can observe that the ROI from the experimental case "Refined ROI-CNN" can perform the highest DSC average value and lowest AvgDist average and HDist average, thus enabling the most similarity to the real tumor region of the three listed experiments.</s>
          <s>Compared with "Refined ROI-CNN", the experiment "No Refined ROI-CNN" lacks the refinement procedure to further determine the ROI from BUS images.</s>
          <s>With lower DSC average, higher AvgDist average, and higher HDist average, the similarity of the predicted ROI from the experiment "No</s>
        </p>
      </div>
      <div>
        <head>Table 2 Comparisons of different identification implementations</head>
        <p>
          <s>Experiments "Refined ROI-CNN", "No-Refined ROI", and "No ROI-CNN" were compared with three similarity measurements, including DSC, AvgDist, and HDist Refined ROI-CNN" is less than that of the experiment "Refined ROI-CNN".</s>
          <s>Compared with the CNN-based experimental test cases, experiment "No ROI-CNN" directly identifies the tumor with level-sets, which illustrates the worst performance on the average values of DSC, AvgDist, and HDist.</s>
          <s>This is because image contrast, speckle noise in BUS images, and initial contour, are likely to affect the evolution procedure of C-V levelset, thus locating an erroneous target and resulting in undesirable performance on the predicted contour.</s>
          <s>Therefore, referring to <ref type="table">Table 2</ref> , the similarity between the predicted ROI and the ground truth manually outlined by the three physicians can gradually be increased with every improvement of implementation from the experimental test cases.</s>
          <s>Furthermore, <ref target="#fig_3" type="figure">Fig. 4</ref> exhibits the ROC curves in the grading categories based on different predicted ROIs resulted from the experiments "No ROI-CNN", "No Refined ROI-CNN", and "Refined ROI-CNN".</s>
          <s>Referring to the ROC curves of each category, the best grading results can be achieved with the experiment "Refined ROI-CNN" with the highest AUC.</s>
          <s>In contrast, experiments "No Refined ROI-CNN" and "No ROI-CNN" provide the G-CNN with ROIs of less quality, thus the AUCs are lower.</s>
          <s>Particularly, the lower quality of the ROI, the lower AUC is referring to <ref target="#fig_3" type="figure">Fig. 4</ref> , "No ROI-CNN" performs the worst the AUC in all the experiments by providing the lowest similarity of the ROI (see in <ref type="table">Table 2</ref> ).</s>
          <s>Therefore, based on comparisons of DSC, AvgDist, HDist, and ROC curves, the best performance for localizing ROI can be achieved with the "Refined ROI-CNN".</s>
        </p>
      </div>
      <div>
        <head>One-stage vs. two-stage framework</head>
        <p>
          <s>The grading accuracy for each tumor category of the one-stage and the two-stage experiments is listed in <ref type="table">Table 3</ref> .</s>
          <s>The one-stage methods refer to directly predicting the unified image (288*288) into five categories without the identifying procedure.</s>
          <s>Experiment cases of the one-stage methods are consisted of "One-stage G-CNN", "One-stage VGG", "One-stage ResNet".</s>
          <s>The two-stage methods mean that an extra identification procedure is involved to facilitate the implementation of the classification task.</s>
          <s>There are two types of two-stage methods, one is the with the refinement procedure and the other is without <ref type="table">Table 3</ref> , the one-stage with image-level classification methods ("Onestage G-CNN", "One-stage VGG", and "One-stage ResNet") performs the worst on the average grading accuracy.</s>
          <s>Specially, the predicted accuracy of these methods can only achieve about 0.5 on the "Category 4A", "Category 4B", and "Category 5".</s>
          <s>By introducing an extra identification network, the two-stage methods without refinement ("ROI-CNN + G-CNN", "ROI-CNN + VGG", and "ROI-CNN + ResNet") can have an accuracy improvement on each category compared with the one-stage methods.</s>
          <s>Particularly, expect the "Category 4B", the average accuracy of the other four categories are over 0.8 for the two-stage methods without refinement.</s>
          <s>Among of the listed three experiments ("ROI-CNN + G-CNN", "ROI-CNN + VGG", and "ROI-CNN + ResNet"), the experiment "ROI-CNN + G-CNN" achieves the highest accuracy on all the categories.</s>
          <s>And the experiment "ROI-CNN + VGG" performs the lowest accuracy among of the three methods.</s>
          <s>By successively adding an extra refinement procedure, we can observe that, the grading accuracy in each category of the two-stage methods with refinement procedure ("Refined ROI-CNN + G-CNN", "Refined ROI-CNN + VGG", "Refined ROI-CNN + ResNet") becomes higher than that of the rest of the methods listed in <ref type="table">Table 3</ref> .</s>
          <s>Particularly, in contrast to the typical classification models (VGG <ref target="#b42" type="bibr">[43]</ref> and ResNet50 [49]), the methods with our G-NN network still performs better on the grading accuracy.</s>
        </p>
        <p>
          <s>For the experiment "Refined ROI-CNN + G-CNN", the predicted accuracy in Category "3" can reach an average of 0.998, which is rather close to one.</s>
          <s>Both Category "4A" and Category "4C" can achieve an average accuracy greater than 0.9, and the prediction accuracy for Category "5" can obtain an average of 0.876, which close to 0.9.</s>
          <s>Thus, we suggest that our predictions for the four categories are effective and highly accurate.</s>
          <s>Note that, the average accuracy of the Category "5" is less than 0.9, and the biased predictions for <ref type="table">Table 3</ref> Comparisons of one-stage models and two-stage models with the grading accuracy of each category</s>
        </p>
        <p>
          <s>One-stage models were consisted of experimental test cases "One-stage G-CNN", "One-stage VGG" and "One-stage ResNet".</s>
          <s>Two-stage models involved six experiment test cases, including "Refined ROI-CNN + G-CNN", "Refined ROI-CNN + VGG", "Refined ROI-CNN + ResNet", "ROI-CNN + G-CNN", "ROI-CNN + VGG", and "ROI-CNN + ResNet"</s>
        </p>
        <p>
          <s>The significance for the italic values is to illustrate the method with the best performance according to each evaluated metric the Category "5" are primarily located in Category "4C", due to the following two factors: 1) the number of samples in Category "5" is smaller than the numbers in the other categories, and 2) the ratio of the benign samples in Category "5" is higher than the ratio of the malignant samples in Category "5".</s>
          <s>Although the prediction for Category "4B" cannot approach 0.8, it is higher than the statistical probablity.</s>
          <s>According to the criteria in <ref type="table">Table 1</ref> , for each test tumor belonging to the Category "4B", the probability of benign and malignant is rather close, even the experienced physicians can hardly determine the tumor's category merely from the US image.</s>
          <s>Clinically, extra diagnostic tests such as biopsy, is required to determine the final accurate results.</s>
          <s>With only one type of source associated with US images, our two-stage grading system in <ref target="#fig_0" type="figure">Fig. 1</ref> can achieve the best performance on grading accuracy than the other methods listed in <ref type="table">Table 3</ref> .</s>
        </p>
        <p>
          <s>The grading accuracy predicted by our two-stage categorization system tumors.</s>
          <s>The third row in <ref target="#fig_5" type="figure">Fig. 5</ref> represents the results from sequential refinement procedures on ROIs.</s>
          <s>The green curves represent the ground truth with agreement among the three physicians.</s>
          <s>The blue curves and red curves represent contours from the ROI-CNN and refinement procedure, respectively.</s>
          <s>We can observe that the refinement procedure can slightly improve the contour fitness of the ROI to the real tumor.</s>
          <s>With a suitable and effective ROI exported to the G-CNN, desirable grading result can be achieved.</s>
          <s>The results in the first column of the third row illustrate that refinement is a robust tool for eliminating the disturbance from other tissues.</s>
        </p>
      </div>
      <div>
        <head>Discussion</head>
        <p>
          <s>Automatic quantitation of the category of breast tumor from US scanning can assist physicians in the tedious diagnosing task.</s>
          <s>This is the first comprehensive quantized grading evaluation on breast ultrasound images based on the criteria of BI-RADS.</s>
          <s>With the CNN architecture, which can automatically learn and extract goal-oriented features from images, our two-stage grading system can accurately identify the tumor region and discriminate the category of the breast tumor in ultrasound images.</s>
          <s>Our grading system can achieve a 5-score categorization of BI-RADS, covering Category 3, Category 4a, Category 4b, Category 4c, and Category 5, thus potentially relieving the burden of a time-consuming image review process and alleviating influence due to physicians' experiences in clinical practice.</s>
          <s>Additionally, the proposed categorization system can ensure the robustness and effectiveness of the fully automated categorization system by decoupling the identification features and classification features.</s>
          <s>The proposed two-stage architecture can make full use of the effective features from breast US images by effectively decoupling the information of identification and categorization, thus improving the final grading accuracy.</s>
          <s>The identification task focuses on distinguishing the tumor from the background, while the grading task concentrates on classifying the breast tumors into different classes, so the features used to identify tumor from the background are different from those applied to classify the tumor into five categories.</s>
          <s>Referring to <ref type="table">Table 3</ref> , the accuracy of the final diagnosis illustrates that our twostage CNNs can achieve better performance than the one-stage methods.</s>
          <s>The results in <ref type="table">Table 3</ref> indicates that the two-stage architecture is more suitable for grading BUS images, because the features of the identification task and categorization task cannot be well compatible.</s>
          <s>Therefore, a two-stage grading system can ensure higher accuracy, which is a rather vital indicator of medical image analysis, in classifying breast tumor categories.</s>
        </p>
        <p>
          <s>In the two-stage grading system, the designed identification model and refinement procedure contribute to achieving an effective ROI for the following classification model, thus reaching a desirable grading result.</s>
          <s>Generally, additional irrelevant information imported to the G-CNN models may be translated into interference and produce an unsatisfactory grading result.</s>
          <s>The results in <ref type="table">Table 2</ref> and <ref target="#fig_3" type="figure">Fig. 4</ref> also suggest that with more accurate and precise ROI input to the G-CNN, the better implementations for grading breast tumors are possible.</s>
          <s>Affected by the abundant speckle noise in ultrasound images, the contours resulted from the level-set methods <ref target="#b35" type="bibr">[36]</ref> may occur large bias during the evolution process.</s>
          <s>Therefore, the single refinement procedure cannot generate effective ROIs for the following G-CNN model (refer to <ref type="table">Table 2</ref> ).</s>
          <s>Meanwhile, the predictions from the ROI-CNN are usually smooth in the boundary, some detail information will be lost particularly for the malignant cases, so the single ROI-CNN model is inadequate in providing a desirable ROI for the following classification network.</s>
          <s>Therefore, by combining the ROI-CNN model and the refinement procedure together, the predicted ROI can be closer to the real breast tumor so as to provide more elaborate ROIs for the following symmetric architecture G-CNN.</s>
          <s>
            <ref type="table">Table 3</ref> illustrates that our G-CNN model performs better accuracy than the typical VGG <ref target="#b42" type="bibr">[43]</ref> and RestNet 50 <ref type="bibr">[49]</ref> .</s>
          <s>Generally, the enhancement of effective features can facilitate the final grading accuracy for the classification model.</s>
          <s>In our proposed G-CNN, the layers embedded in the concatenate path and the skip connections (refer to <ref type="figure">Fig. 3</ref> ) can combine the lower dimensional feature maps and the higher dimensional features together, thus promoting discriminability for classifying different breast tumor categories.</s>
          <s>In contrast, the typical classification models, such as VGG <ref target="#b42" type="bibr">[43]</ref> and RestNet 50 <ref type="bibr">[49]</ref> , only involve the encode path in <ref type="figure">Fig. 3</ref> and export the high-level information to implement classification.</s>
          <s>However, the high-level features may suffer a loss of the texture or boundary information contained in the lower convolution layers.</s>
          <s>The texture and the boundary information usually provide important hints for the classification task according to BI-RADS.</s>
          <s>Due to the lack of a compensation strategy, ResNet50 and VGG cannot perform desirable accuracy on classifying the breast ultrasound images.</s>
          <s>Therefore, we conclude that better grading results can be achieved with the enhanced feature maps from G-CNN.</s>
        </p>
        <p>
          <s>Our grading system has a desirable performance on the "Category 3", "Category 4A" and "Category 4C", which can obtain the accuracy greater than 0.9.</s>
          <s>But for the "Category 4B" and the "Category 5", the grading accuracy of are lower than that of the other three categories (refer to <ref type="table">Table 3</ref> ).</s>
          <s>This may be caused by that the data amount of the "Category 4B" and "Category 5" are less than that of the other three categories.</s>
          <s>Although the prediction for Category "4B" cannot approach 0.8, it is significantly higher than manual decision.</s>
          <s>According to the criteria in <ref type="table">Table 1</ref> , each tumor falling into the Category "4B" may have a close probability of being benign or malignant.</s>
          <s>Even the experienced physicians may have biases in determining the category only from the US image.</s>
          <s>Clinically, further diagnostic tests such as biopsy, is needed to achieve the final accurate results.</s>
          <s>With only one type of source associated with US images, our grading scheme can adequately predict the category of breast tumors.</s>
          <s>In the future, we will continue to collect more data, particularly on the "Category 4B" and "Category 5", to further increase the prediction accuracy of our grading system.</s>
          <s>Moreover, we plan to integrate the images in Category 0, 1, 2, and 6 into current grading system, thus developing a comprehensive and complete computerized BI-RADs grading system.</s>
        </p>
      </div>
      <div>
        <head>Conclusion</head>
        <p>
          <s>In this study, we proposed a two-stage automatic categorization system to quantize the criteria of BI-RADS and offer an objective assessment.</s>
          <s>Based on deep learning techniques, a series of comprehensive explorations were conducted using a combination of the procedures of CNN-based methods, typical image processing schemes, and the CNN architecture applicable to breast US images.</s>
          <s>The proposed scheme can also serve as an assistant computerized toolkit for the education of radiology residents and</s>
        </p>
      </div>
      <figure xml:id="fig_0">
        <head>Fig. 1</head>
        <label>1</label>
        <figDesc>Fig. 1 Schematic Illustration of our methodology. The input data was first unified into the same size 288*288. Then, the ROI-CNN identified the tumor region from the breast ultrasound image. The outputs of the ROI-CNN can be further improved by the refinement procedure. Finally, the G-CNN learned the differentiation of the input and rigorously classified the tumor into one of three categories (Category 3, Category 4, and Category 5), where Category 4 could be divided into three subcategories (Category 4A, Category 4B, and Category 4C)</figDesc>
      </figure>
      <figure xml:id="fig_1">
        <head>Fig. 2</head>
        <label>2</label>
        <figDesc>Fig. 2 Illustration of breast tumor identification procedures. The input orange box denotes that the uniform-size images would first undergo the preprocessing and augmentation procedures. The green rectangle refers to the ROI-CNN architecture</figDesc>
      </figure>
      <figure xml:id="fig_2">
        <head/>
        <label/>
        <figDesc>[49]. Totally, there are six experiments in the two-stage methods. For the one-stage classification methods, three experiments are included: (1) experiment "One-stage G-CNN", which directly clas- sified the input into 5 categories with the our proposed G-CNN architecture (refer to Fig. 3); (2) experiment "One-stage VGG", which directly classified the input BUS image into 5 categories with the VGG architecture; (3) experiment "One-stage ResNet", which directly classified the input BUS image into 5 categories with the ResNet50 architecture.</figDesc>
      </figure>
      <figure xml:id="fig_3">
        <head>Fig. 4</head>
        <label>4</label>
        <figDesc>Fig. 4 ROC curve and AUC values of different implementations in grading breast tumors. The implementations encompassed "Refined ROI-CNN", "No refined ROI-CNN", and "No ROI-CNN". a-e Correspond to the results of Category "3", "4A", "4B", "4C", and "5"</figDesc>
      </figure>
      <figure xml:id="fig_4">
        <head>0</head>
        <label>0</label>
        <figDesc>Refined ROI-CNN + G-CNN 0.998 ± 0.0040 0.940 ± 0.0110 0.734 ± 0.0662 0.922 ± 0.0376 0.876 ± 0.1234 Refined ROI-CNN + VGG 0.990 ± 0.0047 0.920 ± 0.0194 0.673 ± 0.0692. 0.908 ± 0.0493 0.841 ± 0.1319 Refined ROI-CNN + ResNet 0.991 ± 0.0052 0.927 ± 0.0155 0.688 ± 0.0665 0.920 ± 0.0388 0.858 ± 0.1423 ROI-CNN + G-CNN 0.955 ± 0.0076 0.897 ± 0.0112 0.679 ± 0.0678 0.906 ± 0.0434 0.837 ± 0.1232 ROI-CNN + VGG 0.947 ± 0.0145 0.864 ± 0.0132 0.667 ± 0.0726 0.865 ± 0.0463 0.818 ± 0.1281 ROI-CNN + ResNet 0.954 ± 0.0136 0.878 ± 0.0124 0.669 ± 0.0664 0.899 ± 0.0425 0.835 ±</figDesc>
      </figure>
      <figure xml:id="fig_5">
        <head>Figure 5</head>
        <label>5</label>
        <figDesc>Figure 5 sequentially illustrates the crucial intermediate results and the final predictions of the proposed methodology in Fig. 1 for several typical cases. The first row contains BUS images with a uniform size. The localization results of the ROI-CNN are presented in the second row in Fig. 5. We discovered that the ROI-CNN can effectively recognize tumors in BUS images. The ROI mapping results are depicted in jet colormaps, where warm color tones indicate a high possibility of tumor prediction and vice versa. The region with the highest possibility appears in the real tumor region (red part in figures). The centric possibilities of the other regions predicted by the ROI-CNN, are much lower than the centric possibility of the actual tumor. Another observation is that the mapping results can adequately reflect the approximate areas of tumors in the US images; however, the contours may not be smooth and located that close to the corresponding</figDesc>
      </figure>
      <figure type="table" validated="false" xml:id="tab_3">
        <head/>
        <label/>
        <figDesc>The significance for the italic values is to illustrate the method with the best performance according to each evaluated metric</figDesc>
        <table>DSC 
AvgDist (pixel) 
HDist (pixel) 

No ROI-CNN 
0.6007 ± 0.0302 
51.2115 ± 2688.8 
82.7174 ± 3423.1 

No-Refined ROI-CNN 
0.8665 ± 0.0133 
7.6764 ± 157.8465 
23.5292 ± 531.9632 

Refined ROI-CNN 
0.9125 ± 0.0015 
3.9668 ± 7.0654 
11.0110 ± 40.6948 

Page 11 of 18 
Huang et al. BioMed Eng OnLine 
(2019) 18:8 

</table>
      </figure>
    </body>
    <back>
      <div type="acknowledgement">
        <div>
          <head>Acknowledgements</head>
          <p>
            <s>The authors would like to thank to the physicians in the Department of Ultrasound, West China Hospital of Sichuan University for their helpful contribution into validating the categories of each collected breast US image.</s>
          </p>
        </div>
        <div>
          <head>Funding</head>
          <p>
            <s>National Natural Science Foundation of China (Grant No. 61273361).</s>
          </p>
        </div>
        <div>
          <head>Publisher's Note</head>
          <p>
            <s>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</s>
          </p>
          <p>
            <s>Received: 6 December 2018 Accepted: 16 January 2019</s>
          </p>
        </div>
      </div>
      <div type="annex">
        <div>
          <head>Page 16 of 18 Huang et al. BioMed Eng OnLine</head>
          <p>
            <s>(2019) <ref type="bibr">18:8</ref> medical students to improve their discriminative skills in breast tumor examination with US scanning.</s>
            <s>Meanwhile, the proposed grading scheme based on CNN can be easily extended to analyses of other breast ultrasound images generated from other equipment without extra feature engineering.</s>
          </p>
        </div>
        <div>
          <head>Authors' contributions</head>
        </div>
        <div>
          <head>Competing interests</head>
          <p>
            <s>The authors declare that they have no competing interests.</s>
          </p>
        </div>
        <div>
          <head>Availability of data and materials</head>
          <p>
            <s>The datasets used and/or analyzed during the current study are available from the corresponding author on reasonable request.</s>
          </p>
        </div>
        <div>
          <head>Consent for publication</head>
          <p>
            <s>Not applicable.</s>
          </p>
        </div>
        <div>
          <head>Ethics approval and consent to participate</head>
          <p>
            <s>This study was approved by the Medical Ethics Committee of the West China Hospital, Sichuan University, and written informed consent was obtained from each participant.</s>
          </p>
          <p>
            <s>Page</s>
          </p>
          <note type="other">17 of 18 Huang et al. BioMed Eng OnLine (2019) 18:8 Page 18 of 18 Huang et al. BioMed Eng OnLine (2019) 18:8</note>
          <p>
            <s>• fast, convenient online submission</s>
          </p>
          <p>
            <s>• thorough peer review by experienced researchers in your field</s>
          </p>
          <p>
            <s>• rapid publication on acceptance</s>
          </p>
          <p>
            <s>• support for research data, including large and complex data types</s>
          </p>
          <p>
            <s>• gold Open Access which fosters wider collaboration and increased citations maximum visibility for your research: over 100M website views per year</s>
          </p>
        </div>
        <div>
          <head>•</head>
          <p>
            <s>At BMC, research is always in progress.</s>
          </p>
        </div>
        <div>
          <head>Learn more biomedcentral.com/submissions</head>
          <p>
            <s>Ready to submit your research ?</s>
            <s>Choose BMC and benefit from:</s>
          </p>
        </div>
      </div>
      <div type="references">
        <listBibl>
          <biblStruct xml:id="b0">
            <analytic>
              <title level="a" type="main">Breast cancer in China</title>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Fan</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Strasser-Weippl</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">J</forename>
                  <surname>Li</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">St</forename>
                  <surname>Louis</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Finkelstein</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">M</forename>
                  <surname>Yu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <forename type="middle">D</forename>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Lancet Oncol</title>
              <imprint>
                <biblScope unit="volume">15</biblScope>
                <biblScope unit="issue">7</biblScope>
                <biblScope from="279" to="89" unit="page"/>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b1">
            <analytic>
              <title level="a" type="main">Cancer facts and figures</title>
            </analytic>
            <monogr>
              <title level="j">Atlanta</title>
              <imprint>
                <date type="published" when="2013"/>
                <publisher>American Cancer Society Atlanta</publisher>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b2">
            <monogr>
              <title level="m" type="main">Global cancer facts &amp; figures</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Center</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <surname>Siegel</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Jemal</surname>
                </persName>
              </author>
              <imprint>
                <date type="published" when="2011"/>
                <publisher>American Cancer Society</publisher>
                <biblScope from="1" to="52" unit="page"/>
                <pubPlace>Atlanta</pubPlace>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b3">
            <analytic>
              <title level="a" type="main">Periodic mammographic follow-up of probably benign lesions: results in 3,184 consecutive cases</title>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <forename type="middle">A</forename>
                  <surname>Sickles</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Radiology</title>
              <imprint>
                <biblScope unit="volume">179</biblScope>
                <biblScope unit="issue">2</biblScope>
                <biblScope from="463" to="471" unit="page"/>
                <date type="published" when="1991"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b4">
            <analytic>
              <title level="a" type="main">Multifocal, multicentric, and contralateral breast cancers: bilateral whole-breast US in the preoperative evaluation of patients</title>
              <author>
                <persName>
                  <forename type="first">W</forename>
                  <forename type="middle">K</forename>
                  <surname>Moon</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D-Y</forename>
                  <surname>Noh</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J-G</forename>
                  <surname>Im</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Radiology</title>
              <imprint>
                <biblScope unit="volume">224</biblScope>
                <biblScope unit="issue">2</biblScope>
                <biblScope from="569" to="76" unit="page"/>
                <date type="published" when="2002"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b5">
            <analytic>
              <title level="a" type="main">Solid breast nodules: use of sonography to distinguish between benign and malignant lesions</title>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <forename type="middle">T</forename>
                  <surname>Stavros</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Thickman</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <forename type="middle">L</forename>
                  <surname>Rapp</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">A</forename>
                  <surname>Dennis</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">H</forename>
                  <surname>Parker</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <forename type="middle">A</forename>
                  <surname>Sisney</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Radiology</title>
              <imprint>
                <biblScope unit="volume">196</biblScope>
                <biblScope unit="issue">1</biblScope>
                <biblScope from="123" to="157" unit="page"/>
                <date type="published" when="1995"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b6">
            <analytic>
              <title level="a" type="main">Breast mass lesions: computer-aided diagnosis models with mammographic and sonographic descriptors</title>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">L</forename>
                  <surname>Jesneck</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">Y</forename>
                  <surname>Lo</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">A</forename>
                  <surname>Baker</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Radiology</title>
              <imprint>
                <biblScope unit="volume">244</biblScope>
                <biblScope unit="issue">2</biblScope>
                <biblScope from="390" to="398" unit="page"/>
                <date type="published" when="2007"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b7">
            <analytic>
              <title level="a" type="main">Approaches for automated detection and classification of masses in mammograms</title>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <surname>Cheng</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">X</forename>
                  <surname>Shi</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <surname>Min</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Hu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">X</forename>
                  <surname>Cai</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <surname>Du</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Pattern Recogn</title>
              <imprint>
                <biblScope unit="volume">39</biblScope>
                <biblScope unit="issue">4</biblScope>
                <biblScope from="646" to="68" unit="page"/>
                <date type="published" when="2006"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b8">
            <analytic>
              <title level="a" type="main">Characterization of solid breast masses</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Costantini</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <surname>Belli</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <surname>Lombardi</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Franceschini</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Mulè</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Bonomo</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J Ultrasound Med</title>
              <imprint>
                <biblScope unit="volume">25</biblScope>
                <biblScope unit="issue">5</biblScope>
                <biblScope from="649" to="59" unit="page"/>
                <date type="published" when="2006"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b9">
            <monogr>
              <title level="m" type="main">ACR BI-RADS ® atlas, breast imaging reporting and data system. Reston: American College of Radiology</title>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <surname>Mendelson</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Böhm-Vélez</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">W</forename>
                  <surname>Berg</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Whitman</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Feldman</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <surname>Madjar</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <surname>Acr Bi-Rads ® Ultrasound</surname>
                </persName>
              </author>
              <imprint>
                <date type="published" when="2013"/>
                <biblScope unit="page">149</biblScope>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b10">
            <analytic>
              <title level="a" type="main">Semi-automatic breast ultrasound image segmentation based on mean shift and graph cuts</title>
              <author>
                <persName>
                  <forename type="first">Z</forename>
                  <surname>Zhou</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">W</forename>
                  <surname>Wu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Wu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">P-H</forename>
                  <surname>Tsui</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C-C</forename>
                  <surname>Lin</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Zhang</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Ultrason Imaging</title>
              <imprint>
                <biblScope unit="volume">36</biblScope>
                <biblScope unit="issue">4</biblScope>
                <biblScope from="256" to="76" unit="page"/>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b11">
            <analytic>
              <title level="a" type="main">Semi-automatic region-of-interest segmentation based computer-aided diagnosis of mass lesions from dynamic contrast-enhanced magnetic resonance imaging based breast cancer screening</title>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Levman</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <surname>Warner</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <surname>Causer</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Martel</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J Digit Imaging</title>
              <imprint>
                <biblScope unit="volume">27</biblScope>
                <biblScope unit="issue">5</biblScope>
                <biblScope from="670" to="678" unit="page"/>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b12">
            <analytic>
              <title level="a" type="main">Completely automated segmentation approach for breast ultrasound images using multiple-domain features</title>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Shan</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <forename type="middle">D</forename>
                  <surname>Cheng</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Wang</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Ultrasound Med Biol</title>
              <imprint>
                <biblScope unit="volume">38</biblScope>
                <biblScope unit="issue">2</biblScope>
                <biblScope from="262" to="75" unit="page"/>
                <date type="published" when="2012"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b13">
            <analytic>
              <title level="a" type="main">Computer-aided diagnosis of breast masses using quantified BI-RADS findings</title>
              <author>
                <persName>
                  <forename type="first">W</forename>
                  <forename type="middle">K</forename>
                  <surname>Moon</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C-M</forename>
                  <surname>Lo</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Cho</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">M</forename>
                  <surname>Chang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C-S</forename>
                  <surname>Huang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J-H</forename>
                  <surname>Chen</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Comput Methods Programs Biomed</title>
              <imprint>
                <biblScope unit="volume">111</biblScope>
                <biblScope unit="issue">1</biblScope>
                <biblScope from="84" to="92" unit="page"/>
                <date type="published" when="2013"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b14">
            <analytic>
              <title level="a" type="main">Ultrasonic multi-feature analysis procedure for computeraided diagnosis of solid breast lesions</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">K</forename>
                  <surname>Alam</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <forename type="middle">J</forename>
                  <surname>Feleppa</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Rondeau</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Kalisz</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <forename type="middle">S</forename>
                  <surname>Garra</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Ultrason Imaging</title>
              <imprint>
                <biblScope unit="volume">33</biblScope>
                <biblScope unit="issue">1</biblScope>
                <biblScope from="17" to="38" unit="page"/>
                <date type="published" when="2011"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b15">
            <analytic>
              <title level="a" type="main">Robustness of computerized lesion detection and classification scheme across different breast US platforms</title>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Drukker</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">L</forename>
                  <surname>Giger</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <forename type="middle">E</forename>
                  <surname>Metz</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Radiology</title>
              <imprint>
                <biblScope unit="volume">237</biblScope>
                <biblScope unit="issue">3</biblScope>
                <biblScope from="834" to="874" unit="page"/>
                <date type="published" when="2005"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b16">
            <analytic>
              <title level="a" type="main">Breast ultrasound computer-aided diagnosis using BI-RADS features</title>
              <author>
                <persName>
                  <forename type="first">W-C</forename>
                  <surname>Shen</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Chang R-F</forename>
                  <surname>Moon</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">W</forename>
                  <forename type="middle">K</forename>
                  <surname>Chou</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y-H</forename>
                  <surname>Huang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C-S</forename>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Acad Radiol</title>
              <imprint>
                <biblScope unit="volume">14</biblScope>
                <biblScope unit="issue">8</biblScope>
                <biblScope from="928" to="967" unit="page"/>
                <date type="published" when="2007"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b17">
            <analytic>
              <title level="a" type="main">Deep learning in medical image analysis</title>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Shen</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Wu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">H-I</forename>
                  <surname>Suk</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Annu Rev Biomed Eng</title>
              <imprint>
                <biblScope unit="volume">19</biblScope>
                <biblScope from="221" to="269" unit="page"/>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b18">
            <analytic>
              <title level="a" type="main">Motion tracking of the carotid artery wall from ultrasound image sequences: a nonlinear state-space approach</title>
              <author>
                <persName>
                  <forename type="first">Z</forename>
                  <surname>Gao</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Li</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Sun</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Yang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <surname>Xiong</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <surname>Zhang</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">IEEE Trans Med Imaging</title>
              <imprint>
                <biblScope unit="volume">37</biblScope>
                <biblScope unit="issue">1</biblScope>
                <biblScope from="273" to="83" unit="page"/>
                <date type="published" when="2018"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b19">
            <analytic>
              <title level="a" type="main">Robust estimation of carotid artery wall motion using the elasticity-based state-space approach</title>
              <author>
                <persName>
                  <forename type="first">Z</forename>
                  <surname>Gao</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <surname>Xiong</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">X</forename>
                  <surname>Liu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <surname>Zhang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Ghista</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">W</forename>
                  <surname>Wu</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Med Image Anal</title>
              <imprint>
                <biblScope unit="volume">37</biblScope>
                <biblScope from="1" to="21" unit="page"/>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b20">
            <analytic>
              <title level="a" type="main">Domain adaptation with conditional transferable components</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Gong</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Zhang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <surname>Liu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Tao</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Glymour</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Schölkopf</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">JMLR workshop and conference proceedings</title>
              <imprint>
                <biblScope unit="volume">48</biblScope>
                <biblScope from="2839" to="2887" unit="page"/>
                <date type="published" when="2016"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b21">
            <monogr>
              <title level="m" type="main">Holistic and deep feature pyramids for saliency detection</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Dong</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Shanhui</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">W</forename>
                  <surname>Xin</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Ming</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Z</forename>
                  <surname>Heye</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Guang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Huafeng</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Shuo</surname>
                </persName>
              </author>
              <imprint>
                <date type="published" when="2018"/>
                <publisher>BMVC</publisher>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b22">
            <analytic>
              <title level="a" type="main">Joint weakly and semi-supervised deep learning for localization and classification of masses in breast ultrasound images</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">Y</forename>
                  <surname>Shin</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Lee</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">I</forename>
                  <forename type="middle">D</forename>
                  <surname>Yun</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">M</forename>
                  <surname>Kim</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <forename type="middle">M</forename>
                  <surname>Lee</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">IEEE transactions on medical imaging</title>
              <imprint>
                <biblScope from="1" to="1" unit="page"/>
                <date type="published" when="2018"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b23">
            <analytic>
              <title level="a" type="main">Tumor detection in automated breast ultrasound using 3-D CNN and prioritized candidate aggregation</title>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <surname>Chiang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Huang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <surname>Chen</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Huang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <surname>Chang</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">IEEE transactions on medical imaging</title>
              <imprint>
                <biblScope from="1" to="1" unit="page"/>
                <date type="published" when="2018"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b24">
            <analytic>
              <title level="a" type="main">Automated breast ultrasound lesions detection using convolutional neural networks</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">H</forename>
                  <surname>Yap</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Pons</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Marti</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Ganau</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Sentis</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <surname>Zwiggelaar</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">IEEE J Biomed Health Inform</title>
              <imprint>
                <biblScope unit="volume">99</biblScope>
                <biblScope unit="page">1</biblScope>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b25">
            <monogr>
              <title level="m" type="main">Boundary regularized convolutional neural network for layer parsing of breast anatomy in automated whole breast ultrasound</title>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Cheng</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Ran</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <forename type="middle">H</forename>
                  <surname>Chou</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">Z</forename>
                  <surname>Cheng</surname>
                </persName>
              </author>
              <imprint>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b26">
            <analytic>
              <title level="a" type="main">06: computer-aided diagnosis of breast ultrasound images using transfer learning from deep convolutional neural networks</title>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Huynh</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Drukker</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Giger</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <surname>Mo-De-207b</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Med Phys</title>
              <imprint>
                <biblScope unit="volume">43</biblScope>
                <biblScope unit="issue">6</biblScope>
                <biblScope unit="page">3705</biblScope>
                <date type="published" when="2016"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b27">
            <analytic>
              <title level="a" type="main">Deep learning based classification of breast tumors with shearwave elastography</title>
              <author>
                <persName>
                  <forename type="first">Q</forename>
                  <surname>Zhang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Xiao</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">W</forename>
                  <surname>Dai</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Suo</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Wang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Shi</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Ultrasonics</title>
              <imprint>
                <biblScope unit="volume">72</biblScope>
                <biblScope from="150" to="157" unit="page"/>
                <date type="published" when="2016"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b28">
            <analytic>
              <title level="a" type="main">Deep learning analytics for diagnostic support of breast cancer disease management</title>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <surname>He</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Puppala</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <surname>Ogunti</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">J</forename>
                  <surname>Mancuso</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">X</forename>
                  <surname>Yu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Chen</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">2017 IEEE EMBS international conference on biomedical &amp; health informatics (BHI</title>
              <imprint>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b29">
            <analytic>
              <title level="a" type="main">Deep learning</title>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Lecun</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Bengio</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Hinton</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Nature</title>
              <imprint>
                <biblScope unit="volume">521</biblScope>
                <biblScope unit="issue">7553</biblScope>
                <biblScope from="436" to="480" unit="page"/>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b30">
            <analytic>
              <title level="a" type="main">Visualizing and understanding convolutional networks</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">D</forename>
                  <surname>Zeiler</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <surname>Fergus</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">European conference on computer vision</title>
              <meeting>
                <address>
                  <addrLine>Berlin</addrLine>
                </address>
              </meeting>
              <imprint>
                <publisher>Springer</publisher>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b31">
            <analytic>
              <title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">D</forename>
                  <surname>Zeiler</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <forename type="middle">W</forename>
                  <surname>Taylor</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <surname>Fergus</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">2011 IEEE international conference on computer vision (ICCV)</title>
              <imprint>
                <date type="published" when="2011"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b32">
            <analytic>
              <title level="a" type="main">Going deeper with convolutions</title>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Szegedy</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Wei</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Yangqing</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <surname>Sermanet</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Reed</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Anguelov</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">2015 IEEE conference on computer vision and pattern recognition (CVPR)</title>
              <imprint>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b33">
            <analytic>
              <title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Krizhevsky</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">I</forename>
                  <surname>Sutskever</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <forename type="middle">E</forename>
                  <surname>Hinton</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">Advances in neural information processing systems</title>
              <imprint>
                <date type="published" when="2012"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b34">
            <analytic>
              <title level="a" type="main">What is the best multi-stage architecture for object recognition?</title>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Jarrett</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Kavukcuoglu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Ranzato</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Lecun</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">2009 IEEE 12th international conference on computer vision</title>
              <imprint>
                <date type="published" when="2009"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b35">
            <analytic>
              <title level="a" type="main">Active contours without edges</title>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <forename type="middle">F</forename>
                  <surname>Chan</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <forename type="middle">A</forename>
                  <surname>Vese</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">IEEE Trans Image Process</title>
              <imprint>
                <biblScope unit="volume">10</biblScope>
                <biblScope unit="issue">2</biblScope>
                <biblScope from="266" to="77" unit="page"/>
                <date type="published" when="2001"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b36">
            <analytic>
              <title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Long</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <surname>Shelhamer</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <surname>Darrell</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">IEEE conference on computer vision and pattern recognition</title>
              <imprint>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b37">
            <analytic>
              <title level="a" type="main">DeepLab: semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <forename type="middle">C</forename>
                  <surname>Chen</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Papandreou</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">I</forename>
                  <surname>Kokkinos</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Murphy</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <forename type="middle">L</forename>
                  <surname>Yuille</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">IEEE Trans Pattern Anal Mach Intell</title>
              <imprint>
                <biblScope unit="volume">40</biblScope>
                <biblScope unit="issue">4</biblScope>
                <biblScope from="834" to="882" unit="page"/>
                <date type="published" when="2018"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b38">
            <monogr>
              <title level="m" type="main">OverFeat: integrated recognition, localization and detection using convolutional networks</title>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <surname>Sermanet</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Eigen</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">X</forename>
                  <surname>Zhang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Mathieu</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <surname>Fergus</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Lecun</surname>
                </persName>
              </author>
              <imprint>
                <date type="published" when="2013"/>
              </imprint>
            </monogr>
            <note type="report_type">Eprint Arxiv</note>
          </biblStruct>
          <biblStruct xml:id="b39">
            <analytic>
              <title level="a" type="main">Modeling local and global deformations in deep learning: epitomic convolution, multiple instance learning, and sliding window detection</title>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Papandreou</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">I</forename>
                  <surname>Kokkinos</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <forename type="middle">A</forename>
                  <surname>Savalle</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">IEEE conference on computer vision and pattern recognition</title>
              <imprint>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b40">
            <analytic>
              <title level="a" type="main">Batch normalization: accelerating deep network training by reducing internal covariate shift</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Ioffe</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Szegedy</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">International conference on machine learning</title>
              <imprint>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b41">
            <analytic>
              <title level="a" type="main">On the pairing of the softmax activation and cross-entropy penalty functions and the derivation of the softmax activation function</title>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <forename type="middle">A</forename>
                  <surname>Dunne</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <forename type="middle">A</forename>
                  <surname>Campbell</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">Proc. 8th Australian conference on the neural networks</title>
              <meeting>8th Australian conference on the neural networks<address>
                  <addrLine>Melbourne</addrLine>
                </address>
              </meeting>
              <imprint>
                <date type="published" when="1997"/>
                <biblScope unit="page">181</biblScope>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b42">
            <monogr>
              <title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Simonyan</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Zisserman</surname>
                </persName>
              </author>
              <idno type="arXiv">arXiv:1409.1556</idno>
              <imprint>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
            <note type="report_type">arXiv preprint</note>
          </biblStruct>
          <biblStruct xml:id="b43">
            <monogr>
              <title level="m" type="main">Towards optimal one pass large scale learning with averaged stochastic gradient descent</title>
              <author>
                <persName>
                  <forename type="first">W</forename>
                  <surname>Xu</surname>
                </persName>
              </author>
              <idno type="arXiv">arXiv:1107.2490</idno>
              <imprint>
                <date type="published" when="2011"/>
              </imprint>
            </monogr>
            <note type="report_type">arXiv preprint</note>
          </biblStruct>
          <biblStruct xml:id="b44">
            <analytic>
              <title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Bottou</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">Proceedings of COMPSTAT'2010</title>
              <meeting>COMPSTAT'2010<address>
                  <addrLine>Berlin</addrLine>
                </address>
              </meeting>
              <imprint>
                <publisher>Springer</publisher>
                <date type="published" when="2010"/>
                <biblScope from="177" to="86" unit="page"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b45">
            <analytic>
              <title level="a" type="main">Measures of the amount of ecologic association between species</title>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <forename type="middle">R</forename>
                  <surname>Dice</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Ecology</title>
              <imprint>
                <biblScope unit="volume">26</biblScope>
                <biblScope unit="issue">3</biblScope>
                <biblScope from="297" to="302" unit="page"/>
                <date type="published" when="1945"/>
              </imprint>
            </monogr>
          </biblStruct>
        </listBibl>
      </div>
    </back>
  </text>
</TEI>
