<TEI xmlns="http://www.tei-c.org/ns/1.0" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /home/lopez/grobid/dataseer-ml/../grobid-home/schemas/xsd/Grobid.xsd">
  <teiHeader xml:lang="en">
    <encodingDesc>
      <appInfo>
        <application ident="GROBID" version="0.5.6-SNAPSHOT" when="2019-11-05T19:51+0000">
          <ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
        </application>
      </appInfo>
    </encodingDesc>
    <fileDesc>
      <titleStmt>
        <title level="a" type="main">Towards reconstructing intelligible speech from the human auditory cortex OPEN</title>
      </titleStmt>
      <publicationStmt>
        <publisher/>
        <availability status="unknown">
          <licence/>
        </availability>
        <date type="published">Published: xx xx xxxx</date>
      </publicationStmt>
      <sourceDesc>
        <biblStruct>
          <analytic>
            <author>
              <persName>
                <forename type="first">Hassan</forename>
                <surname>Akbari</surname>
              </persName>
              <affiliation key="aff0">
                <orgName type="department">Mortimer B. Zuckerman Mind Brain Behavior Institute</orgName>
                <orgName type="institution">Columbia University</orgName>
                <address>
                  <settlement>New York</settlement>
                  <region>NY</region>
                  <country key="US">United States</country>
                </address>
              </affiliation>
              <affiliation key="aff1">
                <orgName type="department">Department of Electrical Engineering</orgName>
                <orgName type="institution">Columbia University</orgName>
                <address>
                  <settlement>New York</settlement>
                  <region>NY</region>
                  <country key="US">United States</country>
                </address>
              </affiliation>
            </author>
            <author>
              <persName>
                <forename type="first">Bahar</forename>
                <surname>Khalighinejad</surname>
              </persName>
              <affiliation key="aff0">
                <orgName type="department">Mortimer B. Zuckerman Mind Brain Behavior Institute</orgName>
                <orgName type="institution">Columbia University</orgName>
                <address>
                  <settlement>New York</settlement>
                  <region>NY</region>
                  <country key="US">United States</country>
                </address>
              </affiliation>
              <affiliation key="aff1">
                <orgName type="department">Department of Electrical Engineering</orgName>
                <orgName type="institution">Columbia University</orgName>
                <address>
                  <settlement>New York</settlement>
                  <region>NY</region>
                  <country key="US">United States</country>
                </address>
              </affiliation>
            </author>
            <author>
              <persName>
                <forename type="first">Jose</forename>
                <forename type="middle">L</forename>
                <surname>Herrero</surname>
              </persName>
              <affiliation key="aff2">
                <orgName type="institution">Hofstra Northwell School of Medicine</orgName>
                <address>
                  <settlement>Manhasset</settlement>
                  <region>NY</region>
                  <country key="US">United States</country>
                </address>
              </affiliation>
            </author>
            <author>
              <persName>
                <forename type="first">Ashesh</forename>
                <forename type="middle">D</forename>
                <surname>Mehta</surname>
              </persName>
              <affiliation key="aff2">
                <orgName type="institution">Hofstra Northwell School of Medicine</orgName>
                <address>
                  <settlement>Manhasset</settlement>
                  <region>NY</region>
                  <country key="US">United States</country>
                </address>
              </affiliation>
            </author>
            <author>
              <persName>
                <forename type="first">Nima</forename>
                <surname>Mesgarani</surname>
              </persName>
              <affiliation key="aff0">
                <orgName type="department">Mortimer B. Zuckerman Mind Brain Behavior Institute</orgName>
                <orgName type="institution">Columbia University</orgName>
                <address>
                  <settlement>New York</settlement>
                  <region>NY</region>
                  <country key="US">United States</country>
                </address>
              </affiliation>
              <affiliation key="aff1">
                <orgName type="department">Department of Electrical Engineering</orgName>
                <orgName type="institution">Columbia University</orgName>
                <address>
                  <settlement>New York</settlement>
                  <region>NY</region>
                  <country key="US">United States</country>
                </address>
              </affiliation>
            </author>
            <title level="a" type="main">Towards reconstructing intelligible speech from the human auditory cortex OPEN</title>
          </analytic>
          <monogr>
            <imprint>
              <date type="published">Published: xx xx xxxx</date>
            </imprint>
          </monogr>
          <idno type="DOI">10.1038/s41598-018-37359-z</idno>
          <note type="submission">Received: 22 June 2018 Accepted: 30 November 2018</note>
          <note>1 SCIENTIFIC RepoRTs | (2019) 9:874 | 4 The Feinstein Institute for Medical Research, Manhasset, NY, United States. Correspondence and requests for materials should be addressed to N.M. (</note>
        </biblStruct>
      </sourceDesc>
    </fileDesc>
    <profileDesc>
      <abstract>
        <div>
          <p>
            <s>Auditory stimulus reconstruction is a technique that finds the best approximation of the acoustic stimulus from the population of evoked neural activity.</s>
            <s>Reconstructing speech from the human auditory cortex creates the possibility of a speech neuroprosthetic to establish a direct communication with the brain and has been shown to be possible in both overt and covert conditions.</s>
            <s>However, the low quality of the reconstructed speech has severely limited the utility of this method for brain-computer interface (BCI) applications.</s>
            <s>To advance the state-of-the-art in speech neuroprosthesis, we combined the recent advances in deep learning with the latest innovations in speech synthesis technologies to reconstruct closed-set intelligible speech from the human auditory cortex.</s>
            <s>We investigated the dependence of reconstruction accuracy on linear and nonlinear (deep neural network) regression methods and the acoustic representation that is used as the target of reconstruction, including auditory spectrogram and speech synthesis parameters.</s>
            <s>In addition, we compared the reconstruction accuracy from low and high neural frequency ranges.</s>
            <s>Our results show that a deep neural network model that directly estimates the parameters of a speech synthesizer from all neural frequencies achieves the highest subjective and objective scores on a digit recognition task, improving the intelligibility by 65% over the baseline method which used linear regression to reconstruct the auditory spectrogram.</s>
            <s>These results demonstrate the efficacy of deep learning and speech synthesis algorithms for designing the next generation of speech BCI systems, which not only can restore communications for paralyzed patients but also have the potential to transform human-computer interaction technologies.</s>
          </p>
        </div>
      </abstract>
    </profileDesc>
  </teiHeader>
  <text xml:lang="en">
    <body>
      <div>
        <p>
          <s>Auditory stimulus reconstruction is an inverse mapping technique that finds the best approximation of the acoustic stimulus from the population of evoked neural activity.</s>
          <s>Stimulus reconstruction was originally proposed as a method to study the representational properties of the neural population <ref target="#b0" type="bibr">[1]</ref>
            <ref target="#b1" type="bibr">[2]</ref>
            <ref target="#b2" type="bibr">[3]</ref>
            <ref target="#b3" type="bibr">[4]</ref>
            <ref target="#b4" type="bibr">[5]</ref> because this method enables the intuitive interpretation of the neural responses in the stimulus domain.</s>
          <s>Reconstructing speech from the neural responses recorded from the human auditory cortex <ref target="#b5" type="bibr">6</ref> , however, opens up the possibility of using this technique as a speech brain-computer interface (BCI) to restore speech in severely paralyzed patients (for a review, see these references <ref target="#b6" type="bibr">[7]</ref>
            <ref target="#b7" type="bibr">[8]</ref>
            <ref target="#b8" type="bibr">[9]</ref> ).</s>
          <s>The ultimate goal of a speech neuroprosthesis is to create a direct communication pathway to the brain with the potential to benefit patients who have lost their ability to speak, which can result from a variety of clinical disorders leading to conditions such as locked-in syndrome <ref target="#b9" type="bibr">10,</ref>
            <ref target="#b10" type="bibr">11</ref> .</s>
          <s>The practicality of using speech decoding methods in a neuroprosthetic device to restore speech communication was further supported by studies showing successful decoding of speech during both overt and covert (imagined) conditions <ref target="#b11" type="bibr">[12]</ref>
            <ref target="#b12" type="bibr">[13]</ref>
            <ref target="#b13" type="bibr">[14]</ref>
            <ref target="#b14" type="bibr">[15]</ref>
            <ref target="#b15" type="bibr">[16]</ref> .</s>
          <s>These studies showed successful decoding of imagined articulations <ref target="#b12" type="bibr">13,</ref>
            <ref target="#b13" type="bibr">14</ref> , imagined word repetition <ref target="#b14" type="bibr">15</ref> , and silent reading of speech <ref target="#b15" type="bibr">16</ref> from auditory cortical areas, including the superior temporal gyrus (STG).</s>
          <s>While previous studies have established the feasibility of reconstructing speech from neural data, the quality of the reconstructed audio so far has been too low to merit subjective evaluation.</s>
          <s>For this reason, the reconstructed sounds in previous studies have been evaluated only using objective measures such as correlation or recognition accuracy <ref target="#b2" type="bibr">3,</ref>
            <ref target="#b5" type="bibr">6,</ref>
            <ref target="#b7" type="bibr">8,</ref>
            <ref target="#b12" type="bibr">13,</ref>
            <ref target="#b16" type="bibr">[17]</ref>
            <ref target="#b17" type="bibr">[18]</ref>
            <ref target="#b18" type="bibr">[19]</ref>
            <ref target="#b19" type="bibr">[20]</ref>
            <ref target="#b20" type="bibr">[21]</ref>
            <ref target="#b21" type="bibr">[22]</ref>
            <ref target="#b22" type="bibr">[23]</ref>
            <ref target="#b23" type="bibr">[24]</ref>
            <ref target="#b24" type="bibr">[25]</ref> .</s>
          <s>The low quality of the reconstructed sound is currently a major limiting factor in actualizing speech BCI systems <ref target="#b6" type="bibr">7</ref> .</s>
        </p>
        <p>
          <s>The acoustic representation of the stimulus that is used as the decoding target can significantly impact the quality and accuracy of reconstructed sounds.</s>
          <s>Previous studies have used magnitude spectrogram (time-frequency representation) <ref target="#b2" type="bibr">3,</ref>
            <ref target="#b19" type="bibr">20</ref> , speech envelope <ref target="#b20" type="bibr">21,</ref>
            <ref target="#b21" type="bibr">22</ref> , spectrotemporal modulation frequencies <ref target="#b5" type="bibr">6,</ref>
            <ref target="#b12" type="bibr">13,</ref>
            <ref target="#b22" type="bibr">23</ref> , and discrete units such as phonemes and phonetic categories <ref target="#b7" type="bibr">8,</ref>
            <ref target="#b16" type="bibr">17,</ref>
            <ref target="#b23" type="bibr">24,</ref>
            <ref target="#b24" type="bibr">25</ref> and words <ref target="#b17" type="bibr">18,</ref>
            <ref target="#b18" type="bibr">19</ref> .</s>
          <s>Using discrete units can be advantageous by allowing for discriminative training.</s>
          <s>However, decoding discrete representations of speech such as phonemes eliminates the paralinguistic information such as speaker features, emotion, and intonation.</s>
          <s>In comparison, reconstructing continuous speech provides the possibility of real-time, continuous feedback that can be delivered to the user to promote coadaptation of the subject and the BCI algorithm <ref target="#b25" type="bibr">26,</ref>
            <ref target="#b26" type="bibr">27</ref> for enhanced accuracy.</s>
          <s>A natural choice is to directly estimate the parameters of a speech synthesizer from neural data, but this has not been attempted previously because the process requires a highly accurate estimation of several vocoder parameters, which is hard to achieve with traditional machine-learning techniques.</s>
        </p>
        <p>
          <s>To advance the state-of-the-art in speech neuroprosthesis, we aimed to increase the intelligibility of the reconstructed speech by combining recent advances in deep learning <ref target="#b27" type="bibr">28</ref> with the latest innovations in speech synthesis technologies.</s>
          <s>Deep learning models have recently become the dominant technique for acoustic and audio signal processing <ref target="#b28" type="bibr">[29]</ref>
            <ref target="#b29" type="bibr">[30]</ref>
            <ref target="#b30" type="bibr">[31]</ref>
            <ref target="#b31" type="bibr">[32]</ref> .</s>
          <s>These models can improve reconstruction accuracy by imposing more complete constraints on the reconstructed audio by better modeling the statistical properties of the speech signal <ref target="#b2" type="bibr">3</ref> .</s>
          <s>At the same time, nonlinear regression can invert the nonlinearly encoded speech features in neural data <ref target="#b32" type="bibr">33,</ref>
            <ref target="#b33" type="bibr">34</ref> more accurately.</s>
        </p>
        <p>
          <s>We examined the effect of three factors on the reconstruction accuracy: 1) the regression technique (linear regression versus nonlinear deep neural network), 2) the representation of the speech intended for reconstruction (auditory spectrogram versus speech vocoder parameters), and 3) the neural frequency range used for regression (low frequency versus high-gamma envelope) <ref target="#fig_0" type="figure">(Fig. 1A)</ref> .</s>
          <s>Our results showed that a deep neural network model that uses all neural frequencies to directly estimate the parameters of a speech vocoder achieves the highest subjective and objective scores, both for intelligibility and the quality of reconstruction in a digit recognition task.</s>
          <s>These results represent an important step toward successful implementation of the next generation of speech BCI systems.</s>
          <s>The population of evoked neural activity in the auditory cortex of the listener was then used to reconstruct the speech stimulus.</s>
          <s>The responsive electrodes in an example subject are shown in red.</s>
          <s>High and low frequency bands were extracted from the neural data.</s>
          <s>Two types of regression models and two types of speech representations were used, resulting in four combinations: linear regression to auditory spectrogram (light blue), linear regression to vocoder (dark blue), DNN to auditory spectrogram, and DNN to vocoder (dark red).</s>
          <s>(B) The input to all models was a 300 ms sliding window containing both low frequency (LF) and the high-gamma envelope (HG).</s>
          <s>The DNN architecture consists of two modules: feature extraction and feature summation networks.</s>
          <s>Feature extraction for auditory spectrogram reconstruction was a fully connected neural network (FCN).</s>
          <s>For vocoder reconstruction, the feature extraction network consisted of an FCN concatenated with a locally connected network (LCN).</s>
          <s>The feature summation network is a two-layer fully connected neural network (FCN).</s>
          <s>(C) Vocoder parameters consist of spectral envelope, fundamental frequency (f0), voicing, and aperiodicity (total of 516 parameters).</s>
          <s>An autoencoder with a bottleneck layer was used to reduce the 516 vocoder parameters to 256.</s>
          <s>The bottleneck features were then used as the target of reconstruction algorithms.</s>
          <s>The vocoder parameters were calculated from the reconstructed bottleneck features using the decoder part of the autoencoder network.</s>
        </p>
      </div>
      <div>
        <head>LF</head>
      </div>
      <div>
        <head>Target Model</head>
      </div>
      <div>
        <head>A. Reconstruction models and targets</head>
      </div>
      <div>
        <head>Results</head>
        <p>
          <s>Neural recordings.</s>
          <s>We used invasive electrocorticography (ECoG) to measure neural activity from five neurosurgical patients undergoing treatment for epilepsy as they listened to continuous speech sounds.</s>
          <s>Two of the five subjects had high-density subdural grid electrodes implanted in the left hemisphere with coverage primarily over the superior temporal gyrus (STG), and four of the five subjects had depth electrodes with coverage of Heschl's gyrus (HG).</s>
          <s>All subjects had self-reported normal hearing.</s>
          <s>Subjects were presented with short continuous stories spoken by four speakers (two females, total duration: 30 minutes).</s>
          <s>To ensure that the subjects were engaged in the task, the stories were randomly paused, and the subjects were asked to repeat the last sentence.</s>
        </p>
        <p>
          <s>The test data consisted of continuous speech sentences and isolated digit sounds.</s>
          <s>We used eight sentences (40 seconds total) to evaluate the objective quality of the reconstruction models.</s>
          <s>The sentences were repeated six times in random order, and the neural data was averaged over the six repetitions to reduce the effect of neural noise on comparison of reconstruction models (see Supp. <ref target="#fig_0" type="figure">Fig. 1</ref> for the effect of averaging).</s>
          <s>The digit sounds were used for subjective intelligibility and quality assessment of reconstruction methods and were taken from a publicly available corpus, TI-46 <ref target="#b34" type="bibr">35</ref> .</s>
          <s>We chose 40 digit sounds (zero to nine), spoken by four speakers (two females) that were not included in the training of the models.</s>
          <s>Reconstructed digits were used as the test set to evaluate subjective intelligibility and quality of the models.</s>
          <s>Two ranges of neural frequencies were used in the study.</s>
          <s>Low-frequency (0-50 Hz) components of the neural data were extracted by filtering the neural signals using a lowpass filter.</s>
          <s>The high-gamma envelope <ref target="#b35" type="bibr">36</ref> was extracted by filtering the neural signals (70 to 150 Hz) and calculating the Hilbert envelope 37 .</s>
        </p>
        <p>
          <s>Regression models.</s>
          <s>The input to the regression models was a sliding window over the neural data with a duration of 300 ms <ref target="#fig_0" type="figure">(Fig. 1B)</ref> , and the hop size of 10 ms.</s>
          <s>The duration of the sliding window was chosen to maximize reconstruction accuracy (Supp.</s>
          <s>
            <ref target="#fig_5" type="figure">Fig. 2)</ref> .</s>
          <s>We compared the performance of linear and nonlinear regression models to reconstruct the stimulus from the neural signals.</s>
          <s>The linear regression finds a linear mapping between the response of a population of neurons to the stimulus representation 3,6 .</s>
          <s>This method effectively assigns a spatiotemporal filter to each electrode estimated by minimizing the mean-squared-error (MSE) between the original and reconstructed stimulus.</s>
          <s>The nonlinear regression model was implemented using a deep neural network (DNN).</s>
          <s>We designed a deep neural network architecture with two stages: (1) feature extraction and (2) feature summation networks <ref target="#b37" type="bibr">[38]</ref>
            <ref target="#b38" type="bibr">[39]</ref>
            <ref target="#b39" type="bibr">[40]</ref>
            <ref target="#fig_0" type="figure">(Fig. 1B)</ref> .</s>
          <s>In this framework, a high-dimensional representation of the input (neural responses) is first calculated, which results in mid-level features (output of the feature extraction network).</s>
          <s>These mid-level features are then input to the feature summation network to regress the output of the model (acoustic representation).</s>
          <s>The feature summation network in all cases was a two-layer fully connected network (FCN) with regularization, dropout 41 , batch normalization 42 , and nonlinearity between each layer.</s>
          <s>For feature extraction, we compared the efficacy of five different network architectures for auditory spectrogram and vocoder reconstruction (Methods, Supp.</s>
          <s>
            <ref type="table">Table 1</ref> for details of each network).</s>
          <s>Specifically, we found that the fully connected network (FCN), in which no constraint was imposed on the connectivity of the nodes in each layer of the network to the previous layer, achieved the best performance for reconstructing the auditory spectrogram.</s>
          <s>However, the combination of the FCN and a locally connected network (LCN), which constrains the connectivity of each node to only a subset of nodes in the previous layer, achieved the highest performance for the vocoder representation (Supp.</s>
          <s>
            <ref type="bibr">Tables 4,</ref>
            <ref target="#b4" type="bibr">5)</ref> .</s>
          <s>In the combined FCN + LCN, the outputs of the two parallel networks are concatenated and used as the mid-level features <ref target="#fig_0" type="figure">(Fig. 1B)</ref> .</s>
        </p>
        <p>
          <s>Acoustic representations.</s>
          <s>We used two types of acoustic representation of the audio as the target for reconstruction: auditory spectrogram and speech vocoder.</s>
          <s>The auditory spectrogram was calculated using a model of the peripheral auditory system <ref target="#b42" type="bibr">43,</ref>
            <ref target="#b43" type="bibr">44</ref> , which estimates a time-frequency representation of the acoustic signal on a tonotopic frequency axis.</s>
          <s>The reconstruction of the waveform from the auditory spectrogram is achieved using an iterative convex optimization procedure <ref target="#b42" type="bibr">43</ref> because the phase of the signal is lost during this procedure.</s>
        </p>
        <p>
          <s>For speech vocoder, we used a vocoder-based, high-quality speech synthesis algorithm (WORLD 45 ), which synthesizes speech from four main parameters: (1) spectral envelope, (2) f0 or fundamental frequency, (3) band aperiodicity, and (4) a voiced-unvoiced (VUV) excitation label <ref target="#fig_0" type="figure">(Fig. 1C)</ref> .</s>
          <s>These parameters are then used to re-synthesize the speech waveform.</s>
          <s>This model can reconstruct high-quality speech and has been shown to outperform other methods including STRAIGHT <ref target="#b45" type="bibr">46</ref> .</s>
          <s>The large numbers of the parameters in the vocoder (516 total) and the susceptibility of the synthesis quality on inaccurate estimation of parameters however pose a challenge.</s>
          <s>To remedy this, we first projected the sparse vocoder parameters onto a dense subspace in which the number of parameters can be reduced, which allows better training with a limited amount of data.</s>
          <s>We used a dimensionality reduction technique that relies on an autoencoder (AEC) <ref target="#b46" type="bibr">47</ref>
            <ref target="#fig_0" type="figure">(Fig. 1C)</ref> , which compresses the vocoder parameters into a smaller space (encoder, 256 dimensions, Supp.</s>
          <s>
            <ref type="table">Table 3</ref> ) and subsequently recovers (decoder) the original vocoder parameters from the compressed features <ref target="#fig_0" type="figure">(Fig. 1C</ref> ).</s>
          <s>The compressed features (also called bottleneck features) are used as the target for the reconstruction network.</s>
          <s>By adding noise to the bottleneck features before feeding them to the decoder during training, we can make the decoder more robust to unwanted variations in amplitude, which is necessary due to the noise inherently present in the neural signals.</s>
          <s>The autoencoder was trained on 80 hours of speech using a separate speech corpus (Wall Street Journal l 48 ).</s>
          <s>During the test phase, we first reconstructed the bottleneck features from the neural data, and subsequently estimated the vocoder parameters using the decoder part of the autoencoder <ref target="#fig_0" type="figure">(Fig. 1C)</ref> .</s>
          <s>The reconstruction accuracy of individual vocoder parameters with a neural network shows varied improvement over the linear model, where pitch estimation is improved the most (%157.2),</s>
          <s>followed by aperiodicity (%18.5),</s>
          <s>spectral envelope (%6.2), and voiced-unvoiced parameter (%0.15,</s>
          <s>Supp.</s>
          <s>
            <ref target="#fig_6" type="figure">Fig. 3</ref> ).</s>
          <s>
            <ref target="#fig_5" type="figure">Figure 2B</ref> shows the example reconstructed auditory spectrograms from each of the four combinations of the regression models (linear regression and DNN) and acoustic representation (auditory spectrogram and vocoder).</s>
          <s>Comparison of the auditory spectrograms in <ref target="#fig_5" type="figure">Fig. 2A</ref> shows that 1) the overall frequency profile of the speech utterance is better preserved by the DNN compared to the linear regression model, and 2) the harmonic structure of speech is recovered only in the DNN-Vocoder model.</s>
          <s>These observations are shown more explicitly in <ref target="#fig_5" type="figure">Fig. 2B</ref> , where the magnitude power of frequency bands is shown during an unvoiced (t = 1.4 sec) and a voiced speech sound (t = 1.15 sec, shown with dashed lines).</s>
          <s>The frequency profile of original and reconstructed auditory spectrograms during the unvoiced sound shows a more accurate reconstruction of low and high frequencies for the DNN models <ref type="figure">(</ref> Subjective evaluation of the reconstruction accuracy.</s>
          <s>We used the reconstructed digit sounds to assess the subjective intelligibility and quality of the reconstructed audio.</s>
          <s>Forty unique tokens were reconstructed from each model, consisting of ten digits (zero to nine) that were spoken by two male and two female speakers.</s>
          <s>The speakers that uttered the digits were different from the speakers that were used in the training, and no digit sound was included in the training of the networks.</s>
          <s>We asked 11 subjects with normal hearing to listen to the reconstructed digits from all four models (160 tokens total) in a random order.</s>
          <s>Each digit was heard only once.</s>
          <s>The subjects then reported the digits (zero to nine, or uncertain), rated the reconstruction quality using the mean opinion score (MOS 49 , on a scale of 1 to 5), and reported the gender of the speaker <ref target="#fig_6" type="figure">(Fig. 3A)</ref> .</s>
          <s>
            <ref target="#fig_6" type="figure">Figure 3B</ref> shows the average reported intelligibility of the digits from the four reconstruction models.</s>
          <s>The DNN-vocoder combination achieved the best performance (75% accuracy), which is 67% higher than the baseline system (Linear regression with auditory spectrogram).</s>
          <s>
            <ref target="#fig_6" type="figure">Figure 3B</ref> also shows that the reconstructions using DNN models are significantly better than the linear regression models (68.5% vs. 47.5%, paired t-test, p &lt; 0.001).</s>
          <s>
            <ref target="#fig_6" type="figure">Figure 3C</ref> shows that the subjects also rated the quality of the reconstruction significantly higher for the DNN-vocoder system than for the other three models (3.4 vs. 2.5, 2.3, and 2.1, unpaired t-test, p &lt; 0.001), meaning that the DNN-vocoder system sounds closest to natural speech.</s>
          <s>The subjects also accurately reported the gender of the speaker significantly higher than chance for the DNN-vocoder system (80%, t-test, p &lt; 0.001) while the performance for all other methods were at chance <ref target="#fig_6" type="figure">(Fig. 3D)</ref> .</s>
          <s>The higher intelligibility and quality scores for the DNN-Voc system was consistently observed in all the ten listeners (Supp.</s>
          <s>
            <ref target="#fig_8" type="figure">Fig. 4</ref> ).</s>
          <s>This result indicates the importance of accurate reconstruction of harmonics frequencies for identifying speaker dependent information, which are best captured by the DNN-Voc model.</s>
        </p>
        <p>
          <s>Finally, <ref target="#fig_6" type="figure">Fig. 3E</ref> shows the confusion patterns in recognizing the digits for the four models, confirming again the advantage of the DNN based models, and the DNN vocoder in particular.</s>
          <s>As shown in <ref target="#fig_6" type="figure">Fig. 3E</ref> , the discriminant acoustic features of the digit sounds are better preserved in the DNN-Voc model, enabling the listeners to correctly differentiate them from the other digits.</s>
          <s>Linear regression models, however, failed to preserve these y c n e u q e r F ) z H K (</s>
        </p>
      </div>
      <div>
        <head>A.</head>
        <p>
          <s>Original cues, as seen by the high confusion among digit sounds.</s>
          <s>The confusion patterns also show that some errors were associated with the shared phonetic features, for example the confusion between digits one and nine (sharing 'ey' phoneme), or four and fine (sharing the initial fricative /f/ phoneme.</s>
          <s>This result suggests a possible strategy for enabling accurate discrimination in BCI applications by selecting target sounds with a sufficient acoustic distance between them.</s>
          <s>The audio samples from different models can be found online 50 and in the supplementary materials.</s>
        </p>
        <p>
          <s>Objective evaluation of reconstructed audio.</s>
          <s>We compared the objective reconstruction accuracy of reconstructed audio per subject using the extended short time objective intelligibility (ESTOI) 51 measure.</s>
          <s>ESTOI is commonly used for the intelligibility assessment of speech synthesis technologies and is calculated by measuring the distortion in spectrotemporal modulation patterns of the noisy speech signal.</s>
          <s>Therefore, ESTOI score is sensitive to both inaccurate reconstruction of the spectral profile and the inconsistencies in the reconstructed temporal patterns.</s>
          <s>The ESTOI measures were calculated from continuous speech sentences in the test set.</s>
          <s>The average ESTOI of the reconstructed speech for all five subjects <ref target="#fig_8" type="figure">(Fig. 4A</ref> ) confirms the results seen from the subjective tests, which is the superiority of DNN based models over the linear model, and that of vocoder reconstruction over the auditory spectrogram (p &lt; 0.001, t-test).</s>
          <s>This pattern was consistent for each of the five subjects in this study, as shown in <ref target="#fig_8" type="figure">Fig. 4B</ref> alongside the electrode locations for each subject.</s>
          <s>While the overall reconstruction accuracy varies significantly across subjects, which is likely due to the difference in the coverage of the auditory cortical areas, the relative performance of the four models was the same in all subjects.</s>
          <s>In addition, averaging the neural responses over multiple repetitions of the same speech utterance improved the reconstruction accuracy (Supp.</s>
          <s>
            <ref target="#fig_0" type="figure">Fig. 1</ref> ) because averaging reduces the effect of neural noise.</s>
        </p>
      </div>
      <div>
        <head>Reconstruction accuracy from low and high neural frequencies.</head>
        <p>
          <s>There is increasing evidence that the low and high-frequency bands encode different and complementary information about the stimulus <ref target="#b50" type="bibr">52</ref> .</s>
          <s>Considering that the sampling frequency of the reconstruction target is 100 Hz, we used 0-50 Hz as a low-frequency signal, and the envelope of high gamma (70-150 Hz) as high-frequency band information.</s>
          <s>To determine what frequency bands are best to include to achieve maximum reconstruction accuracy, we tested the reconstruction accuracy in three conditions, when the regression model uses only the high-gamma envelope, a low-frequency signal, or a combination of the two.</s>
        </p>
        <p>
          <s>To simplify the comparison, we used only the DNN-auditory spectrogram reconstruction model.</s>
          <s>We calculated the ESTOI scores of the reconstructed speech sound using different frequency bands.</s>
          <s>We found that the combination of the two frequency bands significantly outperforms the reconstruction from only one of the frequency bands <ref target="#fig_0" type="figure">(Fig. 5A, p &lt; 0.001, t-test)</ref> .</s>
          <s>This observation is consistent with the complementary encoding of the stimulus features in the low and high-frequency bands <ref target="#b51" type="bibr">53</ref> , which implicates the advantage of using the entire neural signal to achieve the best performance in speech neuroprosthesis applications when it is practically possible.</s>
        </p>
        <p>
          <s>Effect of the number of electrodes and duration of training data.</s>
          <s>The variability of the reconstruction accuracy across subjects <ref target="#fig_8" type="figure">(Fig. 4B)</ref> suggests an important role of neural coverage in improving the reconstruction 3,6 accuracy.</s>
          <s>In addition, because some of the noise signal across different electrodes is independent, reconstruction from a combination of electrodes may lead to a higher accuracy by finding a signal subspace less affected by the noise in the data <ref target="#b52" type="bibr">54</ref> .</s>
          <s>To examine the effect of the number of electrodes on the reconstruction accuracy, we first combined the electrodes of all five subjects and randomly chose N electrodes (N = 1, 2, 4, 8, 16, 32, 64, 128), twenty times for training the individual networks.</s>
          <s>The average reconstruction accuracy for each N was then used for comparison.</s>
          <s>The results shown in <ref target="#fig_9" type="figure">Fig. 5B</ref> indicate that increasing the number of electrodes improves the reconstruction accuracy; however, the rate of improvement decreased significantly.</s>
          <s>Finally, because the success of neural network models is largely attributed to training on large amounts of data 28 , we examined the effect of training duration on reconstruction accuracy.</s>
          <s>We used 128 randomly chosen electrodes and trained several neural network models each on a segment of the training data as the duration of the segments was gradually increased from 10 to 30 minutes.</s>
          <s>This process was performed twenty times for each duration by choosing a random segment of the training data, and the ESTOI score was averaged over the segments.</s>
          <s>As expected, the results show an increased reconstruction accuracy as the duration of the training was increased <ref target="#fig_9" type="figure">(Fig. 5C)</ref> , which indicates the importance of collecting a larger duration of training data when it is practically feasible.</s>
        </p>
      </div>
      <div>
        <head>Discussion</head>
        <p>
          <s>We compared the performance of linear and nonlinear (DNN) regression models in reconstructing the auditory spectrogram and vocoder representation of speech signals.</s>
          <s>We found that using a deep neural network model to regress vocoder parameters significantly outperformed the linear regression and auditory spectrogram representation of speech, and resulted in 75% intelligibility scores on a closed-set, digit recognition task.</s>
        </p>
        <p>
          <s>Our results are consistent with those of previous reconstruction studies that showed the importance of nonlinear techniques in neural decoding <ref target="#b53" type="bibr">55</ref> .</s>
          <s>The previous methods have used support vector machines <ref target="#b12" type="bibr">13,</ref>
            <ref target="#b54" type="bibr">56</ref> , linear discriminant analysis 57,58 , linear regression 3,14,59 , nonlinear embedding 6 , and Bayes classifiers <ref target="#b14" type="bibr">15</ref> .</s>
          <s>In recent years, deep learning 60 has shown tremendous success in many brain-computer interface technologies <ref target="#b59" type="bibr">61</ref> , and our study extended this trend by showing the benefit of deep learning in speech neuroprosthesis research <ref target="#b53" type="bibr">55</ref> .</s>
        </p>
        <p>
          <s>We showed that the reconstruction accuracy depends on both the number of electrodes and the duration of the data that is available for training.</s>
          <s>This is consistent with the findings of studies showing the superior advantage of deep learning models over other techniques, particularly when the amount of training data is large <ref target="#b27" type="bibr">28</ref> .</s>
          <s>We showed that the rate of improvement slows down as the number of electrodes increases.</s>
          <s>This could indicate the limited diversity of the neural responses in our recording which ultimately limits the added information that is gained from additional electrodes.</s>
          <s>Alternatively, increasing the number of electrodes also increases the complexity and the number of free parameters in the neural network model.</s>
          <s>Because the duration of our training data was limited, it is possible that more training data would be needed before the benefit of additional features becomes apparent.</s>
          <s>Our experiments showed that increasing the amount of training data results in better reconstruction accuracy, therefore recording methods that can increase the amount of data available for the training of deep models are highly desirable, for example, when chronic recordings are possible in long-term implantable devices such as the NeuroPace responsive neurostimulation device (RNS) <ref target="#b60" type="bibr">62</ref> .</s>
          <s>We showed that the representation of the acoustic signal used as the target of reconstruction has an important role in the intelligibility and the quality of the reconstructed audio.</s>
          <s>We used a vocoder representation of speech, which extends the previous studies that used a magnitude spectrogram (time-frequency representation) 3,20 , speech envelope <ref target="#b20" type="bibr">21,</ref>
            <ref target="#b21" type="bibr">22</ref> , spectrotemporal modulation frequencies <ref target="#b5" type="bibr">6,</ref>
            <ref target="#b12" type="bibr">13,</ref>
            <ref target="#b22" type="bibr">23</ref> , and discrete units such as phonemes and phonetic categories <ref target="#b7" type="bibr">8,</ref>
            <ref target="#b16" type="bibr">17,</ref>
            <ref target="#b23" type="bibr">24,</ref>
            <ref target="#b24" type="bibr">25</ref> and words <ref target="#b17" type="bibr">18,</ref>
            <ref target="#b18" type="bibr">19</ref> .</s>
          <s>Reconstruction of the auditory spectrogram, which we also used for comparison, inherently results in suboptimal audio quality because the phase of the auditory spectrogram must be approximated.</s>
          <s>The discrete units such as phonemes enable discriminative training by learning a direct map from the neural data to the class labels, which is typically more efficient than generative regression models <ref target="#b61" type="bibr">63</ref> .</s>
          <s>The continuous nature of parameters in acoustic reconstruction however could prove advantageous for BCI applications because they provide a continuous feedback to the user 64 , which is crucial for the subject and the BCI algorithm to coadapt to increase overall effectiveness <ref target="#b25" type="bibr">26,</ref>
            <ref target="#b26" type="bibr">27</ref> .</s>
          <s>Therefore, direct reconstruction of speech synthesis parameters is a natural choice.</s>
          <s>This choice however poses a challenge, since the vocoder quality is very sensitive to the quality of the decoding.</s>
          <s>As we have reported, reconstructing vocoder parameters resulted in both the worst (when used with linear regression) and the best (when used with DNN) results.</s>
          <s>Therefore, powerful modeling techniques such as deep learning are crucial as more inclusive representations of the speech signal are used for reconstruction and decoding applications.</s>
          <s>We proposed a solution to this problem by compressing the acoustic features into a low-dimensional space and using a decoder that is robust to the fluctuations of the input.</s>
        </p>
        <p>
          <s>We found that the combination of low frequency and the envelope of high gamma results in higher reconstruction accuracy than each frequency band alone.</s>
          <s>This finding is consistent with those of studies that have shown the importance of an oscillatory phase 65 in addition to the neural firing rate, which is reflected in the high-gamma frequency band <ref target="#b64" type="bibr">66</ref> .</s>
          <s>Combining both high and low frequencies not only enables access to the complementary information in each band <ref target="#b50" type="bibr">52,</ref>
            <ref target="#b65" type="bibr">67</ref> but also allows the decoder to use the information that is encoded in the interactions between the two bands, such as cross-frequency coupling <ref target="#b51" type="bibr">53</ref> .</s>
          <s>Overall, we observed that better brain coverage, more training data, and combined neural frequency bands result in the best reconstruction accuracy, which can serve as an upper bound performance where practical limitations prevent the use of all possible factors, for example, where the brain coverage is small, or high-frequency neural signals are not accessible such as in noninvasive neuroimaging methods.</s>
        </p>
        <p>
          <s>The application of neural speech decoding in neuroprosthesis is contingent on the similarity of the underlying neural code in overt and covert (imagined) conditions.</s>
          <s>Several previous studies have examined the generalization of decoding techniques from overt to covert speech <ref target="#b11" type="bibr">[12]</ref>
            <ref target="#b12" type="bibr">[13]</ref>
            <ref target="#b13" type="bibr">[14]</ref>
            <ref target="#b14" type="bibr">[15]</ref>
            <ref target="#b15" type="bibr">[16]</ref> and showed the involvement of the auditory cortical areas, including the superior temporal gyrus (STG) in covert speech condition.</s>
          <s>Specifically, informative electrodes for speech decoding were found in Wernicke and the STG during imagined articulation <ref target="#b12" type="bibr">13,</ref>
            <ref target="#b13" type="bibr">14</ref> , covert word repetition <ref target="#b14" type="bibr">15</ref> , and reading silently <ref target="#b15" type="bibr">16</ref> .</s>
          <s>In addition to imagined articulation, an MEG study 12 measured the neural activity during actual and imagined hearing conditions and compared with actual and imagined articulation conditions.</s>
          <s>This study found that the neural activity during overt and covert states were more similar in hearing than in articulation condition.</s>
          <s>Furthermore, the similarity of the response topographies found in covert and overt hearing suggested a similar neural code in the two states, which is also consistent with the findings of fMRI studies showing a similar neural substrate mediating auditory perception and imagery <ref target="#b66" type="bibr">[68]</ref>
            <ref target="#b67" type="bibr">[69]</ref>
            <ref target="#b68" type="bibr">[70]</ref> .</s>
          <s>It is also worth mentioning that the activation of the auditory cortex is not specific to speech imagery, as a recent study found simlilar response patterns also during music perception and imagery <ref target="#b69" type="bibr">71</ref> .</s>
          <s>While these studues have established the feasibility of speech decoding in covert speech perception and production, further research is needed to devise system architectures and training procedures that can optimally fine-tune a model to perform and generalize well in both overt and covert conditions.</s>
          <s>Furthermore, expanding from the closed-set intelligible speech in this work to continuous, open-set, natural intelligible speech requires additional research, which will undoubtedly benefit from a larger amount of training data, higher-resolution neural recording technologies <ref target="#b70" type="bibr">72</ref> , and the adaptation of regression models 73 and the subject to improve the BCI system <ref target="#b25" type="bibr">26,</ref>
            <ref target="#b26" type="bibr">27</ref> .</s>
        </p>
        <p>
          <s>In summary, we present a general framework that can be used for speech neuroprothesis technologies that can result in accurate and intelligible reconstructed speech from the human auditory cortex.</s>
          <s>Our approach takes a step toward the next generation of human-computer interaction systems and more natural communication channels for patients suffering from paralysis and locked-in syndromes.</s>
        </p>
      </div>
      <div subtype="dataseer">
        <head>Materials and Methods</head>
        <p>
          <s>Participants and neural recording.</s>
          <s cert="0.989916980266571" id="dataset-1" type="Tabular data:Subject Data Table">Five patients with pharmacoresistent focal epilepsy were included in this study.</s>
          <s>All subjects underwent chronic intracranial encephalography (iEEG) monitoring at Northshore University Hospital to identify epileptogenic foci in the brain for later removal.</s>
          <s>Three subjects were implanted with only stereo-electroencephalographic (sEEG) depth arrays, one with a high-density grid, and one with both grid and depth electrodes (PMT, Chanhassen, MN, USA).</s>
          <s>The electrodes showing any sign of abnormal epileptiform discharges, as identified in the epileptologists' clinical reports, were excluded from the analysis.</s>
          <s>All included iEEG time series were manually inspected for signal quality and were free from interictal spikes.</s>
          <s>All research protocols were approved and monitored by the institutional review board at the Feinstein Institute for Medical Research, and informed written consent to participate in the research studies was obtained from each subject before electrode implantation.</s>
          <s>All research was performed in accordance with relevant guidelines and regulations.</s>
        </p>
        <p>
          <s cert="0.5591806769371033" id="dataset-2" type="Electroencephalogram">Intracranial EEG (iEEG) signals were acquired continuously at 3 kHz per channel (16-bit precision, range Â± 8 mV, DC) using a data acquisition module (Tucker-Davis Technologies, Alachua, FL, USA).</s>
          <s>Either subdural or skull electrodes were used as references, as dictated by recording quality at the bedside after online visualization of the spectrogram of the signal.</s>
          <s cert="0.43848851323127747" id="dataset-3" type="Electroencephalogram">Speech signals were recorded simultaneously with the iEEG for subsequent offline analysis.</s>
          <s>Two ranges of neural frequencies were used in the study.</s>
          <s>Low-frequency (0-50 Hz) components of the neural data were extracted by filtering the neural signals using an FIR lowpass filter.</s>
          <s>The high-gamma (70-150 Hz) envelope 36 was extracted by first filtering the data into eight frequency bands between 70 and 150 Hz using IIR filters.</s>
          <s>The envelope of each band was then obtained using a Hilbert transform.</s>
          <s>We took the average of envelopes in all frequency bands as the total envelope which was then resampled to 100 Hz.</s>
          <s>The high-gamma responses were normalized based on the responses recorded during a 2-minute silence interval before each recording.</s>
        </p>
        <p>
          <s>Brain maps.</s>
          <s>The electrode positions were mapped to brain anatomy using registration of the post-implant computed tomography (CT) to the pre-implant MRI via the post-op MRI <ref target="#b72" type="bibr">74</ref> .</s>
          <s>After coregistration, the electrodes were identified on the post-implantation CT scan using BioImage Suite 75 .</s>
          <s>Following coregistration, the subdural grid and strip electrodes were snapped to the closest point on the reconstructed brain surface of the pre-implantation MRI.</s>
          <s>We used the FreeSurfer automated cortical parcellation 76 to identify the anatomical regions in which each electrode contact was located within approximately 3 mm resolution (the maximum parcellation error of a given electrode to a parcellated area was &lt;5 voxels/mm).</s>
          <s>We used Destrieux's parcellation because it provides higher specificity in the ventral and lateral aspects of the medial lobe 77 .</s>
          <s>The automated parcellation results for each electrode were closely inspected by a neurosurgeon using the patient's coregistered post-implant MRI.</s>
        </p>
        <p>
          <s>Stimulus.</s>
          <s cert="0.8812764883041382" id="dataset-4" type="Sound data">The speech materials included continuous speech stories recorded in-house by four voice actors and actresses (duration: 30 min, 11,025 Hz sampling rate).</s>
          <s>Eight of the sentences (40 seconds) were used for objective tests and were presented to the patients eight times to improve the signal to noise ratio.</s>
          <s>The digit sounds were taken from the TI-46 corpus <ref target="#b34" type="bibr">35</ref> .</s>
          <s>Two female (f2 and f8) and two male (m2 and m5) speakers were chosen from the corpus, and one token per digit and speaker was used (total of 40 unique tokens).</s>
          <s>Each digit was repeated six times to improve the signal to noise ratio of the neural responses.</s>
          <s>The speakers that uttered the digits were different from the speakers that narrated the stories.</s>
          <s>Acoustic representation.</s>
          <s>The auditory spectrogram representation of speech was calculated from a model of the peripheral auditory system 78 .</s>
          <s>The model consists of three stages: 1) a cochlear filter bank consisting of 128 constant-Q filters equally spaced on a logarithmic axis, 2) a hair cell stage consisting of a low-pass filter and a nonlinear compression function, and 3) a lateral inhibitory network, consisting of a first-order derivative along the spectral axis.</s>
          <s>Finally, the envelope of each frequency band was calculated to obtain a time-frequency representation simulating the pattern of activity on the auditory nerve <ref target="#b76" type="bibr">78</ref> .</s>
          <s>The final auditory spectrogram has a sampling frequency of 100 Hz.</s>
          <s>The audio signal was reconstructed from the auditory spectrogram using an iterative convex optimization procedure <ref target="#b42" type="bibr">43</ref> .</s>
          <s>For the vocoder-based speech synthesizer, we used the WORLD 45 (D4C edition) system.</s>
          <s id="dataset-5" type="Tabular data">In this model, four major speech parameters were estimated, from which the speech waveform was synthesized: (1) spectral envelope, (2) f0 or fundamental frequency, (3) band aperiodicity, and (4) voiced-unvoiced (VUV) excitation label.</s>
          <s>The dimension of each parameter was automatically calculated by the vocoder method and was based on the window size and the sampling frequency of the waveform (16 KHz).</s>
        </p>
        <p>
          <s>SCIENTIFIC RepoRTs | (2019) 9:874 | DOI:10.1038/s41598-018-37359-z</s>
          <s>DNN architecture.</s>
          <s>We used a common deep neural network architecture that consists of two stages: feature extraction and feature summation <ref target="#b37" type="bibr">[38]</ref>
            <ref target="#b38" type="bibr">[39]</ref>
            <ref target="#b39" type="bibr">[40]</ref>
            <ref target="#fig_5" type="figure">(Fig. 2A)</ref> .</s>
          <s>In this framework, a high-dimensional representation of the input is first calculated (feature extraction), which is then used to regress the output of the model (feature summation).</s>
          <s>The feature summation and feature extraction networks are optimized jointly together during the training phase.</s>
          <s>In all models examined, the feature summation step consisted of a two-layer fully connected network (FCN) with L2 regularization, dropout <ref target="#b40" type="bibr">41</ref> , batch normalization 42 , and nonlinearity in each layer.</s>
          <s>We study five different architectures for the feature extraction part of the network: the fully connected network (FCN, also known as the multilayer perceptron or MLP), the locally connected network (LCN) <ref target="#b77" type="bibr">79</ref> , convolutional neural network (CNN) <ref target="#b78" type="bibr">80</ref> , FCN + CNN, and FCN + LCN (for details of each architecture see Supp.</s>
          <s>
            <ref type="table">Table 1</ref> ).</s>
          <s>In the combined networks, we concatenated the output of two parallel paths, which were fed into the summation network.</s>
          <s>For FCN, the windowed neural responses were flattened and fed to a multilayer FCN.</s>
          <s>However, in LCN and CNN, all the extracted features were of the same size as the input, meaning that we did not use flattening, strided convolution, or downsampling prior to the input layer or between the two consecutive layers.</s>
          <s>Instead, the final output of the multilayer LCN or CNN was flattened prior to feeding the output into the feature summation network.</s>
        </p>
        <p>
          <s>The optimal network structure was found separately for the auditory spectrogram and vocoder parameters using an ablation study.</s>
          <s>For auditory spectrogram reconstruction, we directly regressed the 128 frequency bands using a multilayer FCN model for feature extraction (Supp.</s>
          <s>
            <ref type="table">Table 5</ref> ).</s>
          <s>This architecture, however, was not plausible for reconstructing vocoder parameters due to the high-dimensionality and statistical variability of the vocoder parameters.</s>
          <s>To remedy this, we used a deep autoencoder network (AEC) <ref target="#b46" type="bibr">47</ref> to find a compact representation of the 516-dimensional vocoder parameters (consisting of 513 spectral envelopes, pitch, voiced-unvoiced, and band periodicity) <ref target="#b44" type="bibr">45</ref> .</s>
          <s>We confirmed that decoding the AEC features performed significantly better than decoding the vocoder parameters directly (Supp.</s>
          <s>
            <ref type="table">Table 2</ref> ).</s>
          <s>The structure for the proposed deep AEC is illustrated in <ref target="#fig_5" type="figure">Fig. 2D</ref> .</s>
          <s>To carry out decoding, we used a multilayer FCN, in which the number of the nodes changed in a descending (encoder) and then ascending order (decoder) <ref target="#fig_5" type="figure">(Fig. 2C</ref> )(Supp.</s>
          <s>
            <ref type="table">Table 6</ref> ).</s>
          <s>The bottleneck layer of such a network (or the output of the encoder part of the pre-trained AEC) can be used as a low-dimensional reconstruction target by employing the neural network model, from which the vocoder parameters can be estimated using the decoder part of the AEC.</s>
          <s>We chose the number of nodes in the bottleneck layer to be 256, because it maximized both the objective reconstruction accuracy (Supp.</s>
          <s>
            <ref type="table">Table 3)</ref> , and the subjective assessment of the reconstructed sound.</s>
          <s>To increase the robustness to unwanted variations in the encoded features, we used two methods in the bottleneck layer: (1) the hyperbolic tangent function (tanh) was used as a nonlinearity to control the range of the encoded features, and (2) Gaussian noise was added during training prior to feeding into the first layer of the decoder part to make the decoder robust enough to unwanted changes in amplitude resulting from noises in neural responses.</s>
          <s>We confirmed that using additive Gaussian noise in the bottleneck instead of dropout performed significantly better (paired t-test, p &lt; 0.001).</s>
          <s>It is important that we use the same nonlinearity as the bottleneck (tanh) in the output of the main network, since the estimations should be in the same range and space as those in which they were originally coded.</s>
          <s>The best network architecture for decoding the vocoder parameters was found to be the FCN + LCN network (Supp.</s>
          <s>
            <ref type="table">Table 4</ref> ).</s>
        </p>
      </div>
      <div subtype="dataseer">
        <head>DNN training and cross validation.</head>
        <p>
          <s>The networks were implemented in Keras with a Tensorflow backend <ref target="#b79" type="bibr">81</ref> .</s>
          <s>Initialization of the weights was performed using a previously proposed method which was specifically developed for deep multilayer networks with rectified linear units (ReLUs) as their nonlinearities <ref target="#b80" type="bibr">82</ref> .</s>
          <s>It has been shown that using this method helps such networks converge faster.</s>
          <s>We used batch normalization 42 , nonlinearity, and a dropout of p = 0.3 41 between each layer.</s>
          <s>We applied an L2 penalty (with a multiplier weight set to 0.001) on the weights of all the layers in all types of networks (including the AEC).</s>
          <s>However, we found that using additive Gaussian noise in the bottleneck of the AEC instead of dropout and regularization performed significantly better (paired t-test, p &lt; 0.001).</s>
          <s>We used three types of nonlinearities in the networks: (1) LeakyReLU 83 for all layers of AEC except the bottleneck and for all layers of the feature extraction part of the main network, (2) tanh for the output layer of the main network and the bottleneck of the AEC, and (3) the exponential linear unit (ELU) <ref target="#b82" type="bibr">84</ref> for the feature summation network.</s>
          <s>Each epoch of training had a batch size of 256, and optimization was performed using Adam 85 with an initial learning rate of 0.0001, which was reduced by a factor of two if the validation loss did not improve in four consecutive epochs.</s>
          <s>Network training was achieved in 150 epochs and was performed for each subject separately.</s>
          <s>The loss function was a combination of MSE and Pearson's correlation coefficient for each sample: in which y is the actual label (auditory spectrogram or vocoder features) for that sample and Å· is the reconstruction from the output layer of the network.</s>
          <s>The maximum time-lag used was Ï max = 30 ms (Supp.</s>
          <s>
            <ref target="#fig_5" type="figure">Fig. 2</ref> ).</s>
          <s>Because of the higher correlated activity between the neural responses of neighboring electrodes <ref target="#b84" type="bibr">86</ref> , it was important to ensure that the networks can model the local structure in the data.</s>
          <s>Because both CNN and LCN use small receptive fields that take local patterns into account, we retained the spatial organization of the electrode sites in the input to the network, meaning that the electrodes that were close to each other in the brain were arranged to be close together in the input data matrix.</s>
        </p>
        <p>
          <s>Cross validation.</s>
          <s>We trained both the LR model and the DNN models using cross validation.</s>
          <s>We used the speech stories for training all models, and used repeated sentences (separate set from the stories) and digit sounds SCIENTIFIC RepoRTs | (2019) 9:874 | DOI:10.1038/s41598-018-37359-z</s>
          <s>for testing.</s>
          <s>No digit sound was included in the training, and the speakers that uttered the digits were different from those that read the stories.</s>
          <s>The autoencoder network (AEC) was trained on a separate speech corpus (Wall Street Journal, WSJ, 80 hours of read speech) <ref target="#b47" type="bibr">48</ref> .</s>
        </p>
        <p>
          <s>Subjective and objective evaluations.</s>
          <s id="dataset-6" type="Tabular data">We assessed the intelligibility of the reconstructed speech using both subjective and objective tests.</s>
          <s>For subjective assessment, 11 participants with self-reported normal hearing listened to the reconstructed digits using headphones in a quiet environment.</s>
          <s>Each participant listened to 160 tokens including 10 digits, four speakers, and four models.</s>
          <s>The participants were asked to report the digit or to select unsure if the digit was not intelligible.</s>
          <s id="dataset-7" type="Tabular data">In addition, the participants reported the quality of the reconstructed speech using a mean opinion score (MOS): 1 (bad), 2 (poor), 3 (fair), 4 (good), and 5 (excellent).</s>
          <s id="dataset-8" type="Tabular data">The participants also reported the gender of the speaker.</s>
          <s>For objective evaluation, we used the ESTOI measure 51 which is a monaural intelligibility prediction algorithm commonly used in speech enhancement and synthesis research.</s>
          <s>The range of ESTOI measure is between zero (worst) and one (best).</s>
        </p>
        <p>
          <s>Code Availability.</s>
          <s>The codes for performing phoneme analysis, calculating high-gamma envelope, and reconstructing the auditory spectrogram are available at http://naplab.ee.columbia.edu/naplib.html</s>
          <s>87 .</s>
        </p>
      </div>
      <div>
        <head>Data Availability</head>
        <p>
          <s>The data that support the findings of this study are available upon request from the corresponding author <ref type="bibr">[NM]</ref> .</s>
        </p>
      </div>
      <figure xml:id="fig_0">
        <head>Figure 1 .</head>
        <label>1</label>
        <figDesc>Schematic of the speech reconstruction method. (A) Subjects listened to natural speech sentences.</figDesc>
      </figure>
      <figure xml:id="fig_1">
        <head>SCIENTIFIC</head>
        <label/>
        <figDesc>RepoRTs | (2019) 9:874 | DOI:10.1038/s41598-018-37359-z</figDesc>
      </figure>
      <figure xml:id="fig_2">
        <head>SCIENTIFIC</head>
        <label/>
        <figDesc>RepoRTs | (2019) 9:874 | DOI:10.1038/s41598-018-37359-z</figDesc>
      </figure>
      <figure xml:id="fig_3">
        <head/>
        <label/>
        <figDesc>Fig. 2Bleft, comparison of blue and red plots). The comparison of frequency profiles during the voiced sound(Fig. 2B, right) reveals the recovery of speech harmonics only in the DNN-Vocoder model (comparison of top and bottom plots).</figDesc>
      </figure>
      <figure xml:id="fig_5">
        <head>Figure 2 .</head>
        <label>2</label>
        <figDesc>Deep neural network architecture (A) An original auditory spectrogram of a speech sample is shown on top. The reconstructed auditory spectrograms of the four models are shown below. (B) Magnitude power of frequency bands during an unvoiced (t = 1.4 sec) and a voiced speech sound (t = 1.15 sec, shown with dashed lines in A) for original (top) and the four reconstruction models. SCIENTIFIC RepoRTs | (2019) 9:874 | DOI:10.1038/s41598-018-37359-z</figDesc>
      </figure>
      <figure xml:id="fig_6">
        <head>Figure 3 .</head>
        <label>3</label>
        <figDesc>Subjective evaluation of the reconstruction accuracy. (A) The behavioral experiment design used to test the intelligibility and the quality of the reconstructed digits. Eleven subjects listened to digit sounds (zero to nine) spoken by two male and two female speakers. The subjects were asked to report the digit, the quality on the mean-opinion-scale, and the gender of the speaker. (B) The intelligibility score for each model defined as the percentage of correct digits reported by the subject. (C) The quality score on the MOS scale. (D) The speaker gender identification rate for each model. (E) The digit confusion patterns for each of the four models. The DNN vocoder shows the least amount of confusion among the digits. SCIENTIFIC RepoRTs | (2019) 9:874 | DOI:10.1038/s41598-018-37359-z</figDesc>
      </figure>
      <figure xml:id="fig_8">
        <head>Figure 4 .</head>
        <label>4</label>
        <figDesc>Objective intelligibly scores for different models. (A) The average ESTOI score based on all subjects for the four models. (B) Coverage and the location of the electrodes and ESTOI score for each of the five subjects. In all subjects, the ESTOI score of the DNN vocoder was higher than in the other models. SCIENTIFIC RepoRTs | (2019) 9:874 | DOI:10.1038/s41598-018-37359-z</figDesc>
      </figure>
      <figure xml:id="fig_9">
        <head>Figure 5 .</head>
        <label>5</label>
        <figDesc>Effect of neural frequency range, number of electrodes, and stimulus duration on reconstruction accuracy. (A) The reconstruction ESTOI score based on high gamma, low frequency, and high gamma and low frequency combined. (B) The accuracy of reconstruction when the number of electrodes increases from one to 128. For each condition, 20 random subsets were chosen. (C) The accuracy of reconstruction when the duration of the training data increases. Each condition is the average of 20 random subsets. SCIENTIFIC RepoRTs | (2019) 9:874 | DOI:10.1038/s41598-018-37359-z</figDesc>
      </figure>
      <note place="foot">Â© The Author(s) 2019</note>
    </body>
    <back>
      <div type="acknowledgement">
        <div>
          <head>Acknowledgements</head>
          <p>
            <s>We thank James O'Sullivan for providing helpful comments on the manuscript.</s>
            <s>This work was funded by a grant from the National Institutes of Health, NIDCD-DC014279, National Institute of Mental Health, R21MH114166, and the Pew Charitable Trusts, Pew Biomedical Scholars Program.</s>
          </p>
        </div>
      </div>
      <div type="annex">
        <div>
          <head>Author Contributions</head>
        </div>
        <div>
          <head>Additional Information</head>
          <p>
            <s>Supplementary information accompanies this paper at https://doi.org/10.1038/s41598-018-37359-z.</s>
          </p>
        </div>
        <div>
          <head>Competing Interests: The authors declare no competing interests.</head>
          <p>
            <s>Publisher's note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</s>
          </p>
          <p>
            <s>Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</s>
            <s>The images or other third party material in this article are included in the article's Creative Commons license, unless indicated otherwise in a credit line to the material.</s>
            <s>If material is not included in the article's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</s>
            <s>To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.</s>
          </p>
        </div>
      </div>
      <div type="references">
        <listBibl>
          <biblStruct xml:id="b0">
            <analytic>
              <title level="a" type="main">Reading a neural code</title>
              <author>
                <persName>
                  <forename type="first">W</forename>
                  <surname>Bialek</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">F</forename>
                  <surname>Rieke</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <forename type="middle">R</forename>
                  <surname>De Ruyter Van Steveninck</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Warland</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Science</title>
              <imprint>
                <biblScope unit="volume">252</biblScope>
                <biblScope from="1854" to="1857" unit="page"/>
                <date type="published" when="1991"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b1">
            <analytic>
              <title level="a" type="main">Naturalistic stimuli increase the rate and efficiency of information transmission by primary auditory afferents</title>
              <author>
                <persName>
                  <forename type="first">F</forename>
                  <surname>Rieke</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">A</forename>
                  <surname>Bodnar</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">W</forename>
                  <surname>Bialek</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Proc Biol Sci</title>
              <imprint>
                <biblScope unit="volume">262</biblScope>
                <biblScope from="259" to="265" unit="page"/>
                <date type="published" when="1995"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b2">
            <analytic>
              <title level="a" type="main">Influence of context and behavior on stimulus reconstruction from neural activity in primary auditory cortex</title>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Mesgarani</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">V S V</forename>
                  <surname>David</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">B J B</forename>
                  <surname>Fritz</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">A S A</forename>
                  <surname>Shamma</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J Neurophysiol</title>
              <imprint>
                <biblScope unit="volume">102</biblScope>
                <biblScope from="3329" to="3339" unit="page"/>
                <date type="published" when="2009"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b3">
            <analytic>
              <title level="a" type="main">Reconstruction of natural scenes from ensemble responses in the lateral geniculate nucleus</title>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <forename type="middle">B</forename>
                  <surname>Stanley</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">F</forename>
                  <forename type="middle">F</forename>
                  <surname>Li</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Dan</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J Neurosci</title>
              <imprint>
                <biblScope unit="volume">19</biblScope>
                <biblScope from="8036" to="8042" unit="page"/>
                <date type="published" when="1999"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b4">
            <analytic>
              <title level="a" type="main">Incorporating naturalistic correlation structure improves spectrogram reconstruction from neuronal activity in the songbird auditory midbrain</title>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <forename type="middle">D</forename>
                  <surname>Ramirez</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Neurosci</title>
              <imprint>
                <biblScope unit="volume">31</biblScope>
                <biblScope from="3828" to="3842" unit="page"/>
                <date type="published" when="2011"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b5">
            <analytic>
              <title level="a" type="main">Reconstructing speech from human auditory cortex</title>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <forename type="middle">N B N</forename>
                  <surname>Pasley</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">PLoS Biol</title>
              <imprint>
                <biblScope unit="page">10</biblScope>
                <date type="published" when="2012"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b6">
            <analytic>
              <title level="a" type="main">Progress in speech decoding from the electrocorticogram</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Chakrabarti</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <forename type="middle">M</forename>
                  <surname>Sandberg</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">S</forename>
                  <surname>Brumberg</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">J</forename>
                  <surname>Krusienski</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Biomed. Eng. Lett</title>
              <imprint>
                <biblScope unit="volume">5</biblScope>
                <biblScope from="10" to="21" unit="page"/>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b7">
            <analytic>
              <title level="a" type="main">Automatic speech recognition from neural signals: a focused review</title>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Herff</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <surname>Schultz</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Front. Neurosci</title>
              <imprint>
                <biblScope unit="volume">10</biblScope>
                <biblScope unit="page">429</biblScope>
                <date type="published" when="2016"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b8">
            <analytic>
              <title level="a" type="main">Neurolinguistic and machine-learning perspectives on direct speech BCIs for restoration of naturalistic communication</title>
              <author>
                <persName>
                  <forename type="first">O</forename>
                  <surname>Iljina</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Brain-Computer Interfaces</title>
              <imprint>
                <biblScope unit="volume">4</biblScope>
                <biblScope from="186" to="199" unit="page"/>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b9">
            <analytic>
              <title level="a" type="main">The locked-in syndrome: what is it like to be conscious but paralyzed and voiceless? Prog</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Laureys</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Brain Res</title>
              <imprint>
                <biblScope unit="volume">150</biblScope>
                <biblScope from="495" to="611" unit="page"/>
                <date type="published" when="2005"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b10">
            <analytic>
              <title level="a" type="main">Noninvasive brain-computer interface enables communication after brainstem stroke</title>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <forename type="middle">W</forename>
                  <surname>Sellers</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">B</forename>
                  <surname>Ryan</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <forename type="middle">K</forename>
                  <surname>Hauser</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Sci. Transl. Med</title>
              <imprint>
                <biblScope unit="volume">6</biblScope>
                <biblScope from="257" to="264" unit="page"/>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b11">
            <analytic>
              <title level="a" type="main">Mental imagery of speech and movement implicates the dynamics of internal forward models</title>
              <author>
                <persName>
                  <forename type="first">X</forename>
                  <surname>Tian</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Poeppel</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Front. Psychol</title>
              <imprint>
                <biblScope unit="volume">1</biblScope>
                <biblScope unit="page">166</biblScope>
                <date type="published" when="2010"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b12">
            <analytic>
              <title level="a" type="main">Word pair classification during imagined speech using direct brain recordings</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Martin</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Sci. Rep</title>
              <imprint>
                <biblScope unit="volume">6</biblScope>
                <biblScope unit="page">25803</biblScope>
                <date type="published" when="2016"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b13">
            <analytic>
              <title level="a" type="main">Using the electrocorticographic speech network to control a brain-computer interface in humans</title>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <forename type="middle">C</forename>
                  <surname>Leuthardt</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Neural Eng</title>
              <imprint>
                <biblScope unit="volume">8</biblScope>
                <biblScope unit="page">36004</biblScope>
                <date type="published" when="2011"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b14">
            <analytic>
              <title level="a" type="main">Decoding vowels and consonants in spoken and imagined words using electrocorticographic signals in humans</title>
              <author>
                <persName>
                  <forename type="first">X</forename>
                  <surname>Pei</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">L</forename>
                  <surname>Barbour</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <forename type="middle">C</forename>
                  <surname>Leuthardt</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Schalk</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Neural Eng</title>
              <imprint>
                <biblScope unit="volume">8</biblScope>
                <biblScope unit="page">46028</biblScope>
                <date type="published" when="2011"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b15">
            <analytic>
              <title level="a" type="main">Decoding spectrotemporal features of overt and covert speech from the human cortex</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Martin</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Front. Neuroeng</title>
              <imprint>
                <biblScope unit="volume">7</biblScope>
                <biblScope unit="page">14</biblScope>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b16">
            <analytic>
              <title level="a" type="main">Low-Frequency Cortical Entrainment to Speech Reflects Phoneme-Level Processing</title>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <forename type="middle">M</forename>
                  <surname>Di Liberto</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">A</forename>
                  <surname>O&amp;apos;sullivan</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <forename type="middle">C</forename>
                  <surname>Lalor</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Curr. Biol</title>
              <imprint>
                <biblScope unit="volume">25</biblScope>
                <biblScope from="2457" to="2465" unit="page"/>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b17">
            <analytic>
              <title level="a" type="main">Decoding spoken words using local field potentials recorded from the cortical surface</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Kellis</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Neural Eng</title>
              <imprint>
                <biblScope unit="volume">7</biblScope>
                <biblScope unit="page">56007</biblScope>
                <date type="published" when="2010"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b18">
            <analytic>
              <title level="a" type="main">Brain-to-text: decoding spoken phrases from phone representations in the brain</title>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Herff</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Front. Neurosci</title>
              <imprint>
                <biblScope unit="volume">9</biblScope>
                <biblScope unit="page">217</biblScope>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b19">
            <analytic>
              <title level="a" type="main">Selective cortical representation of attended speaker in multi-talker speech perception</title>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Mesgarani</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <forename type="middle">F E F</forename>
                  <surname>Chang</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Nature</title>
              <imprint>
                <biblScope unit="volume">485</biblScope>
                <biblScope from="233" to="236" unit="page"/>
                <date type="published" when="2012"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b20">
            <analytic>
              <title level="a" type="main">Attentional Selection in a Cocktail Party Environment Can Be Decoded from Single-Trial EEG</title>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">A</forename>
                  <surname>O&amp;apos;sullivan</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Cereb. Cortex</title>
              <imprint>
                <biblScope unit="volume">355</biblScope>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b21">
            <analytic>
              <title level="a" type="main">Emergence of neural encoding of auditory objects while listening to competing speakers</title>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Ding</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">Z</forename>
                  <surname>Simon</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Proc. Natl. Acad. Sci</title>
              <imprint>
                <biblScope unit="volume">109</biblScope>
                <biblScope from="11854" to="11859" unit="page"/>
                <date type="published" when="2012"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b22">
            <analytic>
              <title level="a" type="main">Reconstructing the spectrotemporal modulations of real-life sounds from fMRI response patterns</title>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <surname>Santoro</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Proc. Natl. Acad. Sci</title>
              <imprint>
                <biblScope unit="volume">114</biblScope>
                <biblScope from="4799" to="4804" unit="page"/>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b23">
            <analytic>
              <title level="a" type="main">Neural speech recognition: continuous phoneme decoding using spatiotemporal representations of human cortical activity</title>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">A D A</forename>
                  <surname>Moses</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Mesgarani</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">K M K</forename>
                  <surname>Leonard</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <forename type="middle">F E F</forename>
                  <surname>Chang</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Neural Eng</title>
              <imprint>
                <biblScope unit="volume">13</biblScope>
                <biblScope unit="page">56004</biblScope>
                <date type="published" when="2016"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b24">
            <analytic>
              <title level="a" type="main">Dynamic Encoding of Acoustic Features in Neural Responses to Continuous Speech</title>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Khalighinejad</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <forename type="middle">C</forename>
                  <surname>Da Silva</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Mesgarani</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Neurosci</title>
              <imprint>
                <biblScope unit="volume">37</biblScope>
                <biblScope from="2176" to="2185" unit="page"/>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b25">
            <analytic>
              <title level="a" type="main">Machine-learning-based coadaptive calibration for brain-computer interfaces</title>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Vidaurre</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Sannelli</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K.-R</forename>
                  <surname>MÃ¼ller</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Blankertz</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Neural Comput</title>
              <imprint>
                <biblScope unit="volume">23</biblScope>
                <biblScope from="791" to="816" unit="page"/>
                <date type="published" when="2011"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b26">
            <analytic>
              <title level="a" type="main">Should the parameters of a BCI translation algorithm be continually adapted?</title>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">J</forename>
                  <surname>Mcfarland</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">W</forename>
                  <forename type="middle">A</forename>
                  <surname>Sarnacki</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">R</forename>
                  <surname>Wolpaw</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Neurosci. Methods</title>
              <imprint>
                <biblScope unit="volume">199</biblScope>
                <biblScope from="103" to="107" unit="page"/>
                <date type="published" when="2011"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b27">
            <analytic>
              <title level="a" type="main">Deep learning</title>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Lecun</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Bengio</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Hinton</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Nature</title>
              <imprint>
                <biblScope unit="volume">521</biblScope>
                <biblScope unit="page">436</biblScope>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b28">
            <analytic>
              <title level="a" type="main">Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Process. Mag</title>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Hinton</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">IEEE</title>
              <imprint>
                <biblScope unit="volume">29</biblScope>
                <biblScope from="82" to="97" unit="page"/>
                <date type="published" when="2012"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b29">
            <analytic>
              <title level="a" type="main">Speaker-Independent Speech Separation With Deep AttractorNetwork</title>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <forename type="middle">Y</forename>
                  <surname>Luo</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Z</forename>
                  <surname>Chen</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Mesgarani</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
              <imprint>
                <biblScope unit="volume">26</biblScope>
                <biblScope from="787" to="796" unit="page"/>
                <date type="published" when="2018"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b30">
            <analytic>
              <title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
              <author>
                <persName>
                  <forename type="first">Z</forename>
                  <surname>Chen</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <forename type="middle">Y</forename>
                  <surname>Luo</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Mesgarani</surname>
                </persName>
              </author>
              <idno type="DOI">10.1109/ICASSP.2017.7952155</idno>
            </analytic>
            <monogr>
              <title level="m">Acoustics, Speech and Signal Processing</title>
              <imprint>
                <publisher>IEEE</publisher>
                <date type="published" when="2017"/>
                <biblScope from="246" to="250" unit="page"/>
              </imprint>
            </monogr>
            <note>2017 IEEE International Conference on</note>
          </biblStruct>
          <biblStruct xml:id="b31">
            <analytic>
              <title level="a" type="main">Neural decoding of attentional selection in multi-speaker environments without access to clean sources</title>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>O&amp;apos;sullivan</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Neural Eng</title>
              <imprint>
                <biblScope unit="volume">14</biblScope>
                <biblScope unit="page">56001</biblScope>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b32">
            <analytic>
              <title level="a" type="main">Rapid synaptic depression explains nonlinear modulation of spectro-temporal tuning in primary auditory cortex by natural stimuli</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">V S V</forename>
                  <surname>David</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Mesgarani</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">B J B</forename>
                  <surname>Fritz</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">A S A</forename>
                  <surname>Shamma</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J Neurosci</title>
              <imprint>
                <biblScope unit="volume">29</biblScope>
                <biblScope from="3374" to="3386" unit="page"/>
                <date type="published" when="2009"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b33">
            <analytic>
              <title level="a" type="main">Mechanisms of noise robust representation of speech in primary auditory cortex</title>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Mesgarani</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">V S V</forename>
                  <surname>David</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">B J B</forename>
                  <surname>Fritz</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">A S A</forename>
                  <surname>Shamma</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Proc. Natl. Acad. Sci</title>
              <imprint>
                <biblScope unit="volume">111</biblScope>
                <biblScope from="6792" to="6797" unit="page"/>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b34">
            <monogr>
              <title level="m" type="main">TI 46-Word LDC93S9. Linguistic Data Consortium</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Liberman</surname>
                </persName>
              </author>
              <imprint>
                <date type="published" when="1993"/>
                <pubPlace>Philadelphia</pubPlace>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b35">
            <analytic>
              <title level="a" type="main">Induced electrocorticographic gamma activity during auditory perception</title>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <forename type="middle">E</forename>
                  <surname>Crone</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Boatman</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Gordon</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Hao</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Clin. Neurophysiol</title>
              <imprint>
                <biblScope unit="volume">112</biblScope>
                <biblScope from="565" to="582" unit="page"/>
                <date type="published" when="2001"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b36">
            <analytic>
              <title level="a" type="main">Comparison of time-frequency responses and the event-related potential to auditory speech stimuli in human cortex</title>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <surname>Edwards</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Neurophysiol</title>
              <imprint>
                <biblScope unit="volume">102</biblScope>
                <biblScope from="377" to="386" unit="page"/>
                <date type="published" when="2009"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b37">
            <analytic>
              <title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Lecun</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">Advances in neural information processing systems</title>
              <imprint>
                <date type="published" when="1990"/>
                <biblScope from="396" to="404" unit="page"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b38">
            <analytic>
              <title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Krizhevsky</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">I</forename>
                  <surname>Sutskever</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <forename type="middle">E</forename>
                  <surname>Hinton</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">Advances in neural information processing systems</title>
              <imprint>
                <date type="published" when="2012"/>
                <biblScope from="1097" to="1105" unit="page"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b39">
            <analytic>
              <title level="a" type="main">A high-throughput screening approach to discovering good forms of biologically inspired visual representation</title>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Pinto</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Doukhan</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">J</forename>
                  <surname>Dicarlo</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">D</forename>
                  <surname>Cox</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">PLoS Comput. Biol</title>
              <imprint>
                <biblScope unit="volume">5</biblScope>
                <biblScope unit="page">1000579</biblScope>
                <date type="published" when="2009"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b40">
            <analytic>
              <title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Srivastava</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>Hinton</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Krizhevsky</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">I</forename>
                  <surname>Sutskever</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <surname>Salakhutdinov</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Mach. Learn. Res</title>
              <imprint>
                <biblScope unit="volume">15</biblScope>
                <biblScope from="1929" to="1958" unit="page"/>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b41">
            <monogr>
              <title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Ioffe</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Szegedy</surname>
                </persName>
              </author>
              <idno>arXiv1502.03167</idno>
              <imprint>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
            <note type="report_type">arXiv Prepr.</note>
          </biblStruct>
          <biblStruct xml:id="b42">
            <analytic>
              <title level="a" type="main">Multiresolution spectrotemporal analysis of complex sounds</title>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <surname>Chi</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <surname>Ru</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">A</forename>
                  <surname>Shamma</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J Acoust Soc Am</title>
              <imprint>
                <biblScope unit="volume">118</biblScope>
                <biblScope from="887" to="906" unit="page"/>
                <date type="published" when="2005"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b43">
            <analytic>
              <title level="a" type="main">Discrimination of speech from nonspeech based on multiscale spectro-temporal modulations</title>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Mesgarani</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Slaney</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">A S A</forename>
                  <surname>Shamma</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">IEEE Trans. Audio. Speech. Lang. Processing</title>
              <imprint>
                <biblScope unit="volume">14</biblScope>
                <biblScope from="920" to="930" unit="page"/>
                <date type="published" when="2006"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b44">
            <analytic>
              <title level="a" type="main">WORLD: a vocoder-based high-quality speech synthesis system for real-time applications</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Morise</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">F</forename>
                  <surname>Yokomori</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Ozawa</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">IEICE Trans. Inf. Syst</title>
              <imprint>
                <biblScope unit="volume">99</biblScope>
                <biblScope from="1877" to="1884" unit="page"/>
                <date type="published" when="2016"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b45">
            <analytic>
              <title level="a" type="main">Restructuring speech representations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequency-based F0 extraction: Possible role of a repetitive structure in sounds1</title>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <surname>Kawahara</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">I</forename>
                  <surname>Masuda-Katsuse</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>De Cheveigne</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Speech Commun</title>
              <imprint>
                <biblScope unit="volume">27</biblScope>
                <biblScope from="187" to="207" unit="page"/>
                <date type="published" when="1999"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b46">
            <monogr>
              <title level="m" type="main">Reducing the dimensionality of data with neural networks. Science (80-.)</title>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <forename type="middle">E</forename>
                  <surname>Hinton</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <forename type="middle">R</forename>
                  <surname>Salakhutdinov</surname>
                </persName>
              </author>
              <imprint>
                <date type="published" when="2006"/>
                <biblScope unit="volume">313</biblScope>
                <biblScope from="504" to="507" unit="page"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b47">
            <analytic>
              <title level="a" type="main">The design for the Wall Street Journal-based CSR corpus</title>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">B</forename>
                  <surname>Paul</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">M</forename>
                  <surname>Baker</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">Proceedings of the workshop on Speech and Natural Language</title>
              <meeting>the workshop on Speech and Natural Language</meeting>
              <imprint>
                <publisher>Association for Computational Linguistics</publisher>
                <date type="published" when="1992"/>
                <biblScope from="357" to="362" unit="page"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b48">
            <analytic>
              <title level="a" type="main">MOS and pair comparison combined methods for quality evaluation of text-to-speech systems</title>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <forename type="middle">L</forename>
                  <surname>Salza</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <surname>Foti</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Nebbia</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Oreglia</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Acta Acust. united with Acust</title>
              <imprint>
                <biblScope unit="volume">82</biblScope>
                <biblScope from="650" to="656" unit="page"/>
                <date type="published" when="1996"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b49">
            <analytic>
              <title level="a" type="main">An Algorithm for Predicting the Intelligibility of Speech Masked by Modulated Noise Maskers</title>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Jensen</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <forename type="middle">H</forename>
                  <surname>Taal</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">IEEE/ACM Trans. Audio, Speech Lang. Process</title>
              <imprint>
                <biblScope unit="volume">24</biblScope>
                <date type="published" when="2009"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b50">
            <analytic>
              <title level="a" type="main">The origin of extracellular fields and currents-EEG, ECoG, LFP and spikes</title>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <surname>BuzsÃ¡ki</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <forename type="middle">A</forename>
                  <surname>Anastassiou</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Koch</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Nat. Rev. Neurosci</title>
              <imprint>
                <biblScope unit="volume">13</biblScope>
                <biblScope from="407" to="420" unit="page"/>
                <date type="published" when="2012"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b51">
            <analytic>
              <title level="a" type="main">The functional role of cross-frequency coupling</title>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <forename type="middle">T</forename>
                  <surname>Canolty</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <forename type="middle">T</forename>
                  <surname>Knight</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Trends Cogn. Sci</title>
              <imprint>
                <biblScope unit="volume">14</biblScope>
                <biblScope from="506" to="515" unit="page"/>
                <date type="published" when="2010"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b52">
            <analytic>
              <title level="a" type="main">Statistical models for neural encoding, decoding, and optimal stimulus design. Prog</title>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Paninski</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Pillow</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Lewi</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Brain Res</title>
              <imprint>
                <biblScope unit="volume">165</biblScope>
                <biblScope from="493" to="507" unit="page"/>
                <date type="published" when="2007"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b53">
            <analytic>
              <title level="a" type="main">Speech reconstruction from human auditory cortex with deep neural networks</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Yang</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
              <imprint>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b54">
            <analytic>
              <title level="a" type="main">Localization and classification of phonemes using high spatial resolution electrocorticography (ECoG) grids</title>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <surname>Blakely</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <forename type="middle">J</forename>
                  <surname>Miller</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <forename type="middle">P N</forename>
                  <surname>Rao</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <forename type="middle">D</forename>
                  <surname>Holmes</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">G</forename>
                  <surname>Ojemann</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Conf. Proc. IEEE Eng. Med. Biol. Soc</title>
              <imprint>
                <biblScope from="4964" to="4971" unit="page"/>
                <date type="published" when="2008"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b55">
            <analytic>
              <title level="a" type="main">Direct classification of all American English phonemes using signals from functional speech motor cortex</title>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <forename type="middle">M</forename>
                  <surname>Mugler</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Neural Eng</title>
              <imprint>
                <biblScope unit="volume">11</biblScope>
                <biblScope unit="page">35015</biblScope>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b56">
            <analytic>
              <title level="a" type="main">Electrocorticographic representations of segmental features in continuous speech</title>
              <author>
                <persName>
                  <forename type="first">F</forename>
                  <surname>Lotte</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Front. Hum. Neurosci</title>
              <imprint>
                <biblScope unit="volume">9</biblScope>
                <biblScope unit="page">97</biblScope>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b57">
            <analytic>
              <title level="a" type="main">Towards direct speech synthesis from ECoG: A pilot study</title>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Herff</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">Engineering in Medicine and Biology Society (EMBC)</title>
              <imprint>
                <publisher>IEEE</publisher>
                <date type="published" when="2016"/>
                <biblScope from="1540" to="1543" unit="page"/>
              </imprint>
            </monogr>
            <note>IEEE 38th Annual International Conference of the</note>
          </biblStruct>
          <biblStruct xml:id="b58">
            <analytic>
              <title level="a" type="main">A fast learning algorithm for deep belief nets</title>
              <author>
                <persName>
                  <forename type="first">G</forename>
                  <forename type="middle">E</forename>
                  <surname>Hinton</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Neural Comput</title>
              <imprint>
                <biblScope unit="volume">18</biblScope>
                <biblScope from="1527" to="1554" unit="page"/>
                <date type="published" when="2006"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b59">
            <analytic>
              <title level="a" type="main">EEG-based prediction of driver's cognitive performance by deep convolutional neural network. Signal Process</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Hajinoroozi</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Z</forename>
                  <surname>Mao</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">T.-P</forename>
                  <surname>Jung</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">C.-T</forename>
                  <surname>Lin</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Huang</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Image Commun</title>
              <imprint>
                <biblScope unit="volume">47</biblScope>
                <biblScope from="549" to="555" unit="page"/>
                <date type="published" when="2016"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b60">
            <analytic>
              <title level="a" type="main">Brain stimulation for epilepsy: can scheduled or responsive neurostimulation stop seizures?</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Morrell</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Curr. Opin. Neurol</title>
              <imprint>
                <biblScope unit="volume">19</biblScope>
                <biblScope from="164" to="168" unit="page"/>
                <date type="published" when="2006"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b61">
            <analytic>
              <title level="a" type="main">The efficiency of logistic regression compared to normal discriminant analysis</title>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Efron</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Am. Stat. Assoc</title>
              <imprint>
                <biblScope unit="volume">70</biblScope>
                <biblScope from="892" to="898" unit="page"/>
                <date type="published" when="1975"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b62">
            <analytic>
              <title level="a" type="main">Comparison of brain-computer interface decoding algorithms in open-loop and closed-loop control</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Koyama</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Comput. Neurosci</title>
              <imprint>
                <biblScope unit="volume">29</biblScope>
                <biblScope from="73" to="87" unit="page"/>
                <date type="published" when="2010"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b63">
            <analytic>
              <title level="a" type="main">Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</title>
              <author>
                <persName>
                  <forename type="first">H</forename>
                  <surname>Luo</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Poeppel</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Neuron</title>
              <imprint>
                <biblScope unit="volume">54</biblScope>
                <biblScope from="1001" to="1010" unit="page"/>
                <date type="published" when="2007"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b64">
            <analytic>
              <title level="a" type="main">Different Origins of Gamma Rhythm and High-Gamma Activity in Macaque Visual Cortex</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Ray</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">H</forename>
                  <surname>Maunsell</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">PLoS Biol</title>
              <imprint>
                <biblScope unit="volume">9</biblScope>
                <date type="published" when="2011"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b65">
            <analytic>
              <title level="a" type="main">&amp; Den Nijs, M. Power-law scaling in the brain surface electric potential</title>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <forename type="middle">J</forename>
                  <surname>Miller</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <forename type="middle">B</forename>
                  <surname>Sorensen</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <forename type="middle">G</forename>
                  <surname>Ojemann</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">PLoS Comput. Biol</title>
              <imprint>
                <biblScope unit="volume">5</biblScope>
                <biblScope unit="page">1000609</biblScope>
                <date type="published" when="2009"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b66">
            <analytic>
              <title level="a" type="main">Hearing in the mind's ear: a PET investigation of musical imagery and perception</title>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <forename type="middle">J</forename>
                  <surname>Zatorre</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <forename type="middle">R</forename>
                  <surname>Halpern</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">W</forename>
                  <surname>Perry</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <surname>Meyer</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <forename type="middle">C</forename>
                  <surname>Evans</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Cogn. Neurosci</title>
              <imprint>
                <biblScope unit="volume">8</biblScope>
                <biblScope from="29" to="46" unit="page"/>
                <date type="published" when="1996"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b67">
            <analytic>
              <title level="a" type="main">The functional neuroanatomy of metrical stress evaluation of perceived and imagined spoken words</title>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Aleman</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Cereb. Cortex</title>
              <imprint>
                <biblScope unit="volume">15</biblScope>
                <biblScope from="221" to="228" unit="page"/>
                <date type="published" when="2005"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b68">
            <analytic>
              <title level="a" type="main">Scanning silence: mental imagery of complex sounds</title>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Bunzeck</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <surname>Wuestenberg</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>Lutz</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">H.-J</forename>
                  <surname>Heinze</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Jancke</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Neuroimage</title>
              <imprint>
                <biblScope unit="volume">26</biblScope>
                <biblScope from="1119" to="1127" unit="page"/>
                <date type="published" when="2005"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b69">
            <analytic>
              <title level="a" type="main">Neural encoding of auditory features during music perception and imagery</title>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Martin</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Cereb. Cortex</title>
              <imprint>
                <biblScope unit="volume">1</biblScope>
                <biblScope unit="page">12</biblScope>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b70">
            <analytic>
              <title level="a" type="main">NeuroGrid: recording action potentials from the surface of the brain</title>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <surname>Khodagholy</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Nat. Neurosci</title>
              <imprint>
                <biblScope unit="volume">18</biblScope>
                <biblScope unit="page">310</biblScope>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b71">
            <analytic>
              <title level="a" type="main">Towards adaptive classification for BCI</title>
              <author>
                <persName>
                  <forename type="first">P</forename>
                  <surname>Shenoy</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Krauledat</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Blankertz</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">R</forename>
                  <forename type="middle">P N</forename>
                  <surname>Rao</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K.-R</forename>
                  <surname>MÃ¼ller</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Neural Eng</title>
              <imprint>
                <biblScope unit="volume">3</biblScope>
                <biblScope unit="page">13</biblScope>
                <date type="published" when="2006"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b72">
            <monogr>
              <title level="m" type="main">iELVis: An open source MATLAB toolbox for localizing and visualizing human intracranial electrode data</title>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">M</forename>
                  <surname>Groppe</surname>
                </persName>
              </author>
              <imprint>
                <biblScope unit="page">281</biblScope>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b73">
            <analytic>
              <title level="a" type="main">BioImage Suite: An integrated medical image analysis suite: An update</title>
              <author>
                <persName>
                  <forename type="first">X</forename>
                  <surname>Papademetris</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Insight J</title>
              <imprint>
                <biblScope unit="page">209</biblScope>
                <date type="published" when="2006"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b74">
            <analytic>
              <title level="a" type="main">Automatically parcellating the human cerebral cortex</title>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Fischl</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Cereb. cortex</title>
              <imprint>
                <biblScope unit="volume">14</biblScope>
                <biblScope from="11" to="22" unit="page"/>
                <date type="published" when="2004"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b75">
            <analytic>
              <title level="a" type="main">Automatic parcellation of human cortical gyri and sulci using standard anatomical nomenclature</title>
              <author>
                <persName>
                  <forename type="first">C</forename>
                  <surname>Destrieux</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Fischl</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Dale</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <surname>Halgren</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">Neuroimage</title>
              <imprint>
                <biblScope unit="volume">53</biblScope>
                <biblScope from="1" to="15" unit="page"/>
                <date type="published" when="2010"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b76">
            <analytic>
              <title level="a" type="main">Auditory representations of acoustic signals</title>
              <author>
                <persName>
                  <forename type="first">X</forename>
                  <surname>Yang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <forename type="middle">A W</forename>
                  <surname>Shamma</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">IEEE Trans. Inf. Theory</title>
              <imprint>
                <biblScope unit="volume">38</biblScope>
                <biblScope from="824" to="839" unit="page"/>
                <date type="published" when="1992"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b77">
            <analytic>
              <title level="a" type="main">Selecting receptive fields in deep networks</title>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Coates</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <forename type="middle">Y</forename>
                  <surname>Ng</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">Advances in Neural Information Processing Systems</title>
              <imprint>
                <date type="published" when="2011"/>
                <biblScope from="2528" to="2536" unit="page"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b78">
            <analytic>
              <title level="a" type="main">Convolutional networks for images, speech, and time series</title>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Lecun</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">Y</forename>
                  <surname>Bengio</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">Handb. brain theory neural networks</title>
              <imprint>
                <date type="published" when="1995"/>
                <biblScope unit="page">3361</biblScope>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b79">
            <analytic>
              <title level="a" type="main">TensorFlow: A System for Large-Scale Machine Learning</title>
              <author>
                <persName>
                  <forename type="first">M</forename>
                  <surname>Abadi</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">OSDI 16</title>
              <imprint>
                <date type="published" when="2016"/>
                <biblScope from="265" to="283" unit="page"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b80">
            <analytic>
              <title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <surname>He</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">X</forename>
                  <surname>Zhang</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Ren</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Sun</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">Proceedings of the IEEE international conference on computer vision</title>
              <meeting>the IEEE international conference on computer vision</meeting>
              <imprint>
                <date type="published" when="2015"/>
                <biblScope from="1026" to="1034" unit="page"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b81">
            <analytic>
              <title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <forename type="middle">L</forename>
                  <surname>Maas</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <forename type="middle">Y</forename>
                  <surname>Hannun</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <forename type="middle">Y</forename>
                  <surname>Ng</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="m">Proc. icml</title>
              <meeting>icml</meeting>
              <imprint>
                <date type="published" when="2013"/>
                <biblScope unit="volume">30</biblScope>
                <biblScope unit="page">3</biblScope>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b82">
            <monogr>
              <title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
              <author>
                <persName>
                  <forename type="first">D.-A</forename>
                  <surname>Clevert</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <surname>Unterthiner</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">S</forename>
                  <surname>Hochreiter</surname>
                </persName>
              </author>
              <idno>arXiv1511.07289</idno>
              <imprint>
                <date type="published" when="2015"/>
              </imprint>
            </monogr>
            <note type="report_type">arXiv Prepr.</note>
          </biblStruct>
          <biblStruct xml:id="b83">
            <monogr>
              <title level="m" type="main">A method for stochastic optimization</title>
              <author>
                <persName>
                  <forename type="first">D</forename>
                  <forename type="middle">P</forename>
                  <surname>Kingma</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">J</forename>
                  <surname>Ba</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <surname>Adam</surname>
                </persName>
              </author>
              <idno>arXiv1412.6980</idno>
              <imprint>
                <date type="published" when="2014"/>
              </imprint>
            </monogr>
            <note type="report_type">arXiv Prepr.</note>
          </biblStruct>
          <biblStruct xml:id="b84">
            <analytic>
              <title level="a" type="main">Spatial resolution dependence on spectral frequency in human speech cortex electrocorticography</title>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <surname>Muller</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">L</forename>
                  <forename type="middle">S</forename>
                  <surname>Hamilton</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <surname>Edwards</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">K</forename>
                  <forename type="middle">E</forename>
                  <surname>Bouchard</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">E</forename>
                  <forename type="middle">F</forename>
                  <surname>Chang</surname>
                </persName>
              </author>
            </analytic>
            <monogr>
              <title level="j">J. Neural Eng</title>
              <imprint>
                <biblScope unit="volume">13</biblScope>
                <biblScope unit="page">56013</biblScope>
                <date type="published" when="2016"/>
              </imprint>
            </monogr>
          </biblStruct>
          <biblStruct xml:id="b85">
            <analytic>
              <title level="a" type="main">NAPLib: An open source toolbox for real-time and offline Neural Acoustic Processing</title>
              <author>
                <persName>
                  <forename type="first">B</forename>
                  <surname>Khalighinejad</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">T</forename>
                  <surname>Nagamine</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">A</forename>
                  <surname>Mehta</surname>
                </persName>
              </author>
              <author>
                <persName>
                  <forename type="first">N</forename>
                  <surname>Mesgarani</surname>
                </persName>
              </author>
              <idno type="DOI">10.1109/ICASSP.2017.7952275</idno>
              <ptr target="https://doi.org/10.1109/ICASSP.2017.7952275"/>
            </analytic>
            <monogr>
              <title level="m">2017 IEEE International Conference on 846-850</title>
              <imprint>
                <publisher>IEEE</publisher>
                <date type="published" when="2017"/>
              </imprint>
            </monogr>
            <note>Acoustics, Speech and Signal Processing</note>
          </biblStruct>
        </listBibl>
      </div>
    </back>
  </text>
</TEI>
